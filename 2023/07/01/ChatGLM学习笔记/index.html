<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="description" content="Tan&#39;s Blog">
    <meta name="keyword"  content="Hello world">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          ChatGLM学习笔记 - Tan&#39;s Blog
        
    </title>

    <link rel="canonical" href="https://master-tan.github.io/2023/07/01/ChatGLM学习笔记/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
        
<link rel="stylesheet" href="/css/dusign-light.css">

        
<link rel="stylesheet" href="/css/dusign-common-light.css">

        
<link rel="stylesheet" href="/css/font-awesome.css">

        
<link rel="stylesheet" href="/css/toc.css">

        <!-- background effects end -->
    
    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- photography -->
    
<link rel="stylesheet" href="/css/photography.css">


    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('../../../../img/default.jpg')
                /*post*/
            
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
                            
                              <a class="tag" href="/tags/#暑期实习" title="暑期实习">暑期实习</a>
                            
                        </div>
                        <h1>ChatGLM学习笔记</h1>
                        <h2 class="subheading">暑期实习笔记-1</h2>
                        <span class="meta">
                            Posted by Tan on
                            2023-07-01
                        </span>

	       
                            <div class="blank_box"></div>
                            <span class="meta">
                                 <span class="post-count">4.8k</span> Words
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-light.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-light.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Tan&#39;s Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        	<li>
                          	  <a href="/archive/">Archives</a>
                        	</li>
                        
                    

                        
                        	<li>
                          	  <a href="/about/">About Tan</a>
                        	</li>
                        
                    

                        
                    

                        
                        	<li>
                          	  <a href="/tags/">Tags</a>
                        	</li>
                        
                    

                        
                    

                        
                    

                        
                    
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h1 id="Chatglm-官网"><a href="#Chatglm-官网" class="headerlink" title="Chatglm 官网"></a>Chatglm 官网</h1><p><a href="https://chatglm.cn/" target="_blank" rel="noopener">https://chatglm.cn/</a></p>
<h2 id="官方主页"><a href="#官方主页" class="headerlink" title="官方主页"></a>官方主页</h2><p><a href="https://chatglm.cn/blog" target="_blank" rel="noopener">https://chatglm.cn/blog</a></p>
<h2 id="官方社区"><a href="#官方社区" class="headerlink" title="官方社区"></a>官方社区</h2><p><a href="https://modelnet.ai/home" target="_blank" rel="noopener">https://modelnet.ai/home</a></p>
<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><h2 id="tar"><a href="#tar" class="headerlink" title="tar"></a>tar</h2><ul>
<li>参数 <code>-xvzf</code> : 用于<strong>解压缩</strong>一个 gzip 压缩的 tar 文件其中，-x 表示解包，-v 表示显示详细的解包过程，-z 表示解压缩 gzip 压缩，-f 表示指定要解压缩的文件名例如，tar -xvzf archive.tar.gz 将解压缩名为 archive.tar.gz 的 gzip 压缩的 tar 文件</li>
<li>参数 <code>-cvzf</code> : 用于<strong>压缩</strong>创建一个 gzip 压缩的 tar 文件其中，-c 表示创建归档文件，-v 表示显示详细的打包过程，-z 表示使用 gzip 压缩，-f 表示指定要创建的文件名例如，tar -cvzf archive.tar.gz folder/ 将创建名为 archive.tar.gz 的 gzip 压缩的 tar 文件，其中包含名为 folder 的目录下的所有文件和子目录</li>
<li>样例：  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -cvzf env.tar.gz anaconda3/envs/chatglm_tan <span class="comment"># 压缩</span></span><br><span class="line">tar -xvzf env.tar.gz -C anaconda3/envs/chatglm_tan <span class="comment"># 解压缩</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="示范流程"><a href="#示范流程" class="headerlink" title="示范流程"></a>示范流程</h1>   <!-- 65  30/06/23 08:59:42 ls
   66  30/06/23 08:59:49 vim test.txt 
   67  30/06/23 08:59:53 ls
   68  30/06/23 09:03:23 pwd
   69  30/06/23 09:04:15 ls
   70  30/06/23 09:04:17 cd chatglm2-6b/
   71  30/06/23 09:04:18 ls
   72  30/06/23 09:05:31 cd ../
   73  30/06/23 09:05:31 ls
   74  30/06/23 09:05:40 tar -xvzf ChatGLM-Efficient-Tuning.tar.gz 
   75  30/06/23 09:05:49 ls
   76  30/06/23 09:06:10 cd ChatGLM-Efficient-Tuning/
   77  30/06/23 09:06:11 ls
   78  30/06/23 09:06:14 cd data/
   79  30/06/23 09:06:14 ls
   80  30/06/23 09:06:22 cd ../
   81  30/06/23 09:06:25 cd examples/
   82  30/06/23 09:06:25 ls
   83  30/06/23 09:06:29 vim train_poi.sh 
   84  30/06/23 09:09:05 cd
   85  30/06/23 09:09:25 tar -cvzf env.tar.gz ~/env
   86  30/06/23 09:09:29 tar -cvzf env.tar.gz 
   87  30/06/23 09:09:34 tar -xvzf env.tar.gz 
   88  30/06/23 09:09:37 tar -xvzf env.tar.gz
   89  30/06/23 09:09:44 tar -xvzf env.tar.gz ~/env
   90  30/06/23 09:09:53 tar -xvzf ~/env env.tar.gz
   91  30/06/23 09:09:59 tar -xvzf env.tar.gz
   92  30/06/23 09:10:04 ll -la
   93  30/06/23 09:31:22 ls -ll
   94  30/06/23 09:31:27 tar -xvzf env.tar.gz
   95  30/06/23 09:34:58 slist
   96  30/06/23 09:35:08 sinfo
   97  30/06/23 09:35:54 conda activate chatglm_etuning
   98  30/06/23 09:35:57 ls
   99  30/06/23 09:36:02 cd ChatGLM-Efficient-Tuning/
  100  30/06/23 09:36:02 ls
  101  30/06/23 09:36:05 cd examples/
  102  30/06/23 09:36:05 ls
  103  30/06/23 09:38:33 cd
  104  30/06/23 09:38:43 vim run.sh
  105  30/06/23 09:40:35 vim run.slurm
  106  30/06/23 09:42:49 pwd
  107  30/06/23 09:42:55 vim run.slurm
  108  30/06/23 09:43:23 cd ChatGLM-Efficient-Tuning/
  109  30/06/23 09:43:24 pwd
  110  30/06/23 09:43:32 cd
  111  30/06/23 09:43:35 vim run.slurm
  112  30/06/23 10:07:55 sbatch run.slurm 
  113  30/06/23 10:08:20 vim run.slurm
  114  30/06/23 10:08:34 sbatch run.slurm 
  115  30/06/23 10:08:41 vim run.slurm
  116  30/06/23 10:08:49 sbatch run.slurm 
  117  30/06/23 10:09:02 vim run.slurm
  118  30/06/23 10:35:01 pip install deepseed
  119  30/06/23 10:35:10 python
  120  30/06/23 10:37:44 vim run.slurm 
  121  30/06/23 10:47:59 pwd
  122  30/06/23 10:48:47 vim run.slurm 
  123  30/06/23 10:50:30 ll
  124  30/06/23 10:50:42 tar -xvzf env.tar.gz
  125  30/06/23 10:55:45 accelerate config
  126  30/06/23 10:56:27 vim ~/.bashrc
  127  30/06/23 11:01:06 vim /gs/home/zhangzhibo/anaconda3/envs/chatglm_etuning/bin/accelerate
  128  30/06/23 11:01:34 accelerate config
  129  30/06/23 11:03:06 vim run.slurm 
  130  30/06/23 11:03:15 sbatch run.slurm 
  131  30/06/23 11:03:21 ls
  132  30/06/23 11:03:27 cat glm2.out 
  133  30/06/23 11:03:33 tail -100f glm2.out 
  134  30/06/23 11:03:37 tail -100f glm2.err 
  135  30/06/23 11:03:57 conda init bash
  136  30/06/23 11:04:08 vim run.slurm 
  137  30/06/23 11:05:48 scancel 8138036
  138  30/06/23 16:55:17 vim run.slurm 
  139  30/06/23 16:55:52 conda activate chatglm-etuning
  140  30/06/23 16:55:58 conda info --envs
  141  30/06/23 16:56:16 conda activate chatglm_etuning
  142  30/06/23 16:56:18 ls
  143  30/06/23 16:56:26 cd ChatGLM-Efficient-Tuning/
  144  30/06/23 16:56:28 cd ../
  145  30/06/23 16:56:34 sbatch run.slurm 
  146  30/06/23 16:56:47 tail -100f glm2.out 
  147  30/06/23 16:56:51 tail -100f glm2.err 
  148  30/06/23 16:57:02 scancel 8138197
  149  30/06/23 16:57:12 vim run.slurm 
  150  30/06/23 16:57:24 sbatch run.slurm 
  151  30/06/23 16:57:29 tail -100f glm2.err 
  152  30/06/23 16:57:35 tail -100f glm2.out
  153  30/06/23 16:57:59 vim run.slurm 
  154  30/06/23 16:58:09 scancel 8138198
  155  30/06/23 16:58:20 accelerate config
  156  30/06/23 17:05:18 sbatch run.slurm 
  157  30/06/23 17:05:30 tail -100f glm2.err 
  158  30/06/23 17:05:56 tail -100f glm2.out
  159  30/06/23 17:06:05 tail f glm2.out
  160  30/06/23 17:06:09 tail -f glm2.out
  161  30/06/23 17:06:18 tail -f glm2.out glm2.err
  162  30/06/23 17:06:33 tail -100f glm2.err 
  163  30/06/23 17:06:39 cat glm2.err 
  164  30/06/23 17:07:19 scancel 8138205
  165  30/06/23 17:07:23 vim run.slurm 
  166  30/06/23 17:07:44 rm -rf glm2.out 
  167  30/06/23 17:07:47 rm -rf glm2.err 
  168  30/06/23 17:07:53 sbatch run.slurm 
  169  30/06/23 17:07:56 tail -f glm2.out glm2.err
  170  30/06/23 17:13:46 scancel 8138208
  171  30/06/23 17:13:48 vim run.slurm 
  172  30/06/23 17:14:20 sbatch run.slurm 
  173  30/06/23 17:14:57 tail -f glm2.out glm2.err
  174  30/06/23 17:25:18 scancel 8138214
  175  30/06/23 17:25:24 scancel 8138215
  176  30/06/23 17:25:35 accelerate config
  177  30/06/23 17:26:50 sbatch run.slurm 
  178  30/06/23 17:26:53 tail -f glm2.out glm2.err
  179  30/06/23 18:37:31 vim run.slurm 
  180  30/06/23 18:43:31 sbatch run.slurm 
  181  30/06/23 18:43:33 tail -f glm2.out glm2.err -->
<pre><code>tar -xvzf ChatGLM-Efficient-Tuning.tar.gz 
tar -xvzf env.tar.gz
slist
sinfo
conda activate chatglm_etuning
pip install deepseed
accelerate config
vim ~/.bashrc
vim /gs/home/zhangzhibo/anaconda3/envs/chatglm_etuning/bin/accelerate
accelerate config
vim run.slurm 
sbatch run.slurm 
ls
cat glm2.out 
tail -100f glm2.out 
tail -100f glm2.err 
conda init bash
vim run.slurm 
scancel 8138036
vim run.slurm 
conda activate chatglm-etuning
conda info --envs
sbatch run.slurm 
tail -100f glm2.out 
tail -100f glm2.err 
tail -f glm2.out glm2.err
squeue
</code></pre><h1 id="训练原理"><a href="#训练原理" class="headerlink" title="训练原理"></a>训练原理</h1><p>？</p>
<h1 id="CHATGLM-6B-模型参数微调"><a href="#CHATGLM-6B-模型参数微调" class="headerlink" title="CHATGLM - 6B 模型参数微调"></a>CHATGLM - 6B 模型参数微调</h1><h2 id="模型地址"><a href="#模型地址" class="headerlink" title="模型地址"></a>模型地址</h2><p><a href="https://github.com/THUDM/ChatGLM-6B" target="_blank" rel="noopener">https://github.com/THUDM/ChatGLM-6B</a></p>
<h2 id="P-Tuning-v2"><a href="#P-Tuning-v2" class="headerlink" title="P-Tuning v2"></a>P-Tuning v2</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>P-Tuning是一种较新的模型微调方法，它采用了参数剪枝的技术，可以将微调的参数量减少到原来的0.1%。具体来说，P-Tuning v2是基于P-Tuning v1的升级版，主要的改进在于采用了更加高效的剪枝方法，可以进一步减少模型微调的参数量。</p>
<p>P-Tuning v2的原理是通过对已训练好的大型语言模型进行参数剪枝，得到一个更加小巧、效率更高的轻量级模型。具体地，P-Tuning v2首先使用一种自适应的剪枝策略，对大型语言模型中的参数进行裁剪，去除其中不必要的冗余参数。然后，对于被剪枝的参数，P-Tuning v2使用了一种特殊的压缩方法，能够更加有效地压缩参数大小，并显著减少模型微调的总参数量。</p>
<p>总的来说，P-Tuning v2的核心思想是让模型变得更加轻便、更加高效，同时尽可能地保持模型的性能不受影响。这不仅可以加快模型的训练和推理速度，还可以减少模型在使用过程中的内存和计算资源消耗，让模型更适用于各种实际应用场景中。</p>
<p>对于 ChatGLM-6B 模型基于 P-Tuning v2 进行微调。可将需要微调的参数量减少到原来的 0.1%，再通过模型量化、Gradient Checkpoint 等方法，最低只需要 7GB 显存即可运行。</p>
<p><img src='./1226f28683ec4b7093cea03868a3ea28.jpeg'></p>
<h3 id="官方论文"><a href="#官方论文" class="headerlink" title="官方论文"></a>官方论文</h3><p><a href="https://arxiv.org/pdf/2110.07602.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2110.07602.pdf</a><br><a href="./2110.07602.pdf"> 查看 </a></p>
<h3 id="文件组织"><a href="#文件组织" class="headerlink" title="文件组织"></a>文件组织</h3><p>ChatGLM/ptuning/<br>├── arguments.py - 定义了模型、数据和训练相关参数的类<br>├── deepspeed.json - Deepspeed配置文件<br>├── ds_train_finetune.sh - 使用Deepspeed进行微调的shell脚本<br>├── evaluate_finetune.sh - 用于评估微调后的模型的shell脚本<br>├── evaluate.sh - 用于评估模型的shell脚本<br>├── main.py - 解析命令行参数并调用相应的训练或预测逻辑<br>├── README.md<br>├── README_en.md<br>├── trainer.py - 定义了模型训练和验证的主要逻辑<br>├── trainer_seq2seq.py - 继承自trainer.py，用于处理序列到序列模型的训练和验证<br>├── train_chat.sh - 用于训练chat模型的shell脚本，它调用main.py并传递一些参数来启动训练过程<br>├── train.sh - 用于训练模型的shell脚本<br>├── web_demo.py - Web Demo的主要逻辑<br>└── web_demo.sh - 启动Web Demo的shell脚本</p>
<h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h3><p>运行微调需要 4.27.1 版本的 transformers<br>pip install rouge_chinese nltk jieba datasets</p>
<h3 id="训练数据集"><a href="#训练数据集" class="headerlink" title="训练数据集"></a>训练数据集</h3><h4 id="官方样例数据集"><a href="#官方样例数据集" class="headerlink" title="官方样例数据集"></a>官方样例数据集</h4><p>官方微调样例是以 ADGEN (广告生成) 数据集为例来介绍微调的具体使用。</p>
<p>ADGEN 数据集为根据输入（content）生成一段广告词（summary），具体格式如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;content&quot;: &quot;类型#上衣*版型#宽松*版型#显瘦*图案#线条*衣样式#衬衫*衣袖型#泡泡袖*衣款式#抽绳&quot;,</span><br><span class="line">    &quot;summary&quot;: &quot;这件衬衫的款式非常的宽松，利落的线条可以很好的隐藏身材上的小缺点，穿在身上有着很好的显瘦效果。领口装饰了一个可爱的抽绳，漂亮的绳结展现出了十足的个性，配合时尚的泡泡袖型，尽显女性甜美可爱的气息。&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>将 ADGEN 数据集放到 ptuning 目录下并将其解压到 AdvertiseGen 目录</p>
<h4 id="自己的数据集"><a href="#自己的数据集" class="headerlink" title="自己的数据集"></a>自己的数据集</h4><p>修改 train.sh 和 evaluate.sh 中的 train_file、validation_file和test_file为你自己的 JSON 格式数据集路径，并将 prompt_column 和 response_column 改为 JSON 文件中输入文本和输出文本对应的 KEY</p>
<h3 id="参数详解"><a href="#参数详解" class="headerlink" title="参数详解"></a>参数详解</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ptuning/main.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) == <span class="number">2</span> <span class="keyword">and</span> sys.argv[<span class="number">1</span>].endswith(<span class="string">".json"</span>):</span><br><span class="line">        <span class="comment"># If we pass only one argument to the script and it's the path to a json file,</span></span><br><span class="line">        <span class="comment"># let's parse it to get our arguments.</span></span><br><span class="line">        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model_args, data_args, training_args = parser.parse_args_into_dataclasses()</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>在以上代码中可知，原参数分为三种，分别为ModelArguments（模型相关参数）、DataTrainingArguments（数据相关参数）和Seq2SeqTrainingArguments（训练相关参数）</p>
<h4 id="arguments-ModelArguments"><a href="#arguments-ModelArguments" class="headerlink" title="arguments.ModelArguments"></a>arguments.ModelArguments</h4><p>用于指定模型的参数</p>
<ul>
<li><p><code>model_name_or_path</code> (str):</p>
<ul>
<li>描述：预训练模型或模型标识符的路径，可以是 huggingface.co/models 上的模型。</li>
<li>默认值：无</li>
</ul>
</li>
<li><p><code>ptuning_checkpoint</code> (str):</p>
<ul>
<li>描述：p-tuning v2 检查点的路径。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>config_name</code> (Optional[str]):</p>
<ul>
<li>描述：预训练配置的名称或路径，如果不同于 <code>model_name_or_path</code>。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>tokenizer_name</code> (Optional[str]):</p>
<ul>
<li>描述：预训练 tokenizer 的名称或路径，如果不同于 <code>model_name_or_path</code>。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>cache_dir</code> (Optional[str]):</p>
<ul>
<li>描述：存储从 huggingface.co 下载的预训练模型的目录。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>use_fast_tokenizer</code> (bool):</p>
<ul>
<li>描述：是否使用由 tokenizers 库支持的快速 tokenizer。</li>
<li>默认值：True</li>
</ul>
</li>
<li><p><code>model_revision</code> (str):</p>
<ul>
<li>描述：要使用的具体模型版本（可以是分支名称、标签名称或提交 ID）。</li>
<li>默认值：”main”</li>
</ul>
</li>
<li><p><code>use_auth_token</code> (bool):</p>
<ul>
<li>描述：是否使用运行 <code>huggingface-cli login</code> 时生成的令牌（用于使用私有模型）。</li>
<li>默认值：False</li>
</ul>
</li>
<li><p><code>resize_position_embeddings</code> (Optional[bool]):</p>
<ul>
<li>描述：是否在 <code>max_source_length</code> 超过模型位置嵌入时自动调整位置嵌入大小。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>quantization_bit</code> (Optional[int]):</p>
<ul>
<li>描述：量化的位数。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>pre_seq_len</code> (Optional[int]):</p>
<ul>
<li>描述：预处理的序列长度。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>prefix_projection</code> (bool):</p>
<ul>
<li>描述：是否对输入文本应用前缀投影。</li>
<li>默认值：False</li>
</ul>
</li>
</ul>
<h4 id="arguments-DataTrainingArguments"><a href="#arguments-DataTrainingArguments" class="headerlink" title="arguments.DataTrainingArguments"></a>arguments.DataTrainingArguments</h4><p>用于指定训练和评估数据的参数</p>
<ul>
<li><p><code>lang</code> (Optional[str]):</p>
<ul>
<li>描述：摘要的语言 ID。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>dataset_name</code> (Optional[str]):</p>
<ul>
<li>描述：要使用的数据集名称（通过 datasets 库）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>dataset_config_name</code> (Optional[str]):</p>
<ul>
<li>描述：要使用的数据集配置名称（通过 datasets 库）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>prompt_column</code> (Optional[str]):</p>
<ul>
<li>描述：数据集中包含完整文本的列名（用于摘要）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>response_column</code> (Optional[str]):</p>
<ul>
<li>描述：数据集中包含摘要的列名（用于摘要）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>history_column</code> (Optional[str]):</p>
<ul>
<li>描述：数据集中包含聊天历史的列名。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>train_file</code> (Optional[str]):</p>
<ul>
<li>描述：输入的训练数据文件（jsonlines 或 csv 文件）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>validation_file</code> (Optional[str]):</p>
<ul>
<li>描述：用于评估指标（rouge）的可选输入评估数据文件（jsonlines 或 csv 文件）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>test_file</code> (Optional[str]):</p>
<ul>
<li>描述：用于评估指标（rouge）的可选输入测试数据文件（jsonlines 或 csv 文件）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>overwrite_cache</code> (bool):</p>
<ul>
<li>描述：是否覆盖缓存的训练和评估集。</li>
<li>默认值：False</li>
</ul>
</li>
<li><p><code>preprocessing_num_workers</code> (Optional[int]):</p>
<ul>
<li>描述：用于预处理的进程数。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>max_source_length</code> (Optional[int]):</p>
<ul>
<li>描述：标记化后的最大输入序列长度。超过此长度的序列将被截断，较短的序列将被填充。</li>
<li>默认值：1024</li>
</ul>
</li>
<li><p><code>max_target_length</code> (Optional[int]):</p>
<ul>
<li>描述：标记化后的目标文本的最大序列长度。超过此长度的序列将被截断，较短的序列将被填充。</li>
<li>默认值：128</li>
</ul>
</li>
<li><p><code>val_max_target_length</code> (Optional[int]):</p>
<ul>
<li>描述：验证目标文本的最大序列长度。超过此长度的序列将被截断，较短的序列将被填充。默认为 <code>max_target_length</code>。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>pad_to_max_length</code> (bool):</p>
<ul>
<li>描述：是否将所有样本填充到模型的最大句子长度。如果为 False，将在批处理时动态地将样本填充到批次中的最大长度。在 GPU 上更高效，但对 TPU 来说非常低效。</li>
<li>默认值：False</li>
</ul>
</li>
<li><p><code>max_train_samples</code> (Optional[int]):</p>
<ul>
<li>描述：为调试或更快的训练而将训练示例的数量截断到该值。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>max_eval_samples</code> (Optional[int]):</p>
<ul>
<li>描述：为调试或更快的训练而将评估示例的数量截断到该值。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>max_predict_samples</code> (Optional[int]):</p>
<ul>
<li>描述：为调试或更快的训练而将预测示例的数量截断到该值。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>num_beams</code> (Optional[int]):</p>
<ul>
<li>描述：用于评估的 beam 数。此参数将传递给 <code>model.generate</code>，用于 <code>evaluate</code> 和 <code>predict</code>。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>ignore_pad_token_for_loss</code> (bool):</p>
<ul>
<li>描述：是否在损失计算中忽略与填充标签对应的令牌。</li>
<li>默认值：True</li>
</ul>
</li>
<li><p><code>source_prefix</code> (Optional[str]):</p>
<ul>
<li>描述：在每个源文本之前要添加的前缀（适用于 T5 模型）。</li>
<li>默认值：””（空字符串）</li>
</ul>
</li>
<li><p><code>forced_bos_token</code> (Optional[str]):</p>
<ul>
<li>描述：在 <code>decoder_start_token_id</code> 之后强制作为第一个生成的令牌的令牌。对于类似 mBART 的多语言模型很有用，其中第一个生成的令牌需要是目标语言令牌（通常是目标语言令牌）。</li>
<li>默认值：Nones</li>
</ul>
</li>
</ul>
<h4 id="Seq2SeqTrainingArguments"><a href="#Seq2SeqTrainingArguments" class="headerlink" title="Seq2SeqTrainingArguments"></a>Seq2SeqTrainingArguments</h4><ul>
<li>此类为transformers库中的Seq2SeqTrainingArguments类，用于指定训练相关的参数，具体参数可参考transformers库中的Seq2SeqTrainingArguments类的注释</li>
</ul>
<h4 id="具体-sh代码解析"><a href="#具体-sh代码解析" class="headerlink" title="具体.sh代码解析"></a>具体.sh代码解析</h4><h5 id="train-sh"><a href="#train-sh" class="headerlink" title="train.sh"></a>train.sh</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ptuning/train.sh</span></span><br><span class="line">PRE_SEQ_LEN=128 <span class="comment"># 模型输入的最大长度</span></span><br><span class="line">LR=2e-2 <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=0 python3 main.py</span><br><span class="line">    --do_train \ <span class="comment"># 进行训练</span></span><br><span class="line">    --train_file AdvertiseGen/train.json \ <span class="comment"># 训练数据集文件名</span></span><br><span class="line">    --validation_file AdvertiseGen/dev.json \ <span class="comment"># 验证数据集文件名</span></span><br><span class="line">    --prompt_column content \ <span class="comment"># 输入列名</span></span><br><span class="line">    --response_column summary \ <span class="comment"># 输出列名</span></span><br><span class="line">    --overwrite_cache \ <span class="comment"># 覆盖缓存文件</span></span><br><span class="line">    --model_name_or_path THUDM/chatglm-6b \ <span class="comment"># 模型名称或路径</span></span><br><span class="line">    --output_dir output/adgen-chatglm-6b-pt-<span class="variable">$PRE_SEQ_LEN</span>-<span class="variable">$LR</span> \ <span class="comment"># 输出目录</span></span><br><span class="line">    --overwrite_output_dir \ <span class="comment"># 覆盖输出目录</span></span><br><span class="line">    --max_source_length 64 \ <span class="comment"># 输入的最大长度</span></span><br><span class="line">    --max_target_length 64 \ <span class="comment"># 输出的最大长度</span></span><br><span class="line">    --per_device_train_batch_size 1 \ <span class="comment"># 训练时每个GPU的batch size</span></span><br><span class="line">    --per_device_eval_batch_size 1 \ <span class="comment"># 验证时每个GPU的batch size</span></span><br><span class="line">    --gradient_accumulation_steps 16 \ <span class="comment"># 梯度累积步数</span></span><br><span class="line">    --predict_with_generate \ <span class="comment"># 使用生成模式进行预测</span></span><br><span class="line">    --max_steps 3000 \ <span class="comment"># 最大训练步数</span></span><br><span class="line">    --logging_steps 10 \ <span class="comment"># 日志输出步数间隔</span></span><br><span class="line">    --save_steps 1000 \ <span class="comment"># 保存模型步数间隔</span></span><br><span class="line">    --learning_rate <span class="variable">$LR</span> \ <span class="comment"># 学习率</span></span><br><span class="line">    --pre_seq_len <span class="variable">$PRE_SEQ_LEN</span> \ <span class="comment"># 输入序列长度</span></span><br><span class="line">    --quantization_bit 4 <span class="comment"># 量化位数</span></span><br></pre></td></tr></table></figure>
<h5 id="evaluate-sh"><a href="#evaluate-sh" class="headerlink" title="evaluate.sh"></a>evaluate.sh</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># evaluate.sh</span></span><br><span class="line">PRE_SEQ_LEN=128 <span class="comment"># 模型输入的最大长度</span></span><br><span class="line">CHECKPOINT=adgen-chatglm-6b-pt-128-2e-2 <span class="comment"># checkpoint目录</span></span><br><span class="line">STEP=3000 <span class="comment"># 微调步数</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=0 python3 main.py</span><br><span class="line">    --do_predict \ <span class="comment"># 进行预测</span></span><br><span class="line">    --validation_file AdvertiseGen/dev.json \ <span class="comment"># 验证数据集文件名</span></span><br><span class="line">    --test_file AdvertiseGen/dev.json \ <span class="comment"># 测试数据集文件名</span></span><br><span class="line">    --overwrite_cache \ <span class="comment"># 覆盖缓存文件</span></span><br><span class="line">    --prompt_column content \ <span class="comment"># 输入列名</span></span><br><span class="line">    --response_column summary \ <span class="comment"># 输出列名</span></span><br><span class="line">    --model_name_or_path THUDM/chatglm-6b \ <span class="comment"># 模型名称或路径</span></span><br><span class="line">    --ptuning_checkpoint ./output/<span class="variable">$CHECKPOINT</span>/checkpoint-<span class="variable">$STEP</span> \ <span class="comment"># 微调后的checkpoint目录和步数</span></span><br><span class="line">    --output_dir ./output/<span class="variable">$CHECKPOINT</span> \ <span class="comment"># 输出目录</span></span><br><span class="line">    --overwrite_output_dir \ <span class="comment"># 覆盖输出目录</span></span><br><span class="line">    --max_source_length 64 \ <span class="comment"># 输入的最大长度</span></span><br><span class="line">    --max_target_length 64 \ <span class="comment"># 输出的最大长度</span></span><br><span class="line">    --per_device_eval_batch_size 1 \ <span class="comment"># 验证时每个GPU的batch size</span></span><br><span class="line">    --predict_with_generate \ <span class="comment"># 使用生成模式进行预测</span></span><br><span class="line">    --pre_seq_len <span class="variable">$PRE_SEQ_LEN</span> \ <span class="comment"># 输入序列长度</span></span><br><span class="line">    --quantization_bit 4 <span class="comment"># 量化位数</span></span><br></pre></td></tr></table></figure>
<h3 id="模型微调"><a href="#模型微调" class="headerlink" title="模型微调"></a>模型微调</h3><p>进入到ptuning目录，修改train.sh脚本，主要是修改其中的train_file、validation_file、model_name_or_path、output_dir参数：</p>
<ul>
<li>train_file：训练数据文件位置</li>
<li>validation_file：验证数据文件位置</li>
<li>model_name_or_path：原始ChatGLM-6B模型文件路径</li>
<li><p>output_dir：输出模型文件路径</p>
</li>
<li><p>模型训练速度过慢：修改增大batch_size</p>
</li>
</ul>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>修改evaluate.sh文件，修改model_name_or_path、ptuning_checkpoint等参数：</p>
<p>model_name_or_path：原始ChatGLM-6B模型文件路径<br>ptuning_checkpoint：训练完成后，生成的文件目录</p>
<h3 id="模型验证"><a href="#模型验证" class="headerlink" title="模型验证"></a>模型验证</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import torch</span><br><span class="line">from transformers import AutoConfig, AutoModel, AutoTokenizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MODEL_PATH &#x3D; &quot;.&#x2F;model&#x2F;chatglm-6b&quot;</span><br><span class="line">CHECKPOINT_PATH &#x3D; &quot;.&#x2F;output&#x2F;adgen-chatglm-6b-pt-128-2e-2&#x2F;checkpoint-1000&quot;</span><br><span class="line"></span><br><span class="line"># 载入Tokenizer</span><br><span class="line">tokenizer &#x3D; AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code&#x3D;True)</span><br><span class="line"></span><br><span class="line">config &#x3D; AutoConfig.from_pretrained(MODEL_PATH, trust_remote_code&#x3D;True, pre_seq_len&#x3D;128)</span><br><span class="line">model &#x3D; AutoModel.from_pretrained(MODEL_PATH, config&#x3D;config, trust_remote_code&#x3D;True).cuda()</span><br><span class="line"></span><br><span class="line">prefix_state_dict &#x3D; torch.load(os.path.join(CHECKPOINT_PATH, &quot;pytorch_model.bin&quot;))</span><br><span class="line">new_prefix_state_dict &#x3D; &#123;&#125;</span><br><span class="line"></span><br><span class="line">for k, v in prefix_state_dict.items():</span><br><span class="line">    if k.startswith(&quot;transformer.prefix_encoder.&quot;):</span><br><span class="line">        new_prefix_state_dict[k[len(&quot;transformer.prefix_encoder.&quot;):]] &#x3D; v</span><br><span class="line">model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)</span><br><span class="line"></span><br><span class="line">print(f&quot;Quantized to 4 bit&quot;)</span><br><span class="line">model &#x3D; model.quantize(4)</span><br><span class="line">model &#x3D; model.half().cuda()</span><br><span class="line">model.transformer.prefix_encoder.float()</span><br><span class="line">model &#x3D; model.eval()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(&quot;用户：你好\n&quot;)</span><br><span class="line">response, history &#x3D; model.chat(tokenizer, &quot;你好&quot;, history&#x3D;[])</span><br><span class="line">print(&quot;ChatGLM-6B：\n&quot;,response)</span><br><span class="line">print(&quot;\n------------------------------------------------\n用户：&quot;)</span><br><span class="line"></span><br><span class="line">line &#x3D; input()</span><br><span class="line">while line:</span><br><span class="line">    response, history &#x3D; model.chat(tokenizer, line, history&#x3D;history)</span><br><span class="line">    print(&quot;ChatGLM-6B：\n&quot;, response)</span><br><span class="line">    print(&quot;\n------------------------------------------------\n用户：&quot;)</span><br><span class="line">    line &#x3D; input()</span><br></pre></td></tr></table></figure>
<p>运行命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0 python3 inference.py</span><br></pre></td></tr></table></figure>
<h2 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h2><p>官方数据库中没有给出LoRA微调样例，使用Huggingface开源的 PEFT 大模型高效微调工具包(Parameter-Efficient Fine-Tuning)库，其中封装了LoRA这个方法（<br><a href="https://github.com/huggingface/peft）" target="_blank" rel="noopener">https://github.com/huggingface/peft）</a><br>但因为官方没有给出具体的样例，所以有许多版本（以下介绍采用的是比较多人引用的 <a href="https://github.com/hiyouga/ChatGLM-Efficient-Tuning" target="_blank" rel="noopener">https://github.com/hiyouga/ChatGLM-Efficient-Tuning</a> 版本）</p>
<h3 id="模型仓库"><a href="#模型仓库" class="headerlink" title="模型仓库"></a>模型仓库</h3><p><a href="https://github.com/yanqiangmiffy/InstructGLM" target="_blank" rel="noopener">https://github.com/yanqiangmiffy/InstructGLM</a><br><a href="https://github.com/mymusise/ChatGLM-Tuning" target="_blank" rel="noopener">https://github.com/mymusise/ChatGLM-Tuning</a><br><a href="https://github.com/hiyouga/ChatGLM-Efficient-Tuning" target="_blank" rel="noopener">https://github.com/hiyouga/ChatGLM-Efficient-Tuning</a><br>…</p>
<h3 id="方法简介"><a href="#方法简介" class="headerlink" title="方法简介"></a>方法简介</h3><p>LORA方法的原理是在微调大型语言模型时，通过精心设计的策略和技术手段，最大限度地提升模型在低资源环境下的性能。传统的微调方法可能需要大量的训练数据和计算资源，但在现实场景中，往往存在数据有限、计算资源有限的情况。因此，LORA的目标是克服这些限制，实现高效的低资源微调。</p>
<p>参考paper链接：<a href='https://arxiv.org/pdf/2106.09685.pdf' target="_blank" rel="noopener"> LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS </a></p>
<p>LORA(Low-Rank Adaptation)微调冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到 Transformer 架构的每一层，极大地减少了下游任务的可训练参数的数量。基于LORA的微调产生保存了新的权重，可以将生成的LORA权重认为是一个原来预训练模型的补丁权重 。所以LORA模型无法单独使用，需要搭配原模型，两者进行合并即可获得完整版权重。（后面会给出代码部分解释）</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>可使用的数据集，但是数据集的格式均需经过处理才可使用</p>
<h4 id="斯坦福52k英文指令数据"><a href="#斯坦福52k英文指令数据" class="headerlink" title="斯坦福52k英文指令数据"></a>斯坦福52k英文指令数据</h4><p>instruction:52K 条指令中的每一条都是唯一的,答案由text-davinci-003模型生成得到的</p>
<h4 id="BELLE项目生成的中文指令数据：0-5m-amp-1m"><a href="#BELLE项目生成的中文指令数据：0-5m-amp-1m" class="headerlink" title="BELLE项目生成的中文指令数据：0.5m&amp;1m"></a>BELLE项目生成的中文指令数据：0.5m&amp;1m</h4><p>百万数据：<a href="https://huggingface.co/datasets/BelleGroup/generated_train_1M_CN" target="_blank" rel="noopener">https://huggingface.co/datasets/BelleGroup/generated_train_1M_CN</a><br>生成方式基于种子prompt，调用openai的api生成中文指令</p>
<h4 id="GuanacoDataset-多语言指令数据集"><a href="#GuanacoDataset-多语言指令数据集" class="headerlink" title="GuanacoDataset 多语言指令数据集"></a>GuanacoDataset 多语言指令数据集</h4><p>Guanaco 是在 Meta 的 LLaMA 7B 模型上训练的指令跟随语言模型。在 Alpaca 模型原始 52K 数据的基础上，我们添加了额外的 98,369 个条目，涵盖英语、简体中文、繁体中文（台湾）、繁体中文（香港）、日语、德语以及各种语言和语法任务。通过使用这些丰富的数据重新训练和优化模型，Guanaco 在多语言环境中展示了出色的性能和潜力。项目链接可以查看 <a href="https://guanaco-model.github.io/" target="_blank" rel="noopener">https://guanaco-model.github.io/</a></p>
<h4 id="alpaca中文指令微调数据集"><a href="#alpaca中文指令微调数据集" class="headerlink" title="alpaca中文指令微调数据集"></a>alpaca中文指令微调数据集</h4><p>与原始alpaca数据json格式相同,数据生成的方法是机器翻译和self-instruct</p>
<h4 id="firefly-train-1-1M"><a href="#firefly-train-1-1M" class="headerlink" title="firefly-train-1.1M"></a>firefly-train-1.1M</h4><p>一份高质量的包含1.1M中文多任务指令微调数据集，包含23种常见的中文NLP任务的指令数据。对于每个任务，由人工书写若干指令模板，保证数据的高质量与丰富度。</p>
<h3 id="参数详解-1"><a href="#参数详解-1" class="headerlink" title="参数详解"></a>参数详解</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0 python src&#x2F;train_sft.py \ 使用第一块GPU</span><br><span class="line">    --do_train \ # 进行训练</span><br><span class="line">    --use_v2 \ # ?</span><br><span class="line">    --dataset self_cognition \ # 数据集名称</span><br><span class="line">    --finetuning_type lora \ # 微调类型</span><br><span class="line">    --lora_rank 32 \ # LoRA的rank</span><br><span class="line">    --output_dir cognition \ # 输出目录</span><br><span class="line">    --overwrite_cache \ # 覆盖缓存文件</span><br><span class="line">    --per_device_train_batch_size 2 \ # 训练时每个GPU的batch size，如果项目过慢，可以适当增大</span><br><span class="line">    --gradient_accumulation_steps 2 \ # 梯度累积步数</span><br><span class="line">    --lr_scheduler_type cosine \ # 学习率调度器类型</span><br><span class="line">    --logging_steps 10 \ # 日志输出步数间隔</span><br><span class="line">    --save_steps 1000 \ # 保存模型步数间隔</span><br><span class="line">    --warmup_steps 0 \ # warmup步数</span><br><span class="line">    --learning_rate 1e-3 \ # 学习率</span><br><span class="line">    --num_train_epochs 10.0 \ # 训练轮数</span><br><span class="line">    --fp16 \ # 是否使用fp16</span><br></pre></td></tr></table></figure>
<h3 id="模型微调-1"><a href="#模型微调-1" class="headerlink" title="模型微调"></a>模型微调</h3><p>具体可设置的主要参数包括：</p>
<p>dataset, 分词后的数据集，即在 data/ 地址下的文件夹名称<br>lora_rank, 设置 LoRA 的秩，推荐为4或8，显存够的话使用8<br>per_device_train_batch_size, 每块 GPU 上的 batch size,显存不大尽量1-2<br>gradient_accumulation_steps, 梯度累加，可以在不提升显存占用的情况下增大 batch size<br>save_steps, 多少步保存一次<br>save_total_limit, 保存多少个checkpoint<br>learning_rate, 学习率<br>output_dir, 模型文件保存地址</p>
<h3 id="模型加载和推理"><a href="#模型加载和推理" class="headerlink" title="模型加载和推理"></a>模型加载和推理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from peft import PeftModel</span><br><span class="line">from transformers import AutoTokenizer, AutoModel</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">device &#x3D; torch.device(1)</span><br><span class="line"># 加载原始 LLM</span><br><span class="line">model_path &#x3D; &quot;THUDM&#x2F;chatglm-6b&quot;</span><br><span class="line">model &#x3D; AutoModel.from_pretrained(model_path, trust_remote_code&#x3D;True).half().to(device)</span><br><span class="line">tokenizer &#x3D; AutoTokenizer.from_pretrained(model_path, trust_remote_code&#x3D;True)</span><br><span class="line">model.chat(tokenizer, &quot;你好&quot;, history&#x3D;[])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 给原始 LLM 安装上你的 LoRA tool</span><br><span class="line">model &#x3D; PeftModel.from_pretrained(model, &quot;model&#x2F;chatglm2_lora&quot;).half()</span><br><span class="line">model.chat(tokenizer, &quot;你好&quot;, history&#x3D;[])</span><br></pre></td></tr></table></figure>
<h3 id="模型验证-1"><a href="#模型验证-1" class="headerlink" title="模型验证"></a>模型验证</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0 \</span><br><span class="line">python src&#x2F;cli_demo.py \</span><br><span class="line">    --use_v2 \</span><br><span class="line">    --checkpoint_dir cognition</span><br></pre></td></tr></table></figure>
<h3 id="导出微调模型"><a href="#导出微调模型" class="headerlink" title="导出微调模型"></a>导出微调模型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python src&#x2F;export_model.py \</span><br><span class="line">    --use_v2 \</span><br><span class="line">    --checkpoint_dir cognition \</span><br><span class="line">    --output_dir .&#x2F;chatglm2_6b_lora</span><br></pre></td></tr></table></figure>
<h2 id="全参数微调"><a href="#全参数微调" class="headerlink" title="全参数微调"></a>全参数微调</h2><p>修改 ds_train_finetune.sh 脚本使用 DeepSpeed 进行全参数微调。</p>
<h3 id="DeepSpeed"><a href="#DeepSpeed" class="headerlink" title="DeepSpeed"></a>DeepSpeed</h3><p>DeepSpeed 是微软开源的深度学习训练优化工具，它可以帮助我们在单机多卡的环境下训练大型模型，同时还可以帮助我们在单机多卡的环境下训练大型模型。</p>
<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install deepspeed</span><br></pre></td></tr></table></figure>
<h4 id="使用详情"><a href="#使用详情" class="headerlink" title="使用详情"></a>使用详情</h4><p>官方文档 <a href="https://www.deepspeed.ai/getting-started" target="_blank" rel="noopener">https://www.deepspeed.ai/getting-started</a></p>
<h3 id="参数详解-2"><a href="#参数详解-2" class="headerlink" title="参数详解"></a>参数详解</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">LR&#x3D;1e-4 # 学习率</span><br><span class="line"></span><br><span class="line">MASTER_PORT&#x3D;$(shuf -n 1 -i 10000-65535) # 随机生成一个端口号</span><br><span class="line"></span><br><span class="line">deepspeed --num_gpus&#x3D;4 --master_port $MASTER_PORT main.py \ # 使用4块GPU，指定端口号</span><br><span class="line">    --deepspeed deepspeed.json \ # 指定deepspeed配置文件</span><br><span class="line">    --do_train \ # 进行训练</span><br><span class="line">    --train_file AdvertiseGen&#x2F;train.json \ # 训练数据集文件名</span><br><span class="line">    --test_file AdvertiseGen&#x2F;dev.json \ # 测试数据集文件名</span><br><span class="line">    --prompt_column content \ # 输入列名</span><br><span class="line">    --response_column summary \ # 输出列名</span><br><span class="line">    --overwrite_cache \ # 覆盖缓存文件</span><br><span class="line">    --model_name_or_path THUDM&#x2F;chatglm-6b \ # 模型名称或路径</span><br><span class="line">    --output_dir .&#x2F;output&#x2F;adgen-chatglm-6b-ft-$LR \ # 输出目录</span><br><span class="line">    --overwrite_output_dir \ # 覆盖输出目录</span><br><span class="line">    --max_source_length 64 \ # 输入的最大长度</span><br><span class="line">    --max_target_length 64 \ # 输出的最大长度</span><br><span class="line">    --per_device_train_batch_size 4 \ # 训练时每个GPU的batch size</span><br><span class="line">    --per_device_eval_batch_size 1 \ # 验证时每个GPU的batch size</span><br><span class="line">    --gradient_accumulation_steps 1 \ # 梯度累积步数</span><br><span class="line">    --predict_with_generate \ # 使用生成模式进行预测</span><br><span class="line">    --max_steps 5000 \ # 最大训练步数</span><br><span class="line">    --logging_steps 10 \ # 日志输出步数间隔</span><br><span class="line">    --save_steps 1000 \ # 保存模型步数间隔</span><br><span class="line">    --learning_rate $LR \ # 学习率</span><br><span class="line">    --fp16 \ # 是否使用fp16</span><br></pre></td></tr></table></figure>
<h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><ul>
<li>ChatGLM-6B P-Tuning v2 教程 <a href="https://zhuanlan.zhihu.com/p/619417296" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/619417296</a></li>
<li>ChatGLM-6B的P-Tuning微调详细步骤及结果验证 <a href="https://blog.csdn.net/zxd1435513775/article/details/130384164" target="_blank" rel="noopener">https://blog.csdn.net/zxd1435513775/article/details/130384164</a></li>
<li>使用DeepSpeed/P-Tuning v2对ChatGLM-6B进行微调 <a href="https://zhuanlan.zhihu.com/p/622351059" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/622351059</a></li>
<li>【微调】CHATGLM2-6B LoRA 微调 <a href="https://zhuanlan.zhihu.com/p/639581192" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/639581192</a></li>
<li>【教程】从 GLM 到 GLM-130B，到 ChatGLM-6B，一文讲透 <a href="https://modelnet.ai/modeldoc/d2fe7c2239244ba7bf28b816aafc45d3" target="_blank" rel="noopener">https://modelnet.ai/modeldoc/d2fe7c2239244ba7bf28b816aafc45d3</a></li>
<li>类ChatGPT模型LLaMA的解读与其微调：Alpaca-LoRA/Vicuna/BELLE <a href="https://blog.csdn.net/v_JULY_v/article/details/129709105" target="_blank" rel="noopener">https://blog.csdn.net/v_JULY_v/article/details/129709105</a></li>
<li>修改 ChatGLM2-6B 自我认知的 Lora 微调教程 <a href="https://engchina.blog.csdn.net/article/details/131492403" target="_blank" rel="noopener">https://engchina.blog.csdn.net/article/details/131492403</a></li>
<li>【教程】InstructGLM：基于ChatGLM-6B在指令数据集上进行微调 <a href="https://modelnet.ai/modeldoc/e9364590fcb0446b88f5ece94cf1fd1ev" target="_blank" rel="noopener">https://modelnet.ai/modeldoc/e9364590fcb0446b88f5ece94cf1fd1ev</a></li>
<li>【NLP修炼系列之玩转LLM】基于LORA的高效微调ChatGLM方法 <a href="https://zhuanlan.zhihu.com/p/632010770" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/632010770</a></li>
<li>大模型微调实践：ChatGLM-6B全参数微调 <a href="https://zhuanlan.zhihu.com/p/627788620" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/627788620</a></li>
</ul>

                
                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2023/07/06/OpenStreetMap学习笔记/" data-toggle="tooltip" data-placement="top" title="OpenStreetMap学习笔记">&larr; Previous Post
		<br>
		<span><font size="2" face="Calibri" color="grey">OpenStreetMap学习笔记</font></span>
	       </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2023/03/26/x86-上机实验/" data-toggle="tooltip" data-placement="top" title="x86 上机实验">Next Post &rarr;
		<br>
		<span><font size="2" face="Calibri" color="grey">x86 上机实验</font></span>
	        </a>
                    </li>
                    
                </ul>

                <!-- tip start -->
                

                
                <div class="comment_notes">
                    <p>
                        by Tan
                    </p>
                </div>
                
                <!-- tip end -->

                <!-- Music start-->
                
                <!-- Music end -->

                <!-- Sharing -->
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            
              <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#Chatglm-官网"><span class="toc-nav-text">Chatglm 官网</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#官方主页"><span class="toc-nav-text">官方主页</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#官方社区"><span class="toc-nav-text">官方社区</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#常用命令"><span class="toc-nav-text">常用命令</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#tar"><span class="toc-nav-text">tar</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#示范流程"><span class="toc-nav-text">示范流程</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#训练原理"><span class="toc-nav-text">训练原理</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#CHATGLM-6B-模型参数微调"><span class="toc-nav-text">CHATGLM - 6B 模型参数微调</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#模型地址"><span class="toc-nav-text">模型地址</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#P-Tuning-v2"><span class="toc-nav-text">P-Tuning v2</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#简介"><span class="toc-nav-text">简介</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#官方论文"><span class="toc-nav-text">官方论文</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#文件组织"><span class="toc-nav-text">文件组织</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#安装依赖"><span class="toc-nav-text">安装依赖</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#训练数据集"><span class="toc-nav-text">训练数据集</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#官方样例数据集"><span class="toc-nav-text">官方样例数据集</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#自己的数据集"><span class="toc-nav-text">自己的数据集</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#参数详解"><span class="toc-nav-text">参数详解</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#arguments-ModelArguments"><span class="toc-nav-text">arguments.ModelArguments</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#arguments-DataTrainingArguments"><span class="toc-nav-text">arguments.DataTrainingArguments</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Seq2SeqTrainingArguments"><span class="toc-nav-text">Seq2SeqTrainingArguments</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#具体-sh代码解析"><span class="toc-nav-text">具体.sh代码解析</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#train-sh"><span class="toc-nav-text">train.sh</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#evaluate-sh"><span class="toc-nav-text">evaluate.sh</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#模型微调"><span class="toc-nav-text">模型微调</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#模型评估"><span class="toc-nav-text">模型评估</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#模型验证"><span class="toc-nav-text">模型验证</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#LoRA"><span class="toc-nav-text">LoRA</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#模型仓库"><span class="toc-nav-text">模型仓库</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#方法简介"><span class="toc-nav-text">方法简介</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#数据集"><span class="toc-nav-text">数据集</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#斯坦福52k英文指令数据"><span class="toc-nav-text">斯坦福52k英文指令数据</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#BELLE项目生成的中文指令数据：0-5m-amp-1m"><span class="toc-nav-text">BELLE项目生成的中文指令数据：0.5m&amp;1m</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#GuanacoDataset-多语言指令数据集"><span class="toc-nav-text">GuanacoDataset 多语言指令数据集</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#alpaca中文指令微调数据集"><span class="toc-nav-text">alpaca中文指令微调数据集</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#firefly-train-1-1M"><span class="toc-nav-text">firefly-train-1.1M</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#参数详解-1"><span class="toc-nav-text">参数详解</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#模型微调-1"><span class="toc-nav-text">模型微调</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#模型加载和推理"><span class="toc-nav-text">模型加载和推理</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#模型验证-1"><span class="toc-nav-text">模型验证</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#导出微调模型"><span class="toc-nav-text">导出微调模型</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#全参数微调"><span class="toc-nav-text">全参数微调</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#DeepSpeed"><span class="toc-nav-text">DeepSpeed</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#安装"><span class="toc-nav-text">安装</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#使用详情"><span class="toc-nav-text">使用详情</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#参数详解-2"><span class="toc-nav-text">参数详解</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#参考文章"><span class="toc-nav-text">参考文章</span></a></li></ol>
            
          
          </div>
        </aside>
      
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
                        
                          <a class="tag" href="/tags/#暑期实习" title="暑期实习">暑期实习</a>
                        
                    </div>
                </section>
                

            </div>
			
			<div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">
				
				<br>
				<br>

				<script src="https://utteranc.es/client.js"
						repo="Master-Tan/Master-Tan.github.io"
						issue-term="pathname"
						label="Comment"
						theme="github-light"
						crossorigin="anonymous"
						async>
				</script>
			</div>
        </div>
    </div>
</article>




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>


<style  type="text/css">
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                
                    <li>
                        <a target="_blank"  href="https://github.com/Master-Tan">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="https://twitter.com/None">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="https://www.facebook.com/None">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Tan 2025 | Powered by 
                    <a href="https://github.com/Master-Tan/Master-Tan.github.io" target="_blank" rel="noopener">
                        <i>Tan's Blog</i>
                    </a>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://master-tan.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        
            <script type="text/javascript" src="/js/mouse-click.js" content='[&quot;Sunny&quot;, &quot;💖&quot;, &quot;Sunny&quot;, &quot;🧡&quot;, &quot;Sunny&quot;, &quot;💛&quot;, &quot;Sunny&quot; , &quot;💚&quot;, &quot;Sunny&quot;, &quot;💙&quot;, &quot;Sunny&quot;, &quot;💜&quot;, &quot;Sunny&quot;, &quot;😍&quot;]' color='[&quot;rgb(255, 0, 0)&quot; ,&quot;rgb(255, 0, 0)&quot; ,&quot;rgb(255, 125, 0)&quot; ,&quot;rgb(255, 125, 0)&quot; ,&quot;rgb(255, 255, 0)&quot; ,&quot;rgb(255, 255, 0)&quot; ,&quot;rgb(0, 255, 0)&quot; ,&quot;rgb(0, 255, 0)&quot; ,&quot;rgb(0, 255, 255)&quot; ,&quot;rgb(0, 255, 255)&quot; ,&quot;rgb(0, 0, 255)&quot; ,&quot;rgb(0, 0, 255)&quot; ,&quot;rgb(255, 0, 255)&quot; ,&quot;rgb(255, 0, 255)&quot;]'></script>
        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
