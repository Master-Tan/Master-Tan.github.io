<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="description" content="Tan&#39;s Blog">
    <meta name="keyword"  content="Hello world">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          ChatGLM学习笔记 - Tan&#39;s Blog
        
    </title>

    <link rel="canonical" href="https://master-tan.github.io/2023/07/01/ChatGLM学习笔记/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
        
<link rel="stylesheet" href="/css/dusign-light.css">

        
<link rel="stylesheet" href="/css/dusign-common-light.css">

        
<link rel="stylesheet" href="/css/font-awesome.css">

        
<link rel="stylesheet" href="/css/toc.css">

        <!-- background effects end -->
    
    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- photography -->
    
<link rel="stylesheet" href="/css/photography.css">


    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('../../../../img/default.jpg')
                /*post*/
            
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                        </div>
                        <h1>ChatGLM学习笔记</h1>
                        <h2 class="subheading">暑期实习笔记</h2>
                        <span class="meta">
                            Posted by Tan on
                            2023-07-01
                        </span>

	       
                            <div class="blank_box"></div>
                            <span class="meta">
                                 <span class="post-count">3k</span> Words
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-light.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-light.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Tan&#39;s Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        	<li>
                          	  <a href="/archive/">Archives</a>
                        	</li>
                        
                    

                        
                    

                        
                        	<li>
                          	  <a href="/about/">About Tan</a>
                        	</li>
                        
                    

                        
                        	<li>
                          	  <a href="/tags/">Tags</a>
                        	</li>
                        
                    

                        
                    

                        
                    

                        
                    
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h1 id="Chatglm-官网"><a href="#Chatglm-官网" class="headerlink" title="Chatglm 官网"></a>Chatglm 官网</h1><p><a href="https://chatglm.cn/" target="_blank" rel="noopener">https://chatglm.cn/</a></p>
<h2 id="官方主页"><a href="#官方主页" class="headerlink" title="官方主页"></a>官方主页</h2><p><a href="https://chatglm.cn/blog" target="_blank" rel="noopener">https://chatglm.cn/blog</a></p>
<h2 id="官方社区"><a href="#官方社区" class="headerlink" title="官方社区"></a>官方社区</h2><p><a href="https://modelnet.ai/home" target="_blank" rel="noopener">https://modelnet.ai/home</a></p>
<h1 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h1><h2 id="tar"><a href="#tar" class="headerlink" title="tar"></a>tar</h2><ul>
<li>参数 <code>-xvzf</code> : 用于<strong>解压缩</strong>一个 gzip 压缩的 tar 文件其中，-x 表示解包，-v 表示显示详细的解包过程，-z 表示解压缩 gzip 压缩，-f 表示指定要解压缩的文件名例如，tar -xvzf archive.tar.gz 将解压缩名为 archive.tar.gz 的 gzip 压缩的 tar 文件</li>
<li>参数 <code>-cvzf</code> : 用于<strong>压缩</strong>创建一个 gzip 压缩的 tar 文件其中，-c 表示创建归档文件，-v 表示显示详细的打包过程，-z 表示使用 gzip 压缩，-f 表示指定要创建的文件名例如，tar -cvzf archive.tar.gz folder/ 将创建名为 archive.tar.gz 的 gzip 压缩的 tar 文件，其中包含名为 folder 的目录下的所有文件和子目录</li>
<li>样例：  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -cvzf env.tar.gz anaconda3/envs/chatglm_tan <span class="comment"># 压缩</span></span><br><span class="line">tar -xvzf env.tar.gz -C anaconda3/envs/chatglm_tan <span class="comment"># 解压缩</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="示范流程"><a href="#示范流程" class="headerlink" title="示范流程"></a>示范流程</h1>   <!-- 65  30/06/23 08:59:42 ls
   66  30/06/23 08:59:49 vim test.txt 
   67  30/06/23 08:59:53 ls
   68  30/06/23 09:03:23 pwd
   69  30/06/23 09:04:15 ls
   70  30/06/23 09:04:17 cd chatglm2-6b/
   71  30/06/23 09:04:18 ls
   72  30/06/23 09:05:31 cd ../
   73  30/06/23 09:05:31 ls -->
<p>   74  30/06/23 09:05:40 tar -xvzf ChatGLM-Efficient-Tuning.tar.gz<br>   <!-- 75  30/06/23 09:05:49 ls
   76  30/06/23 09:06:10 cd ChatGLM-Efficient-Tuning/
   77  30/06/23 09:06:11 ls
   78  30/06/23 09:06:14 cd data/
   79  30/06/23 09:06:14 ls
   80  30/06/23 09:06:22 cd ../
   81  30/06/23 09:06:25 cd examples/
   82  30/06/23 09:06:25 ls
   83  30/06/23 09:06:29 vim train_poi.sh 
   84  30/06/23 09:09:05 cd
   85  30/06/23 09:09:25 tar -cvzf env.tar.gz ~/env
   86  30/06/23 09:09:29 tar -cvzf env.tar.gz 
   87  30/06/23 09:09:34 tar -xvzf env.tar.gz  --><br>   88  30/06/23 09:09:37 tar -xvzf env.tar.gz<br>   <!-- 89  30/06/23 09:09:44 tar -xvzf env.tar.gz ~/env
   90  30/06/23 09:09:53 tar -xvzf ~/env env.tar.gz
   91  30/06/23 09:09:59 tar -xvzf env.tar.gz
   92  30/06/23 09:10:04 ll -la
   93  30/06/23 09:31:22 ls -ll
   94  30/06/23 09:31:27 tar -xvzf env.tar.gz --><br>   95  30/06/23 09:34:58 slist<br>   96  30/06/23 09:35:08 sinfo<br>   97  30/06/23 09:35:54 conda activate chatglm_etuning<br>   <!-- 98  30/06/23 09:35:57 ls
   99  30/06/23 09:36:02 cd ChatGLM-Efficient-Tuning/
  100  30/06/23 09:36:02 ls
  101  30/06/23 09:36:05 cd examples/
  102  30/06/23 09:36:05 ls
  103  30/06/23 09:38:33 cd
  104  30/06/23 09:38:43 vim run.sh
  105  30/06/23 09:40:35 vim run.slurm
  106  30/06/23 09:42:49 pwd
  107  30/06/23 09:42:55 vim run.slurm
  108  30/06/23 09:43:23 cd ChatGLM-Efficient-Tuning/
  109  30/06/23 09:43:24 pwd
  110  30/06/23 09:43:32 cd
  111  30/06/23 09:43:35 vim run.slurm
  112  30/06/23 10:07:55 sbatch run.slurm 
  113  30/06/23 10:08:20 vim run.slurm
  114  30/06/23 10:08:34 sbatch run.slurm 
  115  30/06/23 10:08:41 vim run.slurm
  116  30/06/23 10:08:49 sbatch run.slurm 
  117  30/06/23 10:09:02 vim run.slurm --><br>  118  30/06/23 10:35:01 pip install deepseed<br>  <!-- 119  30/06/23 10:35:10 python
  120  30/06/23 10:37:44 vim run.slurm 
  121  30/06/23 10:47:59 pwd
  122  30/06/23 10:48:47 vim run.slurm 
  123  30/06/23 10:50:30 ll
  124  30/06/23 10:50:42 tar -xvzf env.tar.gz --><br>  125  30/06/23 10:55:45 accelerate config<br>  126  30/06/23 10:56:27 vim ~/.bashrc<br>  127  30/06/23 11:01:06 vim /gs/home/zhangzhibo/anaconda3/envs/chatglm_etuning/bin/accelerate<br>  128  30/06/23 11:01:34 accelerate config<br>  129  30/06/23 11:03:06 vim run.slurm<br>  130  30/06/23 11:03:15 sbatch run.slurm<br>  131  30/06/23 11:03:21 ls<br>  132  30/06/23 11:03:27 cat glm2.out<br>  133  30/06/23 11:03:33 tail -100f glm2.out<br>  134  30/06/23 11:03:37 tail -100f glm2.err<br>  135  30/06/23 11:03:57 conda init bash<br>  136  30/06/23 11:04:08 vim run.slurm<br>  137  30/06/23 11:05:48 scancel 8138036<br>  138  30/06/23 16:55:17 vim run.slurm<br>  139  30/06/23 16:55:52 conda activate chatglm-etuning<br>  140  30/06/23 16:55:58 conda info —envs<br>  <!-- 141  30/06/23 16:56:16 conda activate chatglm_etuning
  142  30/06/23 16:56:18 ls
  143  30/06/23 16:56:26 cd ChatGLM-Efficient-Tuning/
  144  30/06/23 16:56:28 cd ../ --><br>  145  30/06/23 16:56:34 sbatch run.slurm<br>  146  30/06/23 16:56:47 tail -100f glm2.out<br>  147  30/06/23 16:56:51 tail -100f glm2.err<br>  <!-- 148  30/06/23 16:57:02 scancel 8138197 --><br>  <!-- 149  30/06/23 16:57:12 vim run.slurm 
  150  30/06/23 16:57:24 sbatch run.slurm 
  151  30/06/23 16:57:29 tail -100f glm2.err 
  152  30/06/23 16:57:35 tail -100f glm2.out
  153  30/06/23 16:57:59 vim run.slurm 
  154  30/06/23 16:58:09 scancel 8138198
  155  30/06/23 16:58:20 accelerate config
  156  30/06/23 17:05:18 sbatch run.slurm 
  157  30/06/23 17:05:30 tail -100f glm2.err 
  158  30/06/23 17:05:56 tail -100f glm2.out
  159  30/06/23 17:06:05 tail f glm2.out
  160  30/06/23 17:06:09 tail -f glm2.out --><br>  161  30/06/23 17:06:18 tail -f glm2.out glm2.err<br>  <!-- 162  30/06/23 17:06:33 tail -100f glm2.err 
  163  30/06/23 17:06:39 cat glm2.err 
  164  30/06/23 17:07:19 scancel 8138205
  165  30/06/23 17:07:23 vim run.slurm 
  166  30/06/23 17:07:44 rm -rf glm2.out 
  167  30/06/23 17:07:47 rm -rf glm2.err 
  168  30/06/23 17:07:53 sbatch run.slurm 
  169  30/06/23 17:07:56 tail -f glm2.out glm2.err
  170  30/06/23 17:13:46 scancel 8138208
  171  30/06/23 17:13:48 vim run.slurm 
  172  30/06/23 17:14:20 sbatch run.slurm 
  173  30/06/23 17:14:57 tail -f glm2.out glm2.err
  174  30/06/23 17:25:18 scancel 8138214
  175  30/06/23 17:25:24 scancel 8138215
  176  30/06/23 17:25:35 accelerate config
  177  30/06/23 17:26:50 sbatch run.slurm 
  178  30/06/23 17:26:53 tail -f glm2.out glm2.err
  179  30/06/23 18:37:31 vim run.slurm 
  180  30/06/23 18:43:31 sbatch run.slurm 
  181  30/06/23 18:43:33 tail -f glm2.out glm2.err --></p>
<h1 id="CHATGLM-6B-模型参数微调"><a href="#CHATGLM-6B-模型参数微调" class="headerlink" title="CHATGLM - 6B 模型参数微调"></a>CHATGLM - 6B 模型参数微调</h1><h2 id="模型地址"><a href="#模型地址" class="headerlink" title="模型地址"></a>模型地址</h2><p><a href="https://github.com/THUDM/ChatGLM-6B" target="_blank" rel="noopener">https://github.com/THUDM/ChatGLM-6B</a></p>
<h2 id="P-Tuning-v2"><a href="#P-Tuning-v2" class="headerlink" title="P-Tuning v2"></a>P-Tuning v2</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>P-Tuning是一种较新的模型微调方法，它采用了参数剪枝的技术，可以将微调的参数量减少到原来的0.1%。具体来说，P-Tuning v2是基于P-Tuning v1的升级版，主要的改进在于采用了更加高效的剪枝方法，可以进一步减少模型微调的参数量。</p>
<p>P-Tuning v2的原理是通过对已训练好的大型语言模型进行参数剪枝，得到一个更加小巧、效率更高的轻量级模型。具体地，P-Tuning v2首先使用一种自适应的剪枝策略，对大型语言模型中的参数进行裁剪，去除其中不必要的冗余参数。然后，对于被剪枝的参数，P-Tuning v2使用了一种特殊的压缩方法，能够更加有效地压缩参数大小，并显著减少模型微调的总参数量。</p>
<p>总的来说，P-Tuning v2的核心思想是让模型变得更加轻便、更加高效，同时尽可能地保持模型的性能不受影响。这不仅可以加快模型的训练和推理速度，还可以减少模型在使用过程中的内存和计算资源消耗，让模型更适用于各种实际应用场景中。</p>
<p>对于 ChatGLM-6B 模型基于 P-Tuning v2 进行微调。可将需要微调的参数量减少到原来的 0.1%，再通过模型量化、Gradient Checkpoint 等方法，最低只需要 7GB 显存即可运行。</p>
<p><img src='./1226f28683ec4b7093cea03868a3ea28.jpeg'></p>
<h3 id="官方论文"><a href="#官方论文" class="headerlink" title="官方论文"></a>官方论文</h3><p><a href="https://arxiv.org/pdf/2110.07602.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2110.07602.pdf</a><br><a href="./2110.07602.pdf"> 查看 </a></p>
<h3 id="文件组织"><a href="#文件组织" class="headerlink" title="文件组织"></a>文件组织</h3><p>ChatGLM/ptuning/<br>├── arguments.py - 定义了模型、数据和训练相关参数的类<br>├── deepspeed.json - Deepspeed配置文件<br>├── ds_train_finetune.sh - 使用Deepspeed进行微调的shell脚本<br>├── evaluate_finetune.sh - 用于评估微调后的模型的shell脚本<br>├── evaluate.sh - 用于评估模型的shell脚本<br>├── main.py - 解析命令行参数并调用相应的训练或预测逻辑<br>├── README.md<br>├── README_en.md<br>├── trainer.py - 定义了模型训练和验证的主要逻辑<br>├── trainer_seq2seq.py - 继承自trainer.py，用于处理序列到序列模型的训练和验证<br>├── train_chat.sh - 用于训练chat模型的shell脚本，它调用main.py并传递一些参数来启动训练过程<br>├── train.sh - 用于训练模型的shell脚本<br>├── web_demo.py - Web Demo的主要逻辑<br>└── web_demo.sh - 启动Web Demo的shell脚本</p>
<h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h3><p>运行微调需要 4.27.1 版本的 transformers<br>pip install rouge_chinese nltk jieba datasets</p>
<h3 id="训练数据集"><a href="#训练数据集" class="headerlink" title="训练数据集"></a>训练数据集</h3><h4 id="官方样例数据集"><a href="#官方样例数据集" class="headerlink" title="官方样例数据集"></a>官方样例数据集</h4><p>官方微调样例是以 ADGEN (广告生成) 数据集为例来介绍微调的具体使用。</p>
<p>ADGEN 数据集为根据输入（content）生成一段广告词（summary），具体格式如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;content&quot;: &quot;类型#上衣*版型#宽松*版型#显瘦*图案#线条*衣样式#衬衫*衣袖型#泡泡袖*衣款式#抽绳&quot;,</span><br><span class="line">    &quot;summary&quot;: &quot;这件衬衫的款式非常的宽松，利落的线条可以很好的隐藏身材上的小缺点，穿在身上有着很好的显瘦效果。领口装饰了一个可爱的抽绳，漂亮的绳结展现出了十足的个性，配合时尚的泡泡袖型，尽显女性甜美可爱的气息。&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>将 ADGEN 数据集放到 ptuning 目录下并将其解压到 AdvertiseGen 目录</p>
<h4 id="自己的数据集"><a href="#自己的数据集" class="headerlink" title="自己的数据集"></a>自己的数据集</h4><p>修改 train.sh 和 evaluate.sh 中的 train_file、validation_file和test_file为你自己的 JSON 格式数据集路径，并将 prompt_column 和 response_column 改为 JSON 文件中输入文本和输出文本对应的 KEY</p>
<h3 id="参数详解"><a href="#参数详解" class="headerlink" title="参数详解"></a>参数详解</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ptuning/main.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) == <span class="number">2</span> <span class="keyword">and</span> sys.argv[<span class="number">1</span>].endswith(<span class="string">".json"</span>):</span><br><span class="line">        <span class="comment"># If we pass only one argument to the script and it's the path to a json file,</span></span><br><span class="line">        <span class="comment"># let's parse it to get our arguments.</span></span><br><span class="line">        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model_args, data_args, training_args = parser.parse_args_into_dataclasses()</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>在以上代码中可知，原参数分为三种，分别为ModelArguments（模型相关参数）、DataTrainingArguments（数据相关参数）和Seq2SeqTrainingArguments（训练相关参数）</p>
<h4 id="arguments-ModelArguments"><a href="#arguments-ModelArguments" class="headerlink" title="arguments.ModelArguments"></a>arguments.ModelArguments</h4><p>用于指定模型的参数</p>
<ul>
<li><p><code>model_name_or_path</code> (str):</p>
<ul>
<li>描述：预训练模型或模型标识符的路径，可以是 huggingface.co/models 上的模型。</li>
<li>默认值：无</li>
</ul>
</li>
<li><p><code>ptuning_checkpoint</code> (str):</p>
<ul>
<li>描述：p-tuning v2 检查点的路径。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>config_name</code> (Optional[str]):</p>
<ul>
<li>描述：预训练配置的名称或路径，如果不同于 <code>model_name_or_path</code>。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>tokenizer_name</code> (Optional[str]):</p>
<ul>
<li>描述：预训练 tokenizer 的名称或路径，如果不同于 <code>model_name_or_path</code>。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>cache_dir</code> (Optional[str]):</p>
<ul>
<li>描述：存储从 huggingface.co 下载的预训练模型的目录。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>use_fast_tokenizer</code> (bool):</p>
<ul>
<li>描述：是否使用由 tokenizers 库支持的快速 tokenizer。</li>
<li>默认值：True</li>
</ul>
</li>
<li><p><code>model_revision</code> (str):</p>
<ul>
<li>描述：要使用的具体模型版本（可以是分支名称、标签名称或提交 ID）。</li>
<li>默认值：”main”</li>
</ul>
</li>
<li><p><code>use_auth_token</code> (bool):</p>
<ul>
<li>描述：是否使用运行 <code>huggingface-cli login</code> 时生成的令牌（用于使用私有模型）。</li>
<li>默认值：False</li>
</ul>
</li>
<li><p><code>resize_position_embeddings</code> (Optional[bool]):</p>
<ul>
<li>描述：是否在 <code>max_source_length</code> 超过模型位置嵌入时自动调整位置嵌入大小。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>quantization_bit</code> (Optional[int]):</p>
<ul>
<li>描述：量化的位数。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>pre_seq_len</code> (Optional[int]):</p>
<ul>
<li>描述：预处理的序列长度。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>prefix_projection</code> (bool):</p>
<ul>
<li>描述：是否对输入文本应用前缀投影。</li>
<li>默认值：False</li>
</ul>
</li>
</ul>
<h4 id="arguments-DataTrainingArguments"><a href="#arguments-DataTrainingArguments" class="headerlink" title="arguments.DataTrainingArguments"></a>arguments.DataTrainingArguments</h4><p>用于指定训练和评估数据的参数</p>
<ul>
<li><p><code>lang</code> (Optional[str]):</p>
<ul>
<li>描述：摘要的语言 ID。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>dataset_name</code> (Optional[str]):</p>
<ul>
<li>描述：要使用的数据集名称（通过 datasets 库）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>dataset_config_name</code> (Optional[str]):</p>
<ul>
<li>描述：要使用的数据集配置名称（通过 datasets 库）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>prompt_column</code> (Optional[str]):</p>
<ul>
<li>描述：数据集中包含完整文本的列名（用于摘要）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>response_column</code> (Optional[str]):</p>
<ul>
<li>描述：数据集中包含摘要的列名（用于摘要）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>history_column</code> (Optional[str]):</p>
<ul>
<li>描述：数据集中包含聊天历史的列名。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>train_file</code> (Optional[str]):</p>
<ul>
<li>描述：输入的训练数据文件（jsonlines 或 csv 文件）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>validation_file</code> (Optional[str]):</p>
<ul>
<li>描述：用于评估指标（rouge）的可选输入评估数据文件（jsonlines 或 csv 文件）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>test_file</code> (Optional[str]):</p>
<ul>
<li>描述：用于评估指标（rouge）的可选输入测试数据文件（jsonlines 或 csv 文件）。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>overwrite_cache</code> (bool):</p>
<ul>
<li>描述：是否覆盖缓存的训练和评估集。</li>
<li>默认值：False</li>
</ul>
</li>
<li><p><code>preprocessing_num_workers</code> (Optional[int]):</p>
<ul>
<li>描述：用于预处理的进程数。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>max_source_length</code> (Optional[int]):</p>
<ul>
<li>描述：标记化后的最大输入序列长度。超过此长度的序列将被截断，较短的序列将被填充。</li>
<li>默认值：1024</li>
</ul>
</li>
<li><p><code>max_target_length</code> (Optional[int]):</p>
<ul>
<li>描述：标记化后的目标文本的最大序列长度。超过此长度的序列将被截断，较短的序列将被填充。</li>
<li>默认值：128</li>
</ul>
</li>
<li><p><code>val_max_target_length</code> (Optional[int]):</p>
<ul>
<li>描述：验证目标文本的最大序列长度。超过此长度的序列将被截断，较短的序列将被填充。默认为 <code>max_target_length</code>。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>pad_to_max_length</code> (bool):</p>
<ul>
<li>描述：是否将所有样本填充到模型的最大句子长度。如果为 False，将在批处理时动态地将样本填充到批次中的最大长度。在 GPU 上更高效，但对 TPU 来说非常低效。</li>
<li>默认值：False</li>
</ul>
</li>
<li><p><code>max_train_samples</code> (Optional[int]):</p>
<ul>
<li>描述：为调试或更快的训练而将训练示例的数量截断到该值。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>max_eval_samples</code> (Optional[int]):</p>
<ul>
<li>描述：为调试或更快的训练而将评估示例的数量截断到该值。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>max_predict_samples</code> (Optional[int]):</p>
<ul>
<li>描述：为调试或更快的训练而将预测示例的数量截断到该值。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>num_beams</code> (Optional[int]):</p>
<ul>
<li>描述：用于评估的 beam 数。此参数将传递给 <code>model.generate</code>，用于 <code>evaluate</code> 和 <code>predict</code>。</li>
<li>默认值：None</li>
</ul>
</li>
<li><p><code>ignore_pad_token_for_loss</code> (bool):</p>
<ul>
<li>描述：是否在损失计算中忽略与填充标签对应的令牌。</li>
<li>默认值：True</li>
</ul>
</li>
<li><p><code>source_prefix</code> (Optional[str]):</p>
<ul>
<li>描述：在每个源文本之前要添加的前缀（适用于 T5 模型）。</li>
<li>默认值：””（空字符串）</li>
</ul>
</li>
<li><p><code>forced_bos_token</code> (Optional[str]):</p>
<ul>
<li>描述：在 <code>decoder_start_token_id</code> 之后强制作为第一个生成的令牌的令牌。对于类似 mBART 的多语言模型很有用，其中第一个生成的令牌需要是目标语言令牌（通常是目标语言令牌）。</li>
<li>默认值：Nones</li>
</ul>
</li>
</ul>
<h4 id="Seq2SeqTrainingArguments"><a href="#Seq2SeqTrainingArguments" class="headerlink" title="Seq2SeqTrainingArguments"></a>Seq2SeqTrainingArguments</h4><ul>
<li>此类为transformers库中的Seq2SeqTrainingArguments类，用于指定训练相关的参数，具体参数可参考transformers库中的Seq2SeqTrainingArguments类的注释</li>
</ul>
<h4 id="具体-sh代码解析"><a href="#具体-sh代码解析" class="headerlink" title="具体.sh代码解析"></a>具体.sh代码解析</h4><h5 id="train-sh"><a href="#train-sh" class="headerlink" title="train.sh"></a>train.sh</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ptuning/train.sh</span></span><br><span class="line">PRE_SEQ_LEN=128 <span class="comment"># 模型输入的最大长度</span></span><br><span class="line">LR=2e-2 <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=0 python3 main.py</span><br><span class="line">    --do_train \ <span class="comment"># 进行训练</span></span><br><span class="line">    --train_file AdvertiseGen/train.json \ <span class="comment"># 训练数据集文件名</span></span><br><span class="line">    --validation_file AdvertiseGen/dev.json \ <span class="comment"># 验证数据集文件名</span></span><br><span class="line">    --prompt_column content \ <span class="comment"># 输入列名</span></span><br><span class="line">    --response_column summary \ <span class="comment"># 输出列名</span></span><br><span class="line">    --overwrite_cache \ <span class="comment"># 覆盖缓存文件</span></span><br><span class="line">    --model_name_or_path THUDM/chatglm-6b \ <span class="comment"># 模型名称或路径</span></span><br><span class="line">    --output_dir output/adgen-chatglm-6b-pt-<span class="variable">$PRE_SEQ_LEN</span>-<span class="variable">$LR</span> \ <span class="comment"># 输出目录</span></span><br><span class="line">    --overwrite_output_dir \ <span class="comment"># 覆盖输出目录</span></span><br><span class="line">    --max_source_length 64 \ <span class="comment"># 输入的最大长度</span></span><br><span class="line">    --max_target_length 64 \ <span class="comment"># 输出的最大长度</span></span><br><span class="line">    --per_device_train_batch_size 1 \ <span class="comment"># 训练时每个GPU的batch size</span></span><br><span class="line">    --per_device_eval_batch_size 1 \ <span class="comment"># 验证时每个GPU的batch size</span></span><br><span class="line">    --gradient_accumulation_steps 16 \ <span class="comment"># 梯度累积步数</span></span><br><span class="line">    --predict_with_generate \ <span class="comment"># 使用生成模式进行预测</span></span><br><span class="line">    --max_steps 3000 \ <span class="comment"># 最大训练步数</span></span><br><span class="line">    --logging_steps 10 \ <span class="comment"># 日志输出步数间隔</span></span><br><span class="line">    --save_steps 1000 \ <span class="comment"># 保存模型步数间隔</span></span><br><span class="line">    --learning_rate <span class="variable">$LR</span> \ <span class="comment"># 学习率</span></span><br><span class="line">    --pre_seq_len <span class="variable">$PRE_SEQ_LEN</span> \ <span class="comment"># 输入序列长度</span></span><br><span class="line">    --quantization_bit 4 <span class="comment"># 量化位数</span></span><br></pre></td></tr></table></figure>
<h5 id="evaluate-sh"><a href="#evaluate-sh" class="headerlink" title="evaluate.sh"></a>evaluate.sh</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># evaluate.sh</span></span><br><span class="line">PRE_SEQ_LEN=128 <span class="comment"># 模型输入的最大长度</span></span><br><span class="line">CHECKPOINT=adgen-chatglm-6b-pt-128-2e-2 <span class="comment"># checkpoint目录</span></span><br><span class="line">STEP=3000 <span class="comment"># 微调步数</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=0 python3 main.py</span><br><span class="line">    --do_predict \ <span class="comment"># 进行预测</span></span><br><span class="line">    --validation_file AdvertiseGen/dev.json \ <span class="comment"># 验证数据集文件名</span></span><br><span class="line">    --test_file AdvertiseGen/dev.json \ <span class="comment"># 测试数据集文件名</span></span><br><span class="line">    --overwrite_cache \ <span class="comment"># 覆盖缓存文件</span></span><br><span class="line">    --prompt_column content \ <span class="comment"># 输入列名</span></span><br><span class="line">    --response_column summary \ <span class="comment"># 输出列名</span></span><br><span class="line">    --model_name_or_path THUDM/chatglm-6b \ <span class="comment"># 模型名称或路径</span></span><br><span class="line">    --ptuning_checkpoint ./output/<span class="variable">$CHECKPOINT</span>/checkpoint-<span class="variable">$STEP</span> \ <span class="comment"># 微调后的checkpoint目录和步数</span></span><br><span class="line">    --output_dir ./output/<span class="variable">$CHECKPOINT</span> \ <span class="comment"># 输出目录</span></span><br><span class="line">    --overwrite_output_dir \ <span class="comment"># 覆盖输出目录</span></span><br><span class="line">    --max_source_length 64 \ <span class="comment"># 输入的最大长度</span></span><br><span class="line">    --max_target_length 64 \ <span class="comment"># 输出的最大长度</span></span><br><span class="line">    --per_device_eval_batch_size 1 \ <span class="comment"># 验证时每个GPU的batch size</span></span><br><span class="line">    --predict_with_generate \ <span class="comment"># 使用生成模式进行预测</span></span><br><span class="line">    --pre_seq_len <span class="variable">$PRE_SEQ_LEN</span> \ <span class="comment"># 输入序列长度</span></span><br><span class="line">    --quantization_bit 4 <span class="comment"># 量化位数</span></span><br></pre></td></tr></table></figure>
<!-- ##### 一次微调中的全部参数
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line">07/03/2023 21:31:55 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(</span><br><span class="line">_n_gpu=1, <span class="comment"># GPU数量</span></span><br><span class="line">adafactor=False, <span class="comment"># 是否使用adafactor优化器</span></span><br><span class="line">adam_beta1=0.9, <span class="comment"># adam优化器的beta1参数</span></span><br><span class="line">adam_beta2=0.999, <span class="comment"># adam优化器的beta2参数</span></span><br><span class="line">adam_epsilon=1e-08, <span class="comment"># adam优化器的epsilon参数</span></span><br><span class="line">auto_find_batch_size=False, <span class="comment"># 是否自动寻找最大的batch size </span></span><br><span class="line">bf16=False, <span class="comment"># 是否使用bf16</span></span><br><span class="line">bf16_full_eval=False, <span class="comment"># 是否在评估时使用bf16</span></span><br><span class="line">data_seed=None, <span class="comment"># 数据种子</span></span><br><span class="line">dataloader_drop_last=False, <span class="comment"># dataloader是否丢弃最后一个batch</span></span><br><span class="line">dataloader_num_workers=0, <span class="comment"># dataloader的进程数</span></span><br><span class="line">dataloader_pin_memory=True, <span class="comment"># dataloader是否使用pin_memory</span></span><br><span class="line">ddp_bucket_cap_mb=None, <span class="comment"># ddp的bucket容量</span></span><br><span class="line">ddp_find_unused_parameters=None, <span class="comment"># ddp是否寻找未使用的参数</span></span><br><span class="line">ddp_timeout=1800, <span class="comment"># ddp超时时间</span></span><br><span class="line">debug=[], <span class="comment"># debug模式</span></span><br><span class="line">deepspeed=None, <span class="comment"># deepspeed配置</span></span><br><span class="line">disable_tqdm=False, <span class="comment"># 是否禁用tqdm</span></span><br><span class="line">do_eval=False, <span class="comment"># 是否进行评估</span></span><br><span class="line">do_predict=False, <span class="comment"># 是否进行预测</span></span><br><span class="line">do_train=True, <span class="comment"># 是否进行训练</span></span><br><span class="line">eval_accumulation_steps=None, <span class="comment"># 评估累积步数</span></span><br><span class="line">eval_delay=0, <span class="comment"># 评估延迟</span></span><br><span class="line">eval_steps=None, <span class="comment"># 评估步数</span></span><br><span class="line">evaluation_strategy=no, <span class="comment"># 评估策略</span></span><br><span class="line">fp16=False, <span class="comment"># 是否使用fp16</span></span><br><span class="line">fp16_backend=auto, <span class="comment"># fp16后端</span></span><br><span class="line">fp16_full_eval=False, <span class="comment"># 是否在评估时使用fp16</span></span><br><span class="line">fp16_opt_level=O1, <span class="comment"># fp16优化级别</span></span><br><span class="line">fsdp=[], <span class="comment"># fsdp配置</span></span><br><span class="line">fsdp_config=&#123;<span class="string">'fsdp_min_num_params'</span>: 0, <span class="string">'xla'</span>: False, <span class="string">'xla_fsdp_grad_ckpt'</span>: False&#125;, <span class="comment"># fsdp配置</span></span><br><span class="line">fsdp_min_num_params=0, <span class="comment"># fsdp最小参数数</span></span><br><span class="line">fsdp_transformer_layer_cls_to_wrap=None, <span class="comment"># fsdp转换器层类</span></span><br><span class="line">full_determinism=False, <span class="comment"># 是否完全确定性</span></span><br><span class="line">generation_max_length=None, <span class="comment"># 生成的最大长度</span></span><br><span class="line">generation_num_beams=None, <span class="comment"># 生成的beam数</span></span><br><span class="line">gradient_accumulation_steps=16, <span class="comment"># 梯度累积步数</span></span><br><span class="line">gradient_checkpointing=False, <span class="comment"># 是否使用梯度检查点</span></span><br><span class="line">greater_is_better=None, <span class="comment"># ?</span></span><br><span class="line">group_by_length=False, <span class="comment"># 是否按长度分组</span></span><br><span class="line">half_precision_backend=auto, <span class="comment"># 半精度后端</span></span><br><span class="line">hub_model_id=None, <span class="comment"># hub模型id</span></span><br><span class="line">hub_private_repo=False, <span class="comment"># hub是否为私有仓库</span></span><br><span class="line">hub_strategy=every_save, <span class="comment"># hub策略</span></span><br><span class="line">hub_token=&lt;HUB_TOKEN&gt;,  <span class="comment"># hub token</span></span><br><span class="line">ignore_data_skip=False, <span class="comment"># ?</span></span><br><span class="line">include_inputs_for_metrics=False, <span class="comment"># 是否包含输入用于评估</span></span><br><span class="line">jit_mode_eval=False, <span class="comment"># 是否在评估时使用jit模式</span></span><br><span class="line">label_names=None, <span class="comment"># 标签名称</span></span><br><span class="line">label_smoothing_factor=0.0, <span class="comment"># 标签平滑因子</span></span><br><span class="line">learning_rate=0.02, <span class="comment"># 学习率</span></span><br><span class="line">length_column_name=length, <span class="comment"># 长度列名</span></span><br><span class="line">load_best_model_at_end=False, <span class="comment"># 是否在训练结束时加载最佳模型</span></span><br><span class="line">local_rank=-1, <span class="comment"># 本地rank</span></span><br><span class="line">log_level=passive, <span class="comment"># 日志级别</span></span><br><span class="line">log_level_replica=warning, <span class="comment"># 日志级别</span></span><br><span class="line">log_on_each_node=True, <span class="comment"># 是否在每个节点上记录</span></span><br><span class="line">logging_dir=output/adgen-chatglm-6b-pt-128-2e-2/runs/Jul03_21-31-54_BSC-4, <span class="comment"># 日志目录</span></span><br><span class="line">logging_first_step=False, <span class="comment"># 是否在第一步记录日志</span></span><br><span class="line">logging_nan_inf_filter=True, <span class="comment"># 是否过滤nan和inf</span></span><br><span class="line">logging_steps=10, <span class="comment"># 日志输出步数间隔</span></span><br><span class="line">logging_strategy=steps, <span class="comment"># 日志策略</span></span><br><span class="line">lr_scheduler_type=linear, <span class="comment"># 学习率调度器类型</span></span><br><span class="line">max_grad_norm=1.0, <span class="comment"># 最大梯度范数</span></span><br><span class="line">max_steps=3000, <span class="comment"># 最大训练步数</span></span><br><span class="line">metric_for_best_model=None, <span class="comment"># 用于最佳模型的指标</span></span><br><span class="line">mp_parameters=, <span class="comment"># mp参数</span></span><br><span class="line">no_cuda=False, <span class="comment"># 是否禁用cuda</span></span><br><span class="line">num_train_epochs=3.0, <span class="comment"># 训练轮数</span></span><br><span class="line">optim=adamw_hf, <span class="comment"># 优化器</span></span><br><span class="line">optim_args=None, <span class="comment"># 优化器参数</span></span><br><span class="line">output_dir=output/adgen-chatglm-6b-pt-128-2e-2, <span class="comment"># 输出目录</span></span><br><span class="line">overwrite_output_dir=True, <span class="comment"># 是否覆盖输出目录</span></span><br><span class="line">past_index=-1, <span class="comment"># ?</span></span><br><span class="line">per_device_eval_batch_size=1, <span class="comment"># 验证时每个GPU的batch size</span></span><br><span class="line">per_device_train_batch_size=1, <span class="comment"># 训练时每个GPU的batch size</span></span><br><span class="line">predict_with_generate=True, <span class="comment"># 是否使用生成模式进行预测</span></span><br><span class="line">prediction_loss_only=False, <span class="comment"># 是否仅计算预测损失</span></span><br><span class="line">push_to_hub=False, <span class="comment"># 是否推送到hub</span></span><br><span class="line">push_to_hub_model_id=None, <span class="comment"># 推送到hub的模型id</span></span><br><span class="line">push_to_hub_organization=None, <span class="comment"># 推送到hub的组织</span></span><br><span class="line">push_to_hub_token=&lt;PUSH_TO_HUB_TOKEN&gt;, <span class="comment"># 推送到hub的token</span></span><br><span class="line">ray_scope=last, <span class="comment"># ray范围</span></span><br><span class="line">remove_unused_columns=True, <span class="comment"># 是否删除未使用的列</span></span><br><span class="line">report_to=[], <span class="comment"># 报告</span></span><br><span class="line">resume_from_checkpoint=None, <span class="comment"># 从检查点恢复</span></span><br><span class="line">run_name=output/adgen-chatglm-6b-pt-128-2e-2, <span class="comment"># 运行名称</span></span><br><span class="line">save_on_each_node=False, <span class="comment"># 是否在每个节点上保存</span></span><br><span class="line">save_steps=1000, <span class="comment"># 保存模型步数间隔</span></span><br><span class="line">save_strategy=steps, <span class="comment"># 保存策略</span></span><br><span class="line">save_total_limit=None, <span class="comment"># 保存的总数限制</span></span><br><span class="line">seed=42, <span class="comment"># ?</span></span><br><span class="line">sharded_ddp=[], <span class="comment"># sharded_ddp配置</span></span><br><span class="line">skip_memory_metrics=True, <span class="comment"># 是否跳过内存指标</span></span><br><span class="line">sortish_sampler=False, <span class="comment"># 是否使用sortish sampler</span></span><br><span class="line">tf32=None, <span class="comment"># 是否使用tf32</span></span><br><span class="line">torch_compile=False, <span class="comment"># 是否使用torch编译</span></span><br><span class="line">torch_compile_backend=None, <span class="comment"># torch编译后端</span></span><br><span class="line">torch_compile_mode=None, <span class="comment"># torch编译模式</span></span><br><span class="line">torchdynamo=None, <span class="comment"># torchdynamo配置</span></span><br><span class="line">tpu_metrics_debug=False, <span class="comment"># 是否启用tpu指标调试</span></span><br><span class="line">tpu_num_cores=None, <span class="comment"># tpu核心数</span></span><br><span class="line">use_ipex=False, <span class="comment"># 是否使用ipex</span></span><br><span class="line">use_legacy_prediction_loop=False, <span class="comment"># 是否使用旧的预测循环</span></span><br><span class="line">use_mps_device=False, <span class="comment"># 是否使用mps设备</span></span><br><span class="line">warmup_ratio=0.0, <span class="comment"># warmup比例</span></span><br><span class="line">warmup_steps=0, <span class="comment"># warmup步数</span></span><br><span class="line">weight_decay=0.0, <span class="comment"># 权重衰减</span></span><br><span class="line">xpu_backend=None, <span class="comment"># xpu后端</span></span><br><span class="line">)</span><br><span class="line">``` --&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">### 模型微调</span></span><br><span class="line">进入到ptuning目录，修改train.sh脚本，主要是修改其中的train_file、validation_file、model_name_or_path、output_dir参数：</span><br><span class="line"></span><br><span class="line">- train_file：训练数据文件位置</span><br><span class="line">- validation_file：验证数据文件位置</span><br><span class="line">- model_name_or_path：原始ChatGLM-6B模型文件路径</span><br><span class="line">- output_dir：输出模型文件路径</span><br><span class="line"></span><br><span class="line">- 模型训练速度过慢：修改增大batch_size</span><br><span class="line"></span><br><span class="line"><span class="comment">### 模型评估</span></span><br><span class="line">修改evaluate.sh文件，修改model_name_or_path、ptuning_checkpoint等参数：</span><br><span class="line"></span><br><span class="line">model_name_or_path：原始ChatGLM-6B模型文件路径</span><br><span class="line">ptuning_checkpoint：训练完成后，生成的文件目录</span><br><span class="line"></span><br><span class="line"><span class="comment">### 模型验证</span></span><br></pre></td></tr></table></figure>
<p>import os<br>import torch<br>from transformers import AutoConfig, AutoModel, AutoTokenizer</p>
<p>MODEL_PATH = “./model/chatglm-6b”<br>CHECKPOINT_PATH = “./output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-1000”</p>
<h1 id="载入Tokenizer"><a href="#载入Tokenizer" class="headerlink" title="载入Tokenizer"></a>载入Tokenizer</h1><p>tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)</p>
<p>config = AutoConfig.from_pretrained(MODEL_PATH, trust_remote_code=True, pre_seq_len=128)<br>model = AutoModel.from_pretrained(MODEL_PATH, config=config, trust_remote_code=True).cuda()</p>
<p>prefix_state_dict = torch.load(os.path.join(CHECKPOINT_PATH, “pytorch_model.bin”))<br>new_prefix_state_dict = {}</p>
<p>for k, v in prefix_state_dict.items():<br>    if k.startswith(“transformer.prefix_encoder.”):<br>        new_prefix_state_dict[k[len(“transformer.prefix_encoder.”):]] = v<br>model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)</p>
<p>print(f”Quantized to 4 bit”)<br>model = model.quantize(4)<br>model = model.half().cuda()<br>model.transformer.prefix_encoder.float()<br>model = model.eval()</p>
<p>print(“用户：你好\n”)<br>response, history = model.chat(tokenizer, “你好”, history=[])<br>print(“ChatGLM-6B：\n”,response)<br>print(“\n————————————————————————\n用户：”)</p>
<p>line = input()<br>while line:<br>    response, history = model.chat(tokenizer, line, history=history)<br>    print(“ChatGLM-6B：\n”, response)<br>    print(“\n————————————————————————\n用户：”)<br>    line = input()<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">运行命令：</span><br></pre></td></tr></table></figure><br>CUDA_VISIBLE_DEVICES=0 python3 inference.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## LoRA</span><br><span class="line">官方数据库中没有给出LoRA微调样例，使用Huggingface开源的 PEFT 大模型高效微调工具包(Parameter-Efficient Fine-Tuning)库，其中封装了LoRA这个方法（</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;peft）</span><br><span class="line">但因为官方没有给出具体的样例，所以有许多版本（以下介绍采用的是比较多人引用的 https:&#x2F;&#x2F;github.com&#x2F;hiyouga&#x2F;ChatGLM-Efficient-Tuning 版本）</span><br><span class="line"></span><br><span class="line">### 模型仓库</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;yanqiangmiffy&#x2F;InstructGLM</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;mymusise&#x2F;ChatGLM-Tuning </span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;hiyouga&#x2F;ChatGLM-Efficient-Tuning</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">### 方法简介</span><br><span class="line">LORA方法的原理是在微调大型语言模型时，通过精心设计的策略和技术手段，最大限度地提升模型在低资源环境下的性能。传统的微调方法可能需要大量的训练数据和计算资源，但在现实场景中，往往存在数据有限、计算资源有限的情况。因此，LORA的目标是克服这些限制，实现高效的低资源微调。</span><br><span class="line"></span><br><span class="line">参考paper链接：&lt;a href&#x3D;&#39;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2106.09685.pdf&#39;&gt; LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS &lt;&#x2F;a&gt;</span><br><span class="line"></span><br><span class="line">LORA(Low-Rank Adaptation)微调冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到 Transformer 架构的每一层，极大地减少了下游任务的可训练参数的数量。基于LORA的微调产生保存了新的权重，可以将生成的LORA权重认为是一个原来预训练模型的补丁权重 。所以LORA模型无法单独使用，需要搭配原模型，两者进行合并即可获得完整版权重。（后面会给出代码部分解释）</span><br><span class="line"></span><br><span class="line">### 数据集</span><br><span class="line">可使用的数据集，但是数据集的格式均需经过处理才可使用</span><br><span class="line">#### 斯坦福52k英文指令数据</span><br><span class="line">instruction:52K 条指令中的每一条都是唯一的,答案由text-davinci-003模型生成得到的</span><br><span class="line"></span><br><span class="line">#### BELLE项目生成的中文指令数据：0.5m&amp;1m</span><br><span class="line">百万数据：https:&#x2F;&#x2F;huggingface.co&#x2F;datasets&#x2F;BelleGroup&#x2F;generated_train_1M_CN</span><br><span class="line">生成方式基于种子prompt，调用openai的api生成中文指令</span><br><span class="line"></span><br><span class="line">#### GuanacoDataset 多语言指令数据集</span><br><span class="line">Guanaco 是在 Meta 的 LLaMA 7B 模型上训练的指令跟随语言模型。在 Alpaca 模型原始 52K 数据的基础上，我们添加了额外的 98,369 个条目，涵盖英语、简体中文、繁体中文（台湾）、繁体中文（香港）、日语、德语以及各种语言和语法任务。通过使用这些丰富的数据重新训练和优化模型，Guanaco 在多语言环境中展示了出色的性能和潜力。项目链接可以查看 https:&#x2F;&#x2F;guanaco-model.github.io&#x2F;</span><br><span class="line"></span><br><span class="line">#### alpaca中文指令微调数据集</span><br><span class="line">与原始alpaca数据json格式相同,数据生成的方法是机器翻译和self-instruct</span><br><span class="line"></span><br><span class="line">#### firefly-train-1.1M</span><br><span class="line">一份高质量的包含1.1M中文多任务指令微调数据集，包含23种常见的中文NLP任务的指令数据。对于每个任务，由人工书写若干指令模板，保证数据的高质量与丰富度。</span><br><span class="line"></span><br><span class="line">### 参数详解</span><br></pre></td></tr></table></figure><br>CUDA_VISIBLE_DEVICES=0 python src/train_sft.py \ 使用第一块GPU<br>    —do_train \ # 进行训练<br>    —use_v2 \ # ?<br>    —dataset self_cognition \ # 数据集名称<br>    —finetuning_type lora \ # 微调类型<br>    —lora_rank 32 \ # LoRA的rank<br>    —output_dir cognition \ # 输出目录<br>    —overwrite_cache \ # 覆盖缓存文件<br>    —per_device_train_batch_size 2 \ # 训练时每个GPU的batch size，如果项目过慢，可以适当增大<br>    —gradient_accumulation_steps 2 \ # 梯度累积步数<br>    —lr_scheduler_type cosine \ # 学习率调度器类型<br>    —logging_steps 10 \ # 日志输出步数间隔<br>    —save_steps 1000 \ # 保存模型步数间隔<br>    —warmup_steps 0 \ # warmup步数<br>    —learning_rate 1e-3 \ # 学习率<br>    —num_train_epochs 10.0 \ # 训练轮数<br>    —fp16 \ # 是否使用fp16<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 模型微调</span><br><span class="line"></span><br><span class="line">具体可设置的主要参数包括：</span><br><span class="line"></span><br><span class="line">dataset, 分词后的数据集，即在 data&#x2F; 地址下的文件夹名称</span><br><span class="line">lora_rank, 设置 LoRA 的秩，推荐为4或8，显存够的话使用8</span><br><span class="line">per_device_train_batch_size, 每块 GPU 上的 batch size,显存不大尽量1-2</span><br><span class="line">gradient_accumulation_steps, 梯度累加，可以在不提升显存占用的情况下增大 batch size</span><br><span class="line">save_steps, 多少步保存一次</span><br><span class="line">save_total_limit, 保存多少个checkpoint</span><br><span class="line">learning_rate, 学习率</span><br><span class="line">output_dir, 模型文件保存地址</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 模型加载和推理</span><br></pre></td></tr></table></figure><br>from peft import PeftModel<br>from transformers import AutoTokenizer, AutoModel<br>import torch</p>
<p>device = torch.device(1)</p>
<h1 id="加载原始-LLM"><a href="#加载原始-LLM" class="headerlink" title="加载原始 LLM"></a>加载原始 LLM</h1><p>model_path = “THUDM/chatglm-6b”<br>model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().to(device)<br>tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)<br>model.chat(tokenizer, “你好”, history=[])</p>
<h1 id="给原始-LLM-安装上你的-LoRA-tool"><a href="#给原始-LLM-安装上你的-LoRA-tool" class="headerlink" title="给原始 LLM 安装上你的 LoRA tool"></a>给原始 LLM 安装上你的 LoRA tool</h1><p>model = PeftModel.from_pretrained(model, “model/chatglm2_lora”).half()<br>model.chat(tokenizer, “你好”, history=[])<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 模型验证</span><br></pre></td></tr></table></figure><br>CUDA_VISIBLE_DEVICES=0 \<br>python src/cli_demo.py \<br>    —use_v2 \<br>    —checkpoint_dir cognition<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 导出微调模型</span><br></pre></td></tr></table></figure><br>python src/export_model.py \<br>    —use_v2 \<br>    —checkpoint_dir cognition \<br>    —output_dir ./chatglm2_6b_lora<br>```</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul>
<li>ChatGLM-6B P-Tuning v2 教程 <a href="https://zhuanlan.zhihu.com/p/619417296" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/619417296</a></li>
<li>ChatGLM-6B的P-Tuning微调详细步骤及结果验证 <a href="https://blog.csdn.net/zxd1435513775/article/details/130384164" target="_blank" rel="noopener">https://blog.csdn.net/zxd1435513775/article/details/130384164</a></li>
<li>使用DeepSpeed/P-Tuning v2对ChatGLM-6B进行微调 <a href="https://zhuanlan.zhihu.com/p/622351059" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/622351059</a></li>
<li>【微调】CHATGLM2-6B LoRA 微调 <a href="https://zhuanlan.zhihu.com/p/639581192" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/639581192</a></li>
<li>【教程】从 GLM 到 GLM-130B，到 ChatGLM-6B，一文讲透 <a href="https://modelnet.ai/modeldoc/d2fe7c2239244ba7bf28b816aafc45d3" target="_blank" rel="noopener">https://modelnet.ai/modeldoc/d2fe7c2239244ba7bf28b816aafc45d3</a></li>
<li>类ChatGPT模型LLaMA的解读与其微调：Alpaca-LoRA/Vicuna/BELLE <a href="https://blog.csdn.net/v_JULY_v/article/details/129709105" target="_blank" rel="noopener">https://blog.csdn.net/v_JULY_v/article/details/129709105</a></li>
<li>修改 ChatGLM2-6B 自我认知的 Lora 微调教程 <a href="https://engchina.blog.csdn.net/article/details/131492403" target="_blank" rel="noopener">https://engchina.blog.csdn.net/article/details/131492403</a></li>
<li>【教程】InstructGLM：基于ChatGLM-6B在指令数据集上进行微调 <a href="https://modelnet.ai/modeldoc/e9364590fcb0446b88f5ece94cf1fd1ev" target="_blank" rel="noopener">https://modelnet.ai/modeldoc/e9364590fcb0446b88f5ece94cf1fd1ev</a></li>
<li>【NLP修炼系列之玩转LLM】基于LORA的高效微调ChatGLM方法 <a href="https://zhuanlan.zhihu.com/p/632010770" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/632010770</a></li>
</ul>

                
                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                    
                    <li class="next">
                        <a href="/2023/03/26/x86-上机实验/" data-toggle="tooltip" data-placement="top" title="x86 上机实验">Next Post &rarr;
		<br>
		<span><font size="2" face="Calibri" color="grey">x86 上机实验</font></span>
	        </a>
                    </li>
                    
                </ul>

                <!-- tip start -->
                

                
                <div class="comment_notes">
                    <p>
                        by Tan
                    </p>
                </div>
                
                <!-- tip end -->

                <!-- Music start-->
                
                <!-- Music end -->

                <!-- Sharing -->
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            
              <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#Chatglm-官网"><span class="toc-nav-text">Chatglm 官网</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#官方主页"><span class="toc-nav-text">官方主页</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#官方社区"><span class="toc-nav-text">官方社区</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#常用命令"><span class="toc-nav-text">常用命令</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#tar"><span class="toc-nav-text">tar</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#示范流程"><span class="toc-nav-text">示范流程</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#CHATGLM-6B-模型参数微调"><span class="toc-nav-text">CHATGLM - 6B 模型参数微调</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#模型地址"><span class="toc-nav-text">模型地址</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#P-Tuning-v2"><span class="toc-nav-text">P-Tuning v2</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#简介"><span class="toc-nav-text">简介</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#官方论文"><span class="toc-nav-text">官方论文</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#文件组织"><span class="toc-nav-text">文件组织</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#安装依赖"><span class="toc-nav-text">安装依赖</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#训练数据集"><span class="toc-nav-text">训练数据集</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#官方样例数据集"><span class="toc-nav-text">官方样例数据集</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#自己的数据集"><span class="toc-nav-text">自己的数据集</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#参数详解"><span class="toc-nav-text">参数详解</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#arguments-ModelArguments"><span class="toc-nav-text">arguments.ModelArguments</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#arguments-DataTrainingArguments"><span class="toc-nav-text">arguments.DataTrainingArguments</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Seq2SeqTrainingArguments"><span class="toc-nav-text">Seq2SeqTrainingArguments</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#具体-sh代码解析"><span class="toc-nav-text">具体.sh代码解析</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#train-sh"><span class="toc-nav-text">train.sh</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#evaluate-sh"><span class="toc-nav-text">evaluate.sh</span></a></li></ol></li></ol></li></ol></li></ol></li></ol>
            
          
          </div>
        </aside>
      
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                    </div>
                </section>
                

            </div>
			
			<div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">
				
				<br>
				<br>

				<script src="https://utteranc.es/client.js"
						repo="Master-Tan/Master-Tan.github.io"
						issue-term="pathname"
						label="Comment"
						theme="github-light"
						crossorigin="anonymous"
						async>
				</script>
			</div>
        </div>
    </div>
</article>




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>


<style  type="text/css">
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                
                    <li>
                        <a target="_blank"  href="https://github.com/Master-Tan">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="https://twitter.com/None">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="https://www.facebook.com/None">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Tan 2023 | Powered by 
                    <a href="https://github.com/Master-Tan/Master-Tan.github.io" target="_blank" rel="noopener">
                        <i>Tan's Blog</i>
                    </a>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://master-tan.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        
            <script type="text/javascript" src="/js/mouse-click.js" content='[&#34;Sunny&#34;, &#34;💖&#34;, &#34;Sunny&#34;, &#34;🧡&#34;, &#34;Sunny&#34;, &#34;💛&#34;, &#34;Sunny&#34; , &#34;💚&#34;, &#34;Sunny&#34;, &#34;💙&#34;, &#34;Sunny&#34;, &#34;💜&#34;, &#34;Sunny&#34;, &#34;😍&#34;]' color='[&#34;rgb(255, 0, 0)&#34; ,&#34;rgb(255, 0, 0)&#34; ,&#34;rgb(255, 125, 0)&#34; ,&#34;rgb(255, 125, 0)&#34; ,&#34;rgb(255, 255, 0)&#34; ,&#34;rgb(255, 255, 0)&#34; ,&#34;rgb(0, 255, 0)&#34; ,&#34;rgb(0, 255, 0)&#34; ,&#34;rgb(0, 255, 255)&#34; ,&#34;rgb(0, 255, 255)&#34; ,&#34;rgb(0, 0, 255)&#34; ,&#34;rgb(0, 0, 255)&#34; ,&#34;rgb(255, 0, 255)&#34; ,&#34;rgb(255, 0, 255)&#34;]'></script>
        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
</body>

</html>
