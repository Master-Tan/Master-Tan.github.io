<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="description" content="Tan&#39;s Blog">
    <meta name="keyword"  content="Hello world">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          机器学习 - 学习笔记 - Tan&#39;s Blog
        
    </title>

    <link rel="canonical" href="https://master-tan.github.io/2023/10/19/机器学习 - 学习笔记/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
        
<link rel="stylesheet" href="/css/dusign-light.css">

        
<link rel="stylesheet" href="/css/dusign-common-light.css">

        
<link rel="stylesheet" href="/css/font-awesome.css">

        
<link rel="stylesheet" href="/css/toc.css">

        <!-- background effects end -->
    
    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- photography -->
    
<link rel="stylesheet" href="/css/photography.css">


    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('../../../../img/default.jpg')
                /*post*/
            
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
                            
                        </div>
                        <h1>机器学习 - 学习笔记</h1>
                        <h2 class="subheading">李宏毅2021春机器学习课程</h2>
                        <span class="meta">
                            Posted by Tan on
                            2023-10-19
                        </span>

	       
                            <div class="blank_box"></div>
                            <span class="meta">
                                 <span class="post-count">2.8k</span> Words
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-light.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-light.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Tan&#39;s Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        	<li>
                          	  <a href="/about/">About Tan</a>
                        	</li>
                        
                    

                        
                        	<li>
                          	  <a href="/archive/">Archives</a>
                        	</li>
                        
                    

                        
                        	<li>
                          	  <a href="/tags/">Tags</a>
                        	</li>
                        
                    

                        
                    

                        
                    

                        
                    

                        
                    
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <p>课程视频：<a href='https://b23.tv/BV1Wv411h7kN' target="_blank" rel="noopener">https://b23.tv/BV1Wv411h7kN</a> 或 <a href='https://www.youtube.com/playlist?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J' target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J</a></p>
<p>课程网站：<a href='https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php' target="_blank" rel="noopener">https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php</a></p>
<p>课程Github：<a href='https://github.com/Fafa-DL/Lhy_Machine_Learning' target="_blank" rel="noopener">https://github.com/Fafa-DL/Lhy_Machine_Learning</a></p>
<hr>
<h2 id="Lecture-1-Introduction-of-Deep-Learning"><a href="#Lecture-1-Introduction-of-Deep-Learning" class="headerlink" title="Lecture 1:Introduction of Deep Learning"></a>Lecture 1:Introduction of Deep Learning</h2><h3 id="預測本頻道觀看人數-上-機器學習基本概念簡介"><a href="#預測本頻道觀看人數-上-機器學習基本概念簡介" class="headerlink" title="預測本頻道觀看人數 (上) - 機器學習基本概念簡介"></a>預測本頻道觀看人數 (上) - 機器學習基本概念簡介</h3><p>机器学习 -&gt; 让机器具备找函式的能力</p>
<p>专有名词：</p>
<ul>
<li>Regression：输出为数值的函式</li>
<li>Classification：给予选项，输出为选项之一的函式</li>
<li>Structured Learning：输出为结构化数据（图像，文本等）的函式</li>
</ul>
<p><strong>机器学习找函式（训练Model）的过程</strong>:</p>
<ol>
<li>写出一个带有未知参数的函式（基于领域知识） -&gt; 机器学习的模型</li>
<li>从训练数据中定义Loss （Loss也为一个关于参数的函式，代表了模型好坏程度）<ul>
<li>error surface</li>
</ul>
</li>
<li>最优化：Gradient Descent（梯度下降）<ul>
<li>取初始点</li>
<li>求该点梯度</li>
<li>向梯度方向根据学习率（hyperparameters）更新参数</li>
<li>迭代</li>
</ul>
</li>
</ol>
<h3 id="預測本頻道觀看人數-下-深度學習基本概念簡介"><a href="#預測本頻道觀看人數-下-深度學習基本概念簡介" class="headerlink" title="預測本頻道觀看人數 (下) - 深度學習基本概念簡介"></a>預測本頻道觀看人數 (下) - 深度學習基本概念簡介</h3><p>Linear Model（线性模型）： sum of (feature * weight) + bias  -&gt; 有很大局限性(Model Bias)</p>
<p>Piecewise Linear Model（分段线性模型）： constraint + sum of  a set of <strong>*Activation function*</strong> （Hard Sigmoid）</p>
<ul>
<li>曲线：取点分割后可以视为分段线性模型</li>
</ul>
<p><strong>*Activation function*</strong>:</p>
<ul>
<li><p>Sigmoid Function:</p>
<script type="math/tex; mode=display">
  \begin{align*}
      y &= c*\frac{1}{1+e^{-(b+w*x_1)}} \\
  ~\\
      &= c*sigmoid(b+w*x_1)
  \end{align*}</script><p>  线性模型到分段线性模型转化：</p>
<script type="math/tex; mode=display">
  \begin{array}{l}
      y=b+w x_{1} \rightarrow 
      y=b+\sum_{i} c_{i} sigmoid\left(\left.b_{i}+w_{i} x_{1}\right)\right. \\
  ~\\
      y=b+\sum_{j} w_{i} x_{i} \rightarrow 
      y=b+\sum_{i} c_{i} sigmoid\left(b_{i}+\sum_{j} w_{i j} x_{i}\right)
  \end{array}</script><p>  写成矩阵形式样例：</p>
<script type="math/tex; mode=display">
  \left[\begin{array}{l}
  r_{1} \\
  ~\\
  r_{2} \\
  ~\\
  r_{3}
  \end{array}\right]=\left[\begin{array}{l}
  b_{1} \\
  ~\\
  b_{2} \\
  ~\\
  b_{3}
  \end{array}\right]+\left[\begin{array}{lll}
  w_{11} & w_{12} & w_{13} \\
  ~\\
  w_{21} & w_{22} & w_{23} \\
  ~\\
  w_{31} & w_{32} & w_{33}
  \end{array}\right]\left[\begin{array}{l}
  x_{1} \\
  ~\\
  x_{2} \\
  ~\\
  x_{3}
  \end{array}\right]</script><p>  即:</p>
<script type="math/tex; mode=display">
  \vec{r}=\vec{b}+\vec{W}  \vec{x}</script><p>  然后：$定义~\sigma = sigmoid~~, ~~\vec{a} = \sigma(\vec{r}) ~~, ~~则：y = b + \vec{c}^{~T} \vec{a}$</p>
<p>  所以：$y = b + \vec{c}^{~T} \sigma(\vec{b}+\vec{W}  \vec{x})$ ，其中 $\vec{x}$ ：feature； $\vec{W}, \vec{b}, \vec{c}^{~T}, b$ ：Unknown parameters</p>
<p>  定义：列向量$\vec{\theta}$为所有Unknown parameters的展开的集合（如果是矩阵，就是所有元素展开的集合）</p>
<p>  最小化：</p>
<script type="math/tex; mode=display">
  \boldsymbol{\theta}^{*}=\arg \min _{\boldsymbol{\theta}} L~~~~
  其中：
  \boldsymbol{\theta}=\left[\begin{array}{c}
  \theta_{1} \\
  ~\\
  {\theta}_{2} \\
  ~\\
  {\theta}_{3} \\
  ~\\
  \vdots
  \end{array}\right]

  \\
  ~\\

  （随机）选择初始值\ \ \boldsymbol{\theta}^{0} \\
  ~\\ 

  ~\\
  ~\\

  取：\begin{array}{l}
  \boldsymbol{g}=\nabla L\left(\boldsymbol{\theta}^{0}\right) \quad ，则参数更新：\boldsymbol{\theta}^{1} \leftarrow \boldsymbol{\theta}^{0}-\eta \boldsymbol{g} \\
  ~\\
  \end{array}

  ~\\
  ~\\
  ~\\
  ~\\

  取：\begin{array}{l}
  \boldsymbol{g}=\nabla L\left(\boldsymbol{\theta}^{1}\right) \quad ，则参数更新：\boldsymbol{\theta}^{2} \leftarrow \boldsymbol{\theta}^{1}-\eta \boldsymbol{g} \\
  ~\\
  \end{array}

  ~\\
  ~\\
  ~\\
  ~\\
  ......</script><p>  每对一个Batch 一次更新参数： Update<br>  遍历一遍 Batch： Epoch</p>
</li>
<li><p>ReLU（Rectified Linear Unit）：</p>
<script type="math/tex; mode=display">
      c ~ max(0, b+w*x_1)</script><p>  则 Hard Sigmoid 可以写成：</p>
<script type="math/tex; mode=display">
      c ~ max(0, b+w*x_1) + c' ~ max(0, b'+w'*x_1)</script><p>  相当于用两个 ReLU 合成一个 Sigmoid</p>
</li>
</ul>
<p>可以套多个Activation function，如$y = b + \vec{c}^{~T} \sigma\left(\vec{b ‘} + \vec{W’}\sigma(\vec{b}+\vec{W}  \vec{x})\right)$</p>
<p><strong>为什么要套用多层Activation function（Deep）？</strong>：后面解答。</p>
<p>Neuron -&gt; Neurl Network<br>layers：一层Neuron<br>many hidden layers -&gt; Deep Learning</p>
<p>在训练集上表现更好而在测试集上变现变差：Overfitting（过拟合） </p>
<hr>
<h2 id="Lecture-2-What-to-do-if-my-network-fails-to-train"><a href="#Lecture-2-What-to-do-if-my-network-fails-to-train" class="headerlink" title="Lecture 2:What to do if my network fails to train"></a>Lecture 2:What to do if my network fails to train</h2><h3 id="機器學習任務攻略"><a href="#機器學習任務攻略" class="headerlink" title="機器學習任務攻略"></a>機器學習任務攻略</h3><p>机器学习框架：</p>
<ul>
<li><p>Training data:  $\left\{\left(x^{1}, \hat{y}^{1}\right),\left(x^{2}, \hat{y}^{2}\right), \ldots,\left(x^{N}, \hat{y}^{N}\right)\right\}$</p>
</li>
<li><p>Training: $y=f<em>{\boldsymbol{\theta}}(\boldsymbol{x})\rightarrow L\left(\boldsymbol{\theta}\right)\rightarrow\boldsymbol{\theta}^{*} = arg\text{min}</em>{\boldsymbol{\theta}}L$</p>
</li>
<li><p>Testing data:  $\left\{x^{N+1}, x^{N+2}, \ldots, x^{N+M}\right\}$</p>
</li>
<li><p>Use  $y<em>{0}=f</em>{\boldsymbol{\theta}^{*}}(\boldsymbol{x})$  to label the testing data  </p>
</li>
<li><p>Get $\left\{y^{N+1}, y^{N+2}, \ldots, y^{N+M}\right\}$ </p>
</li>
</ul>
<p>如何优化：</p>
<ul>
<li>loss on training data<ul>
<li>large：<ul>
<li>model bias（模型偏差）:<br>原因：模型太简单，不能很好的拟合数据<br>解决方法：重新设计model，给你的model更大的弹性（范围）<ul>
<li>more features</li>
<li>deep learning（more neurons，more layers）</li>
</ul>
</li>
<li>optimization issue（最优化过程（梯度下降）中的问题）<ul>
<li>解决方案:更强大的优化技术(下一讲)</li>
</ul>
</li>
<li>如何判断是哪个原因导致的？<ul>
<li>比较不同的模型</li>
<li>从较浅的网络(或其他模型)开始，因为其更容易优化</li>
<li>如果深度网络不能在训练数据上获得更小的损失，那么就存在优化问题</li>
</ul>
</li>
</ul>
</li>
<li>small：<ul>
<li>loss on testing data<ul>
<li>large：<ul>
<li>overfitting（过拟合）<ul>
<li>解决方案：<ul>
<li>more training data（增加训练样本）</li>
<li>data augmentation（数据增强）（如通过旋转、平移、缩放、翻转、加噪声等方式增加图像数据）</li>
<li>constrained model（约束模型）<ul>
<li>less parameters（更少参数），shared parameters（共用参数（CNN））</li>
<li>less features（更少特征）</li>
<li>early stopping（早停）</li>
<li>regularization（正则化）<ul>
<li>L1 regularization</li>
<li>L2 regularization</li>
</ul>
</li>
<li>dropout</li>
</ul>
</li>
<li>注意：不要加以太多限制，否则会有问题（model bias）</li>
</ul>
</li>
</ul>
</li>
<li>mismatch：训练集和测试集不匹配（具有不同的分布）<br>了解资料是如何生成的</li>
</ul>
</li>
<li>small：<ul>
<li><strong>good model ！</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>cross validation（交叉验证）：<br>Training Set -&gt; Training Set + Validation Set<br>不能根据测试集来反向挑选模型</p>
<p>N-fold cross validation（N折交叉验证）：<br>将训练集分成N份，每次用其中一份作为验证集，其余N-1份作为训练集，进行N次训练，取平均值，获得其中最优的模型，然后再把该模型用在全部的训练集上进行训练，得到最终的模型。</p>
<h3 id="類神經網路訓練不起來怎麼辦-一-：-局部最小值-local-minima-與鞍點-saddle-point"><a href="#類神經網路訓練不起來怎麼辦-一-：-局部最小值-local-minima-與鞍點-saddle-point" class="headerlink" title="類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point)"></a>類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point)</h3><p>Optimization（Gradient Descent）：</p>
<p>gradient（梯度）为零：local minima（局部最小值）或saddle point（鞍点）等，统称为critical point（临界点）</p>
<p>如何判断临界点类型：</p>
<ul>
<li><p>泰勒级数近似: </p>
<p>  $L(\boldsymbol{\theta})$ 在 $\boldsymbol{\theta}=\boldsymbol{\theta}^{\prime}$ 可以近似为： </p>
<script type="math/tex; mode=display">
      L(\boldsymbol{\theta}) \approx L\left(\boldsymbol{\theta}^{\prime}\right)+\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right)^{T} \boldsymbol{g}+\frac{1}{2}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right)^{T} \boldsymbol{H}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right)</script><p>  其中Gradient $\boldsymbol{g}$ 是一个向量：</p>
<script type="math/tex; mode=display">
      \boldsymbol{g}=\nabla L\left(\boldsymbol{\theta}^{\prime}\right) \quad \boldsymbol{g}_{i}=\frac{\partial L\left(\boldsymbol{\theta}^{\prime}\right)}{\partial \boldsymbol{\theta}_{i}}</script><p>  其中Hessian $\boldsymbol{H}$ 是一个矩阵：</p>
<script type="math/tex; mode=display">
      \boldsymbol{H}_{i j}=\frac{\partial^{2}}{\partial \boldsymbol{\theta}_{i} \partial \boldsymbol{\theta}_{j}} L\left(\boldsymbol{\theta}^{\prime}\right)</script><p>  因此，在临界点时，由临界点定义：$\boldsymbol{g} = 0$</p>
<p>  代入泰勒级数近似式，得到：</p>
<script type="math/tex; mode=display">
      L(\boldsymbol{\theta}) \approx L\left(\boldsymbol{\theta}^{\prime}\right)+\frac{1}{2}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right)^{T} \boldsymbol{H}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right)</script><p>  所以，有结论：</p>
<ul>
<li>当 $\boldsymbol{v}^{T}\boldsymbol{H}\boldsymbol{v}$ 永远为正即 $\boldsymbol{H}$ 是正定矩阵（所有特征值均为正数）时，$L(\boldsymbol{\theta})$ 为局部最小值（Local Minima）</li>
<li>当 $\boldsymbol{v}^{T}\boldsymbol{H}\boldsymbol{v}$ 永远为负即 $\boldsymbol{H}$ 是负定矩阵（所有特征值均为负数）时，$L(\boldsymbol{\theta})$ 为局部最大值（Local Maxima）</li>
<li>当 $\boldsymbol{v}^{T}\boldsymbol{H}\boldsymbol{v}$ 有正有负时，$L(\boldsymbol{\theta})$ 为鞍点（Saddle Point），此时最优化更新的方向可由 $\boldsymbol{H}$ 的特征值决定，朝负的特征值对应的特征向量 $L(\boldsymbol{u})$ 的方向更新可使Loss变小（实用性不强） </li>
</ul>
</li>
<li><p>在低维中的local minima，或许在高维中是saddle point</p>
</li>
<li>当你有很多参数时，也许局部最小值很少见?<ul>
<li>对的，在实际操作中，几乎找不到所有特征值均为正的临界点（local minima）的情况</li>
</ul>
</li>
</ul>
<h3 id="類神經網路訓練不起來怎麼辦-二-：-批次-batch-與動量-momentum"><a href="#類神經網路訓練不起來怎麼辦-二-：-批次-batch-與動量-momentum" class="headerlink" title="類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum)"></a>類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum)</h3><p>带有Batch的优化：</p>
<ul>
<li>Shuffle: 每次迭代(epoch)时，将训练集重新分为不同的batch，以避免每次迭代时都是相同的batch</li>
<li>为什么要用Batch:<ul>
<li>small batch 与 large batch 比较：<ul>
<li>较大的Batch size不需要更长的时间来计算梯度(除非批处理大小太大)</li>
<li>较小的Batch size需要更长的epoch时间(一次查看所有数据需要更长的时间)(GPU有一定的并行计算能力)</li>
<li>batch大小越小，性能越好</li>
<li>带有“噪声”的更新更适合训练</li>
<li>猜想：小的批量大小在测试集上的表现更好（泛化性能更好）</li>
</ul>
</li>
<li>结论：small batch 与 large batch各有优劣，在真实训练时作为一个超参数（hyper-parameter），需要根据实际情况进行调整</li>
</ul>
</li>
</ul>
<p>Momentum：梯度下降时每一步的更新方向不仅仅取决于当前的梯度，还取决于之前的更新方向，即当前的更新方向是之前更新方向的一个加权平均，这样做的好处是可以在一定程度上减少梯度下降的震荡，从而加快收敛速度。</p>
<p>流程：</p>
<ul>
<li>Starting at  $\boldsymbol{\theta}^{\mathbf{0}}$</li>
<li>Movement  $\boldsymbol{m}^{\mathbf{0}}=\mathbf{0}$</li>
<li>Compute gradient  $\boldsymbol{g}^{0}$</li>
<li>Movement  $\boldsymbol{m}^{1}=\lambda \boldsymbol{m}^{0}-\eta \boldsymbol{g}^{0}$</li>
<li>Move to  $\boldsymbol{\theta}^{1}=\boldsymbol{\theta}^{0}+\boldsymbol{m}^{1}$</li>
<li>Compute gradient  $\boldsymbol{g}^{\mathbf{1}}$</li>
<li>Movement  $\boldsymbol{m}^{2}=\lambda \boldsymbol{m}^{1}-\eta \boldsymbol{g}^{1}$</li>
<li>Move to  $\boldsymbol{\theta}^{2}=\boldsymbol{\theta}^{1}+\boldsymbol{m}^{2}$</li>
</ul>
<p>易得，$\boldsymbol{m}^{i}$ 是 $\boldsymbol{g}^{j}(j&lt;i)$ 的加权和，加权值也是超参数。</p>
<h3 id="類神經網路訓練不起來怎麼辦-三-：自動調整學習速率-Learning-Rate"><a href="#類神經網路訓練不起來怎麼辦-三-：自動調整學習速率-Learning-Rate" class="headerlink" title="類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate)"></a>類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate)</h3><p>当 error surface 崎岖不平时：<br>训练技巧：自适应学习率（Adaptive Learning Rate）</p>
<p>训练卡顿 $\neq$ 小梯度</p>
<p>即使没有到临界点，训练也会出现困难 -&gt; 学习率不能一成不变 -&gt; 不同的参数需要不同的学习率</p>
<p>对任一参数 $\boldsymbol{\theta}_{i}$：</p>
<script type="math/tex; mode=display">
  \begin{aligned}
  \boldsymbol{\theta}_{i}^{\boldsymbol{t}+\boldsymbol{1}} \leftarrow \boldsymbol{\theta}_{i}^{\boldsymbol{t}}-\eta \boldsymbol{g}_{i}^{\boldsymbol{t}} \\
  \boldsymbol{g}_{i}^{\boldsymbol{t}}=\left.\frac{\partial L}{\partial \boldsymbol{\theta}_{i}}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{\boldsymbol{t}}} \\
  \boldsymbol{\theta}_{i}^{\boldsymbol{t}+\mathbf{1}} \leftarrow \boldsymbol{\theta}_{i}^{\boldsymbol{t}}-\frac{\eta}{\sigma_{i}^{\boldsymbol{t}}} \boldsymbol{g}_{i}^{\boldsymbol{t}}
  \end{aligned}</script><p>则，其中的 $\frac{\eta}{\sigma_{i}^{\boldsymbol{t}}}$ 是参数依赖的学习率：</p>
<ul>
<li><p>Root Mean Square:</p>
<script type="math/tex; mode=display">
  \begin{array}{rlrl}
  \boldsymbol{\theta}_{i}^{1} & \leftarrow \boldsymbol{\theta}_{i}^{\mathbf{0}}-\frac{\eta}{\sigma_{i}^{0}} \boldsymbol{g}_{i}^{0} & \sigma_{i}^{0} & =\sqrt{\left(\boldsymbol{g}_{i}^{0}\right)^{2}}=\left|\boldsymbol{g}_{i}^{0}\right| \\
  \boldsymbol{\theta}_{i}^{2} & \leftarrow \boldsymbol{\theta}_{i}^{\mathbf{1}}-\frac{\eta}{\sigma_{i}^{1}} \boldsymbol{g}_{i}^{1} & \sigma_{i}^{1} & =\sqrt{\frac{1}{2}\left[\left(\boldsymbol{g}_{i}^{0}\right)^{2}+\left(\boldsymbol{g}_{i}^{1}\right)^{2}\right]} \\
  \boldsymbol{\theta}_{i}^{\mathbf{3}} & \leftarrow \boldsymbol{\theta}_{i}^{\mathbf{2}}-\frac{\eta}{\sigma_{i}^{2}} \boldsymbol{g}_{i}^{2} & \sigma_{i}^{2} & =\sqrt{\frac{1}{3}\left[\left(\boldsymbol{g}_{i}^{0}\right)^{2}+\left(\boldsymbol{g}_{i}^{1}\right)^{2}+\left(\boldsymbol{g}_{i}^{2}\right)^{2}\right]} \\
  \vdots & \\
  \boldsymbol{\theta}_{i}^{\boldsymbol{t}+\boldsymbol{1}} & \leftarrow \boldsymbol{\theta}_{i}^{\boldsymbol{t}}-\frac{\eta}{\sigma_{i}^{t}} \boldsymbol{g}_{i}^{\boldsymbol{t}} & \sigma_{i}^{t} & =\sqrt{\frac{1}{t+1} \sum_{i=0}^{t}\left(\boldsymbol{g}_{i}^{\boldsymbol{t}}\right)^{2}}
  \end{array}</script><p>（在AdaGrad算法中使用）</p>
<p>作用：在梯度下降大时，学习率降低；在梯度下降小时，学习率增大</p>
</li>
</ul>
<p>很多时候学习率需要动态适应（error surface 会非常复杂）：</p>
<ul>
<li><p>RMSProp</p>
<script type="math/tex; mode=display">
  \begin{array}{rlrl}
  \boldsymbol{\theta}_{i}^{1} & \leftarrow \boldsymbol{\theta}_{i}^{\mathbf{0}}-\frac{\eta}{\sigma_{i}^{0}} \boldsymbol{g}_{i}^{0} & \sigma_{i}^{0} & =\sqrt{\left(\boldsymbol{g}_{i}^{0}\right)^{2}}=\left|\boldsymbol{g}_{i}^{0}\right| \\
  \boldsymbol{\theta}_{i}^{2} & \leftarrow \boldsymbol{\theta}_{i}^{\mathbf{1}}-\frac{\eta}{\sigma_{i}^{1}} \boldsymbol{g}_{i}^{1} & \sigma_{i}^{1} & =\sqrt{\alpha\left(\sigma_{i}^{0}\right)^{2}+\left(1-\alpha\right)\left(\boldsymbol{g}_{i}^{1}\right)^{2}} \left(0<\alpha<1\right) \\
  \boldsymbol{\theta}_{i}^{\mathbf{3}} & \leftarrow \boldsymbol{\theta}_{i}^{\mathbf{2}}-\frac{\eta}{\sigma_{i}^{2}} \boldsymbol{g}_{i}^{2} & \sigma_{i}^{2} & =\sqrt{\alpha\left(\sigma_{i}^{1}\right)^{2}+\left(1-\alpha\right)\left(\boldsymbol{g}_{i}^{2}\right)^{2}} \left(0<\alpha<1\right) \\
  \vdots & \\
  \boldsymbol{\theta}_{i}^{\boldsymbol{t}+\boldsymbol{1}} & \leftarrow \boldsymbol{\theta}_{i}^{\boldsymbol{t}}-\frac{\eta}{\sigma_{i}^{t}} \boldsymbol{g}_{i}^{\boldsymbol{t}} & \sigma_{i}^{t} & =\sqrt{\alpha\left(\sigma_{i}^{t-1}\right)^{2}+\left(1-\alpha\right)\left(\boldsymbol{g}_{i}^{t}\right)^{2}} \left(0<\alpha<1\right)
  \end{array}</script><p>（$\alpha$是一个超参数）<br>（近期的梯度对学习率的影响更大，远期的梯度对学习率的影响更小）</p>
</li>
<li><p>Adam<br>Adam = RMSProp + Momentum</p>
<ul>
<li>deep learning套件(如pytorch)中大多有现成的</li>
<li>超参数往往用预设的就能得到较为不错的结果</li>
</ul>
</li>
</ul>
<p>Learning Rate Scheduling（学习率调度）</p>
<script type="math/tex; mode=display">
  \begin{aligned}
  \boldsymbol{\theta}_{i}^{\boldsymbol{t}+\mathbf{1}} \leftarrow \boldsymbol{\theta}_{i}^{\boldsymbol{t}}-\frac{\eta^{\boldsymbol{t}}}{\sigma_{i}^{\boldsymbol{t}}} \boldsymbol{g}_{i}^{\boldsymbol{t}}
  \end{aligned}</script><p>中的 $\eta$ 设置为与时间相关</p>
<ul>
<li><p>Learning Rate Decay（学习率衰减）</p>
<p>学习率随着时间衰减</p>
<p>原因：随着训练的进行，我们离目标越来越近，所以我们降低了学习率</p>
<p>（$\alpha$是一个超参数）</p>
</li>
<li><p>Warm Up</p>
<p>学习率先从小到大，再从大到小</p>
<p>原因：（未知），解释之一：在训练开始的时候，对 $\frac{\eta}{\sigma_{i}^{\boldsymbol{t}}}$ 的估计不准确，所以先从小到大，而从大到小的原因与Learning Rate Decay相同</p>
</li>
</ul>
<p>因此，各种改进之后：</p>
<script type="math/tex; mode=display">
  \begin{aligned}
  \boldsymbol{\theta}_{i}^{\boldsymbol{t}+\mathbf{1}} \leftarrow \boldsymbol{\theta}_{i}^{\boldsymbol{t}}-\frac{\eta^{\boldsymbol{t}}}{\sigma_{i}^{\boldsymbol{t}}} \boldsymbol{m}_{i}^{\boldsymbol{t}}
  \end{aligned}</script><p>$\boldsymbol{m}_{i}^{\boldsymbol{t}} \rightarrow$ Momentum 之前梯度的加权和<br>（包括方向）</p>
<p>$\sigma_{i}^{\boldsymbol{t}} \rightarrow$ 梯度的均方根（只有大小）</p>
<p>$\eta^{\boldsymbol{t}} \rightarrow$ Learning Rate Scheduling（学习率调度）</p>
<h3 id="類神經網路訓練不起來怎麼辦-四-：損失函數-Loss-也可能有影響"><a href="#類神經網路訓練不起來怎麼辦-四-：損失函數-Loss-也可能有影響" class="headerlink" title="類神經網路訓練不起來怎麼辦 (四)：損失函數 (Loss) 也可能有影響"></a>類神經網路訓練不起來怎麼辦 (四)：損失函數 (Loss) 也可能有影響</h3><p>Classification as Regression？（把分类问题当成回归问题）</p>
<ul>
<li><p>Regression<br>输入向量 -&gt; 输出<strong>一个</strong>数值 -&gt; 对比与label的接近程度</p>
</li>
<li><p>Classification as Regression？<br>label（$\hat{y}$）使用one-hot vector表示每一个class，输出是一个向量，对比向量的接近程度</p>
</li>
</ul>
<p>例如：</p>
<script type="math/tex; mode=display">
\begin{array}{c} ~~~~~~
\text { Class } 1 ~~~~~
\text { Class } 2 ~~~~~
\text { Class } 3 \\
\hat{\boldsymbol{y}}=\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right] \text { or }\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right] \text { or }\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]
\end{array}</script><p>Regression的问题：输出的向量为一个实数，而不是一个向量，因此我们需要将其转换为一个向量。</p>
<p>Regression：</p>
<script type="math/tex; mode=display">
  \hat{y} \leftrightarrow y = b + \vec{c}^{~T} \sigma(\vec{b}+\vec{W}  \vec{x})</script><p>Classification：</p>
<script type="math/tex; mode=display">
  y = b + \vec{c}^{~T} \sigma(\vec{b}+\vec{W}  \vec{x}) \\
  \hat{y} \leftrightarrow y' = softmax(y)</script><p>为啥要有 $softmax$ 函数：（一种不准确的解释是）因为 $y$ 中的值是任意的，而 $\hat{y}$ 中的值是 0 / 1，因此 $y$ 需要通过函数$softmax$转换其所有向量中的值为 0 ~ 1 之间以匹配 $\hat{y}$ ，或与 $\hat{y}$ 计算相似度</p>
<p>Softmax：</p>
<script type="math/tex; mode=display">
  y_{i}^{\prime}=\frac{\exp \left(y_{i}\right)}{\sum_{j} \exp \left(y_{i}\right)}  (1 > y_{i}^{\prime} > 0, {\sum_{j} y_{i}^{\prime} = 1})</script><ul>
<li>只有两个class时，softmax = sigmoid</li>
</ul>
<p>Loss of Classification：（即如何度量 $y$ 与 $label: \hat{y}$ 之间的偏差）</p>
<script type="math/tex; mode=display">
  Mean Square Error (MSE)  \quad e=\sum_{i}\left(\widehat{\boldsymbol{y}}_{i}-\boldsymbol{y}_{i}^{\prime}\right)^{2} \\
  Cross-entropy  \quad e=-\sum_{i} \widehat{\boldsymbol{y}}_{i} \ln \boldsymbol{y}_{i}^{\prime} （更常用，更优）</script><ul>
<li><p>Minimizing cross-entropy is equivalent to maximizing likelihood.（最小化交叉熵相当于最大化可能性。）</p>
</li>
<li><p>在PyTorch中，Cross-entropy 和 Softmax 是绑定使用的</p>
</li>
<li><p>改变损失函数可以改变优化的难度</p>
</li>
</ul>
<h3 id="類神經網路訓練不起來怎麼辦-五-：-批次標準化-Batch-Normalization-簡介"><a href="#類神經網路訓練不起來怎麼辦-五-：-批次標準化-Batch-Normalization-簡介" class="headerlink" title="類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介"></a>類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介</h3><p>Changing Landscape of Machine Learning</p>
<p>不同方向坡度非常不同的error surface：input 的 feature 每一个 dimension 的值 scale 差距很大时</p>
<p>解决：给不同 dimension 同样的数值范围 -&gt; 方法：Feature Normalization</p>
<ul>
<li><p>对 $\boldsymbol{x}^{1} = (\boldsymbol{x}<em>{1}^{1}, \boldsymbol{x}</em>{2}^{1},\dots), \boldsymbol{x}^{2} = (\boldsymbol{x}<em>{1}^{2}, \boldsymbol{x}</em>{2}^{2},\dots),\dots$ 的每个维度 $i$ 取均值 $m<em>i$ 和标准差 $\sigma_i$，然后对每个维度 $i$ 进行标准化（standardization）（也是normalization）： $\tilde{\boldsymbol{x}}</em>{i}^{k} \leftarrow \frac{\boldsymbol{x}<em>{i}^{k}-m</em>{i}}{\sigma_{i}}$ （好处：该维度处理后平均值为0，方差为1）</p>
</li>
<li><p>一般来说，特征归一化会使梯度下降收敛更快速</p>
</li>
<li><p>每一层的输入 / 上一层的输出 都要做 normalization ，在activation function之前还是之后做 normalization 影响不大，根据 activation function 的不同而选择</p>
</li>
<li><p>$z^{1}, z^{2}, z^{3} \dots \rightarrow \mu=\frac{1}{3} \sum<em>{i=1}^{3} z^{i} , ~ \sigma</em>{o}=\sqrt{\frac{1}{3} \sum_{i=1}^{3}\left(z^{i}-\mu\right)^{2}}, ~ \tilde{\mathbf{z}}^{i}=\frac{\mathbf{z}^{i}-\mathbf{\mu}}{\mathbf{\sigma}}$ 这导致了之后的每一个 feature 因标准化不再相互独立，变得相互影响</p>
</li>
</ul>
<p>Batch Normalization</p>
<ul>
<li><p>实作只对一个 batch 做 normalization，而不是对整个 dataset 做 normalization  -&gt; Batch Normalization</p>
</li>
<li><p>Batch Normalization：适用于batch size较大的情况</p>
</li>
<li><p>通常还会做：$\hat{\mathbf{z}}^{i}=\boldsymbol{\gamma} \odot \tilde{\mathbf{z}}^{i}+\boldsymbol{\beta}$, $\boldsymbol{\gamma}$ 和 $\boldsymbol{\beta}$ 是可学习的参数（learnable parameters），$\odot$ 是 element-wise multiplication。原因: 可能标准化后均值为 0 给 network 的限制会带来负面影响，$\boldsymbol{\gamma}$ 和 $\boldsymbol{\beta}$ 用于调整分布。$\boldsymbol{\gamma}$ 和 $\boldsymbol{\beta}$ 的初始值：$\boldsymbol{\gamma}=\mathbf{1}, \boldsymbol{\beta}=\mathbf{0}$</p>
</li>
</ul>
<p>Batch Normalization - Testing</p>
<ul>
<li>问题：我们并不总是在测试阶段有批处理（凑不够一个batch的数据无法计算均值和方差） -&gt; 解决：计算训练过程中批次的μ和σ的移动平均值，用作测试阶段的μ和σ</li>
</ul>
<p>Batch Normalization 为什么能帮助训练优化？</p>
<ul>
<li><p>实验结果(和理论分析)支持 Batch Normalization 会改变error surface 的 landscape</p>
</li>
<li><p>但是改变error surface 的 landscape 的方式还有其它的很多种，所以 Batch Normalization 属于偶然的一次发现</p>
</li>
</ul>
<hr>
<h2 id="Lecture-3-Image-as-input"><a href="#Lecture-3-Image-as-input" class="headerlink" title="Lecture 3:Image as input"></a>Lecture 3:Image as input</h2><h3 id="【機器學習2021】卷積神經網路-Convolutional-Neural-Networks-CNN"><a href="#【機器學習2021】卷積神經網路-Convolutional-Neural-Networks-CNN" class="headerlink" title="【機器學習2021】卷積神經網路 (Convolutional Neural Networks, CNN)"></a>【機器學習2021】卷積神經網路 (Convolutional Neural Networks, CNN)</h3><p>Convolutional Neural Network (CNN)：Network Architecture designed for Image（为图像设计的网络架构） </p>
<p>Image Classification</p>
<ul>
<li><p>假设：所有输入的图片都是同样的大小（$100 \times 100$）</p>
</li>
<li><p>模型目标：分类（把每一个类别表示成一个 One-Hot 的 Vector）（维度多少：能区分类别的数量）</p>
</li>
<li><p>图片：3 - D Tensor（$100 \times 100 \times 3$）（3：RGB）</p>
<ul>
<li>拉直为一个向量（3·100·100），其中每一维的数值表某一个位置某一个颜色的强度</li>
<li>如果把上述向量输入Fully Connected Network，那么参数数量会非常大（$3 \times 100 \times 100 \times 1000 = 30,000,000$）,虽然增加了模型的弹性及能力，但是也增加了 Overfitting 的风险 -&gt; 不一定要用 Fully Connected Network</li>
<li>观察一：图像识别一些关键的特征（Pattern）即可<ul>
<li>每一个 Neuron 只看图片的一小块（Receptive Field），如：$3 \times 3 \times 3$，然后将其展开为 3·3·3 维的向量，再作为 Neuron 的输入，Neruon 给 27 维向量每一个 dimension 一个 weight，再加上 bias 得到输出，作为下一层 Neruon 的输入</li>
<li>如何决定每一个 Neuron 的 Receptive Field？。。。 </li>
<li>Receptive Field 之间可以重叠，甚至可以完全重合，也可以只选择三个  channels 中的一个或两个…（任意像素及channel的组合都可以）</li>
</ul>
</li>
<li>简化一：最经典的 Receptive Field 安排方式（Typical Setting）：<ul>
<li>all channels：所有的 Receptive Field 均包括所有的 channels，因此表述 Receptive Field 的时候不需要表明 channels，只需要表明长和宽，长和宽合起来称为 kernel size（$e.g.,3 \times 3$）</li>
<li>每一个 Receptive Field 均有一组 Neurons （e.g.，64 neurons）</li>
<li>一个 Receptive Field 向右移动 stride（Hyperparameter）个像素，就得到了下一个 Receptive Field，通常 stride 会是 1 或 2（overlap），因为 stride 太大会导致信息丢失（两个 Receptive Field 不重叠了，信息出现在交界处时会丢失）</li>
<li>padding：当 stride 大于 1 时，可能会导致最后一个 Receptive Field 无法完全覆盖图片，因此需要在图片的边缘补 0，或者行平均值等合适值（padding 方法有很多种）</li>
</ul>
</li>
<li>观察二：相同的 patterns 会出现在图片不同的地方<ul>
<li>让不同 Receptive Field 的 Neuron 共享参数</li>
<li>两个完全重合的 Receptive Field 不会共享参数</li>
<li>如何共享参数有多种方法 </li>
</ul>
</li>
<li>简化二：最经典的参数共享的方式<ul>
<li>每个 Receptive Field 都有一组 neurons (e.g., 64 neurons)</li>
<li>每个 Receptive Field 对应的 neurons 参数都相同（都只有一组参数）（公用的参数称为 filter，则对应的 neurons 参数分别为 filter1，filter2，…）</li>
</ul>
</li>
<li>Convolutional Layer = Receptive Field + Parameter Sharing</li>
<li>Convolutional Neural Network （CNN） ：用Convolutional Layer 的 Network<ul>
<li>model bias（模型偏差）较大，但是 Overfitting 的风险较小，但因为是专门为影像的特性设计优化，因此在影像上的表现与 Fully Connected Network 相比仍然不错</li>
</ul>
</li>
</ul>
</li>
<li><p>Convolutional Layer</p>
<ul>
<li>Convolution：包含很多 Filter，每个 Filter 是 $3 \times 3 \times channel$ 的 Tensor （也就是有 3·3·channel 个参数） </li>
<li>每个 Filter 的作用：在对应的 $3 \times 3 \times channel$ 范围内识别（抓取）pattern</li>
<li>每一个 Filter 都以 stride 的间隔扫过整张图片（convolve），得到 Feature Map -&gt; 得到一张 filter_num 个 channel 的新图片</li>
</ul>
</li>
<li><p>neurons 对不同的 receptive fields 共享参数 $\leftrightarrow$ 每一个 filter 扫过整张图片（对整个输入的 image 作卷积）（Convolution）</p>
</li>
<li><p>Pooling：观察三：对一张比较大的图片做 subsampling （降采样）（e.g., 将图片的像素的奇数行去掉，偶数列去掉）后，图片具有的信息并没有丢失太多</p>
<ul>
<li>Pooling 本身没有参数，只是对图片做一些操作</li>
<li>Max Pooling：将图片划分为不重叠的小块，每一块取最大值，得到新的图片</li>
<li>各种 Pooling 的方式有很多种，均为对图片划分为不重叠的小块，然后对每一块做一些操作（降采样），得到新的图片</li>
<li>在实作上，Convolution 和 Pooling 通常交替使用</li>
<li>Pooling 的作用：降低图片的大小（subsampling），减少参数数量，减少计算量，减少 Overfitting 的风险；但如果 Pooling 太多，可能会导致信息丢失，因此如果运算资源足够支撑，可以不用 Pooling</li>
</ul>
</li>
<li><p>Flatten：将图片展开为向量，作为 Fully Connected Network 的输入</p>
</li>
<li><p>整个经典 CNN 的架构</p>
<ul>
<li>图片 -&gt; Convolution -&gt; Pooling -&gt; Convolution -&gt; Pooling -&gt; Flatten -&gt; Fully Connected Network -&gt; Softmax -&gt; 输出</li>
</ul>
</li>
<li><p>Application: Playing Go</p>
<ul>
<li>围棋：19 x 19 vector -&gt; Neural Network -&gt; Next Move（19 x 19 positions） 19 x 19 classes</li>
<li>在这个项目上，CNN 有着比 Fully Connected Network 更好的表现</li>
<li>CNN：把围棋的棋盘当作图片，每一个位置都是一个 pixel，每一个 pixel 都有 48 个 channels 来表示不同的信息（e.g., 有没有棋子，棋子的颜色，棋子的气等等），因此输入的图片是 19 x 19 x 48 的 Tensor</li>
<li>为什么 CNN 能用于围棋？：围棋具有和图像相似的特性<ul>
<li>有些 pattern 比整个图像小得多</li>
<li>相同的 patterns 出现在不同的地区</li>
<li>不使用 Pooling ，因为围棋并不是降采样不影响结果的情况 </li>
</ul>
</li>
</ul>
</li>
<li><p>More Applications</p>
<ul>
<li>Speech</li>
<li>Natural Language Processing</li>
<li>均需要根据对应的特性设计合适的 Receptive Field</li>
</ul>
</li>
<li><p>CNN 不能处理影像放大、缩小、旋转等操作，因为这些操作会导致图片的 pattern 发生变化，因此 CNN 对于这些操作并不具有 Invariance（不变性）</p>
<ul>
<li>因此我们需要 Data Augmentation（数据增强）来增加数据量，同时也可以减少 Overfitting 的风险</li>
<li>可以处理上述问题的 Network：Spatial Transformer Layer</li>
</ul>
</li>
</ul>
<hr>
<h2 id="Lecture-4-Sequence-as-input"><a href="#Lecture-4-Sequence-as-input" class="headerlink" title="Lecture 4:Sequence as input"></a>Lecture 4:Sequence as input</h2><h3 id="【機器學習2021】自注意力機制-Self-attention-上"><a href="#【機器學習2021】自注意力機制-Self-attention-上" class="headerlink" title="【機器學習2021】自注意力機制 (Self-attention) (上)"></a>【機器學習2021】自注意力機制 (Self-attention) (上)</h3><p>复杂的输入</p>
<ul>
<li><p>输入为一个向量 </p>
<ul>
<li>a vector -&gt; Model -&gt; Scalar or Class</li>
</ul>
</li>
<li><p>输入为一组向量？</p>
<ul>
<li><p>a set of vector(may change length) -&gt; Model -&gt; Scalars or Classes</p>
</li>
<li><p>例子1：文字处理，每个句子长度不一样，把句子中每个词汇描述成一个向量，一句中的所有词汇作为一组向量输入模型</p>
<ul>
<li>如何用向量表示一个单词？</li>
<li>One-hot Encoding -&gt; 每个单词均对应且只对应一个维度 -&gt; 无法体现词汇之间的关联性</li>
<li>Word Embedding -&gt; 给要处理的每一个词汇一个向量，此向量具有语义资讯</li>
</ul>
</li>
<li><p>例子2：音频处理，对一段声音讯号取一个范围（Window）（通常取25ms），此范围中的讯号描述为一个向量（Frame），然后平移该范围（通常取10ms），得到下一小段声音讯号，转化为下一个向量</p>
</li>
<li><p>例子3：图（包括许多可以使用图解释的需要输入模型的东西，如分子），把图中每一个节点视作一个向量</p>
</li>
</ul>
</li>
</ul>
<p>输出是？</p>
<ol>
<li><p>每一个向量对应一个标签（Scalar or Class）：此时输入和输出等长（Sequence Labeling）</p>
<ul>
<li>举例：POS Tagging（Part of Speech Tagging）：给每一个词汇标注词性 </li>
<li>举例：语音识别：给每一个 Frame 标注音素（Phoneme）</li>
<li>举例：social network analysis：给每一个节点标注类别 </li>
</ul>
</li>
<li><p>一组向量（sequence）只输出一个标签</p>
<ul>
<li>举例：Sentiment Analysis：给一段话标注情感（positive or negative）</li>
<li>举例：语者识别：给一段声音讯号标注说话者</li>
<li>举例：Graph Classification：给一个图标注类别</li>
</ul>
</li>
<li>模型自己决定输出几个标签（seq2seq）<ul>
<li>举例：Translation：给一段话翻译成另一种语言</li>
<li>真·语音识别：输入一段音频，输出一段话</li>
</ul>
</li>
</ol>
<p>Sequence Labeling：</p>
<ul>
<li>不能直接使用Fully Connected Network，因为相同的输入可能因为上下文不同需要不同的输出（此时一种不那么优的解：输入为Window，其中包含需要识别的向量以及上下文各一部分向量）</li>
<li>需要考虑整个Input Sequence的信息（全局信息）：Self-attention</li>
</ul>
<p>Self-attention：</p>
<ul>
<li>输入n个向量 -&gt; Self-attention -&gt; 输出n个向量（每一个向量都考虑了整个输入序列的信息（包含context）） -&gt; 每个向量均输入 Fully Connected Network -&gt; 输出</li>
<li>上述输出还可以再输入一个 Self-attention Layer，再输入 Fully Connected Network，如此循环多次 </li>
<li>Self-attention 经典应用：Transformer （Paper：Attention is All You Need）</li>
<li>输入： 一组向量（$a^1, a^2, \dots, a^n$），可以是整个network的输入，也可以是某一层（hidden layer）的输出</li>
<li>输出： 一组向量（$b^1, b^2, \dots, b^n$），每个向量都考虑了整个输入序列的信息（包含context）</li>
<li>如何产生 $b^1$ ？：<ol>
<li>根据 $a^1$ 找出 sequence 中每一个与 $a^1$ “相关”的其他向量</li>
<li>如何确定两个向量之间的相关性 $\alpha$ （attention score）？有多种做法。常见做法：Dot-product：$\alpha = a^i W^q \cdot a^j W^k$（最常用且被用在Transformer中），Additive：$\alpha = W \tanh(W^q a^i + W^k a^j)$ …</li>
<li>计算 query：$q^1 = W^q a^1$，计算 key：$k^{2,3,…,n} = W^k a^{2,3,…,n}$，计算 attention score：$\alpha<em>{1,1} = q^1 k^1$（自身之间也需要计算关联性）, $\alpha</em>{1,2} = q^1 k^2$，$\alpha<em>{1,3} = q^1 k^3$，…，$\alpha</em>{1,n} = q^1 k^n$</li>
<li>对 attention score $\alpha<em>{1,1}, \alpha</em>{1,2}…$ 使用 softmax：$\alpha<em>{1,1}^{\prime} = \frac{\exp(\alpha</em>{1,1})}{\sum<em>{j=1}^{n} \exp(\alpha</em>{1,j})}$，$\alpha<em>{1,2}^{\prime} = \frac{\exp(\alpha</em>{1,2})}{\sum<em>{j=1}^{n} \exp(\alpha</em>{1,j})}$，…，$\alpha<em>{1,n}^{\prime} = \frac{\exp(\alpha</em>{1,n})}{\sum<em>{j=1}^{n} \exp(\alpha</em>{1,j})}$ （不一定使用 softmax，也可以使用别的 Activation Function）（normalization）</li>
<li>根据得到的 attention score $\alpha<em>{1,1}^{\prime}, \alpha</em>{1,2}^{\prime}…$ 抽取出 sequence 中重要的信息：取 $v^1 = W^v a^1$，$v^2 = W^v a^2$，…，$v^n = W^v a^n$，然后根据 attention score $\alpha<em>{1,1}^{\prime}, \alpha</em>{1,2}^{\prime}…$ 得到 $b^1 = \sum<em>{j=1}^{n} \alpha</em>{1,j}^{\prime} v^j = \alpha<em>{1,1}^{\prime} v^1 + \alpha</em>{1,2}^{\prime} v^2 + … + \alpha_{1,n}^{\prime} v^n$</li>
<li>关联性可以通过 $b^1$ 和每一个 $v^i$ 的接近程度体现</li>
</ol>
</li>
<li>$b^2, b^3, …$ 的计算与 $b^1$ 的计算类似，并且不需要有先后计算关系，因此可以并行计算（parallel）</li>
</ul>
<h3 id="【機器學習2021】自注意力機制-Self-attention-下"><a href="#【機器學習2021】自注意力機制-Self-attention-下" class="headerlink" title="【機器學習2021】自注意力機制 (Self-attention) (下)"></a>【機器學習2021】自注意力機制 (Self-attention) (下)</h3><p>从矩阵乘法角度看 Self-attention 是如何工作的？</p>
<ul>
<li>每一个 $a^i$ 都分别产生 $q^i, k^i, v^i$ ：$q^i = W^q a^i$，$k^i = W^k a^i$，$v^i = W^v a^i$</li>
<li>则有：$\left[q^1 \quad q^2 \quad \cdots \quad q^n\right] = W^q \left[a^1 \quad a^2 \quad \cdots \quad a^n\right]$，$\left[k^1 \quad k^2 \quad \cdots \quad k^n\right] = W^k \left[a^1 \quad a^2 \quad \cdots \quad a^n\right]$，$\left[v^1 \quad v^2 \quad \cdots \quad v^n\right] = W^v \left[a^1 \quad a^2 \quad \cdots \quad a^n\right]$，令 $Q = \left[q^1 \quad q^2 \quad \cdots \quad q^n\right]$，$K = \left[k^1 \quad k^2 \quad \cdots \quad k^n\right]$，$V = \left[v^1 \quad v^2 \quad \cdots \quad v^n\right]$，$I = \left[a^1 \quad a^2 \quad \cdots \quad a^n\right]$, 则有 $Q = W^q I$，$K = W^k I$，$V = W^v I$</li>
<li>$\alpha<em>{1,1} = {k^1}^T q^1$，$\alpha</em>{1,2} = {k^2}^T q^1$，…，$\alpha_{1,n} = {k^n}^T q^1$</li>
<li>则有 $\left[\begin{array}{cccc} \alpha<em>{1,1} \ \alpha</em>{1,2} \ \vdots \ \alpha_{1,n} \end{array}\right] = \left[\begin{array}{cccc} {k^1}^T \ {k^2}^T \ \vdots \ {k^n}^T \end{array}\right] \left[\begin{array}{c} q^1 \ q^1 \ \vdots \ q^1 \end{array}\right]$</li>
<li>则有 $\left[\begin{array}{cccc} \alpha<em>{1,1} &amp; \alpha</em>{2, 1} &amp; … &amp; \alpha<em>{n, 1} \ \alpha</em>{1,2} &amp; \alpha<em>{2, 2} &amp; … &amp; \alpha</em>{n, 2} \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ \alpha<em>{1, n} &amp; \alpha</em>{2, n} &amp; … &amp; \alpha_{n, n} \end{array}\right] = \left[\begin{array}{cccc} {k^1}^T \ {k^2}^T \ \vdots \ {k^n}^T \end{array}\right] \left[\begin{array}{cccc} q^1 &amp; q^2 &amp; … &amp; q^n \end{array}\right]$ , 记作 $A = K^T Q$</li>
<li>对A中的每一列使用softmax： $A^{\prime} = softmax(A) = \left[\begin{array}{cccc} \alpha<em>{1,1}^{\prime} &amp; \alpha</em>{2, 1}^{\prime} &amp; … &amp; \alpha<em>{n, 1}^{\prime} \ \alpha</em>{1,2}^{\prime} &amp; \alpha<em>{2, 2}^{\prime} &amp; … &amp; \alpha</em>{n, 2}^{\prime} \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ \alpha<em>{1, n}^{\prime} &amp; \alpha</em>{2, n}^{\prime} &amp; … &amp; \alpha_{n, n}^{\prime} \end{array}\right]$，（Attention Matrix）</li>
<li>$\left[b^1 \quad b^2 \quad \cdots \quad b^n\right] = \left[v^1 \quad v^2 \quad \cdots \quad v^n\right] \left[\begin{array}{cccc} \alpha<em>{1,1}^{\prime} &amp; \alpha</em>{2, 1}^{\prime} &amp; … &amp; \alpha<em>{n, 1}^{\prime} \ \alpha</em>{1,2}^{\prime} &amp; \alpha<em>{2, 2}^{\prime} &amp; … &amp; \alpha</em>{n, 2}^{\prime} \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ \alpha<em>{1, n}^{\prime} &amp; \alpha</em>{2, n}^{\prime} &amp; … &amp; \alpha_{n, n}^{\prime} \end{array}\right]$，记作 $O = V A^{\prime}$</li>
<li>此时 O 即为 self-attention 输出： $b^1, b^2, \dots, b^n$</li>
<li>Attention score $\alpha<em>{1,1}^{\prime}, \alpha</em>{1,2}^{\prime}…$ 体现了每一个 $a^i$ 与 $a^1$ 的相关性</li>
<li>$b^1, b^2, \dots, b^n$ 体现了每一个 $a^i$ 与整个输入序列的相关性</li>
<li>因此，训练参数只有 $W^q, W^k, W^v$</li>
</ul>
<p>认为问题有多种不同的相关性：Multi-head Self-attention：</p>
<ul>
<li>$q^i = W^q_i a^i$，$q^{i, 1} = W^{q, 1} q^i$，$q^{i, 2} = W^{q, 2} q^i$…</li>
<li>相当于有多个 Self-attention Layer，每一个 Self-attention Layer 有不同的 $W^q, W^k, W^v$，最后得到 $b^{i, 1}, b^{i, 2}, …$，这个就是不同的相关性，可以把这些相关性合并起来（如通过一个Transform），得到 $b^i$</li>
</ul>
<p>Positional Encoding（位置编码）：</p>
<ul>
<li>在 self-attention 中，并没有位置信息，如果需要位置信息，可以加上位置编码（Positional Encoding），比如在词性标注中，词的位置对词性标注有影响</li>
<li>为每一个位置设定一个向量（positional vector）$e^i$，然后加到输入向量中：$a^i = a^i + e^i$</li>
<li>$e^i$ 是手动设计的（hand-crafted），可以是任意的，但是需要有规律，可以通过公式生成，甚至可以是根据资料学习出来的</li>
<li>每一个位置都有专属的 $e^i$，希望透过给每一个位置不同的 $e^i$，让模型在处理 Input Sequence 时能够考虑到位置信息</li>
</ul>
<p>Self-attention 的应用：</p>
<ul>
<li>Transformer：用于语言模型、翻译、语音识别等</li>
<li>BERT：用于自然语言处理</li>
<li>广泛应用于自然语言处理领域（NLP）</li>
<li>其他领域：</li>
<li>语音识别：<ul>
<li>语音是一排非常长的向量序列（因为声音讯号识别的时间间隔通常是10ms，因此一句话通常有成千上万个向量） </li>
<li>问题：self-attention 中计算输入长度为L的向量序列的 Attention Matrix 的时间复杂度是 $L^2$，因此对于长序列的计算量非常大，训练时间非常长</li>
<li>解决方法：Truncated Self-attention，不看一整句话，而是只看一小范围中的话，这样可以加快训练速度</li>
</ul>
</li>
<li>图片处理：<ul>
<li>图片除了在CNN中视为一整个向量，也可以视为一个向量序列（每一个像素点都是一个向量）</li>
<li>应用：Self-Attention GAN，DEtection Transformer（DETR）</li>
</ul>
</li>
<li>图（Graph）：<ul>
<li>图中的每一个节点都是一个向量，因此可以视为一个向量序列</li>
<li>图中的边：Attention Matrix 可以只计算有边相连的节点之间的关联性，其他节点之间的attention score为0</li>
<li>此时的 Self-attention 可以视为一种 Graph Neural Network（GNN）</li>
</ul>
</li>
</ul>
<p>Self-attention 最大问题：计算量大。<br>Self-attention 最早使用于 Transformer，因此Self-attention 的许多变形都是xxx-former</p>
<p>Self Attention 和 CNN 比较：</p>
<ul>
<li>CNN：通过局部的 Receptive Field 来识别 pattern -&gt; CNN 是一种简化的 Self-attention</li>
<li>Self-attention：通过全局的信息来识别 pattern -&gt; Self-attention 是一种更复杂的 CNN。通过attention找出相关的pixel，就好像receptive field是自动被学出来的</li>
<li>其实CNN就是一种特殊的Self-attention，通过对Self-attention设定一定的参数，就可以做到和CNN一样的效果</li>
<li>Self-attention 更flexible（灵活），但是需要更多的 data 来训练，data不够则容易overfitting</li>
<li>CNN 更适合于小数据集，因为CNN的参数更少，更不容易overfitting</li>
</ul>
<p>Self-attention 和 RNN 比较：</p>
<ul>
<li>输入和输出均为一组向量 </li>
<li>不同：Self-attention 可以轻易地考虑输入向量序列中间隔很远的向量的信息</li>
<li>不同：Self-attention 可以并行计算，而RNN只能串行计算，因此Self-attention 计算速度更快，效率更高，更有优势，因此在很多领域取代了RNN</li>
</ul>
<hr>
<h2 id="Lecture-5-Sequence-to-sequence"><a href="#Lecture-5-Sequence-to-sequence" class="headerlink" title="Lecture 5:Sequence to sequence"></a>Lecture 5:Sequence to sequence</h2><h3 id="【機器學習2021】Transformer-上"><a href="#【機器學習2021】Transformer-上" class="headerlink" title="【機器學習2021】Transformer (上)"></a>【機器學習2021】Transformer (上)</h3><p>Transformer：Sequence to Sequence Model（Seq2seq）<br>输出是一组向量（sequence）：输入一组向量（sequence） -&gt; Model -&gt; 输出一组向量（sequence），此时输出向量的长度是由模型决定的</p>
<p>应用场景：</p>
<ul>
<li>语音识别（Speech Recognition）：输入一段音频，输出一段文字</li>
<li>机器翻译（Machine Translation）：输入一段文字，输出另一种语言的文字 </li>
<li>语音翻译（Speech Translation）：输入一段音频，输出另一种语言的文字（原因：有很多种语言没有文字，只有口语）（训练数据可以从电视剧中得到，因为电视剧有语音-&gt;文字字幕）</li>
</ul>
<p>“硬train一发”：不考虑其他因素，直接向模型倒进去大量的数据，然后让模型自己学习</p>
<p>Text-to-Speech Synthesis（TTS）：输入一段文字，输出一段音频</p>
<p>Seq2seq for Chatbot: 输入一段文字，输出一段文字（vector sequence）。</p>
<ul>
<li>训练：收集大量人与人的对话</li>
<li>在 natural language processing 中，seq2seq 是一个非常重要的模型，很多 natural language processing 可以视为 Question-Answering （QA）问题（给机器 Context，问机器 Question，机器给出 Answer）（如翻译，摘要（entailment），判断评价（sentiment analysis）等），可以被 Seq2seq 解决，因此 seq2seq 在 natural language processing 中有着非常广泛的应用。</li>
<li>question, context -&gt; seq2seq -&gt; answer</li>
<li>对各种任务客制化模型会比单用Seq2seq更好</li>
</ul>
<p>文法剖析（Syntactic Parsing）：输入一段文字，输出文法树（Syntactic Tree）</p>
<ul>
<li>可以硬用 Seq2seq，输出变为文法树的前序遍历（字符串），把文法当做是一种语言，转变为翻译问题，然后用 Seq2seq 做翻译</li>
</ul>
<p>多标记分类（Multi-label Classification）：一个物体可以属于多个类别</p>
<ul>
<li>可以硬用 Seq2seq，输出变为多个向量，每一个向量对应一个Class</li>
</ul>
<p>对象检测（Object Detection）： 输入一张图片，输出图片中的物体的位置和类别（矩形框）</p>
<ul>
<li>也可以硬用 Seq2seq</li>
</ul>
<p>Seq2seq Model：</p>
<ul>
<li>Encoder-Decoder Model：输入一组向量（Input sequence） -&gt; Encoder -&gt; Decoder -&gt; 输出一组向量（Output sequence）</li>
</ul>
<p>Encoder：</p>
<ul>
<li>输入：一组向量（$x^1, x^2, \dots, x^n$），输出：一组向量（$h^1, h^2, \dots, h^n$）</li>
<li>Block：一组层（Layer）的集合，输入输出都是一排向量，（$x^1, x^2, \dots, x^n$）-&gt; Block -&gt; Block -&gt; … -&gt; Block -&gt; （$h^1, h^2, \dots, h^n$）<ul>
<li>在 Transformer 的 Encoder 中，Block 通常是 Self-attention （Multi-Head Attention） + 对 Self-attention 的输出逐个向量做 Fully Connected 的 Feed Forward Network（FFN）</li>
<li>Self-attention 里面需要额外做的：(Add &amp; Norm)<ul>
<li>residual connection：每一个输入向量 $a^i$ 经过 Self-attention 得到 $b^i$ 后，$b^i$ 与 $a^i$ 相加，得到 $c^i$，$c^i$ 作为接下来的输入</li>
<li>layer normalization：在 $c^i$ 上做 normalization，得到 $d^i$，$d^i$ 作为接下来的输入<ul>
<li>对同一个example里面不同的dimension计算mean（m）和standard deviation（$\sigma$），然后对每一个dimension都减去m再除以 $\sigma$ : $x^{i’} = \frac{x^i - m}{\sigma}$</li>
</ul>
</li>
<li>$d^i$ 才是 Feed Forward Network（FFN）的输入</li>
</ul>
</li>
<li>FFN 里面也有需要额外做的：(Add &amp; Norm)<ul>
<li>residual connection：在 $d^i$ 经过 FFN 得到 $e^i$ 后，$e^i$ 与 $d^i$ 相加，得到 $f^i$，$f^i$ 作为接下来的输入</li>
<li>layer normalization：在 $f^i$ 上做 normalization，得到 $g^i$，$g^i$ 作为接下来的输入</li>
</ul>
</li>
<li>Positional Encoding：在输入向量中需要在输入之前先加入位置信息</li>
</ul>
</li>
</ul>
<h3 id="【機器學習2021】Transformer-下"><a href="#【機器學習2021】Transformer-下" class="headerlink" title="【機器學習2021】Transformer (下)"></a>【機器學習2021】Transformer (下)</h3><p>Decoder：</p>
<p>Auto-regressive Model（AT）：（使用语音识别举例）一次输出一个词，输出的词会作为输入的一部分影响下一个词的输出</p>
<ul>
<li>Begin（Special Token）：第一个输入（当做“初始”文字），作用是告诉模型开始生成输出</li>
<li>Vocabulary：模型的输出是一个词汇表中的词，词汇表中的词是模型训练时见过的词，中文的词汇表可以使用常用的几千个方块字，英文的词汇表使用 Subword（就是把单词拆成更小的单元，如：unbelievable -&gt; un, be, lie, vable）</li>
<li>输入 Decoder 中均为 One-hot Vector</li>
<li>一开始为 Begin，然后输入模型，模型输出一个词，然后把这个词与begin一起作为下一个输入，再输出一个词，然后与前两个输入词一起构成下一个输入，…，如此循环，直到输出 End（Special Token）</li>
<li>Mask Self-attention：在 Decoder 中，不能看到未来的信息，因此需要 Mask Self-attention，即在计算 attention score 时，未来的信息的 attention score 为0（如计算 $b^2$ 时， $q^2$ 只能看到 $a^1$ 和 $a^2$，不能看到 $a^3$，因此不与 $a^3$ 计算）</li>
<li>Decoder 输出应该如何决定多长？ END （Special Token）和 Begin 都在词汇表中，当Decoder输出 End 时，就可以停止输出了</li>
</ul>
<p>Non-Auto-regressive Model（NAT）：（使用语音识别举例）一次性输出所有的词</p>
<ul>
<li>一次性输入多个BEGIN，模型一次性输出对应数量的词</li>
<li>此时如何决定输出的长度？可以使用一个额外的模型来决定输出的长度，如：一个RNN，输入为Encoder的输出，输出为一个标量，表示输出的长度；也可以输入的BEGIN很多很多，最后忽略输出中的END之后的词</li>
<li>优点：平行化，速度快；比较能控制输出的长度（通过控制输入的BEGIN的数量）</li>
<li>但是 NAT 的表现通常不如 AT （为什么？Multi-modality）</li>
</ul>
<p>Encoder-Decoder：   </p>
<ul>
<li>Cross-attention：Decoder 中的每一个向量都需要考虑整个输入序列的信息（包含context），因此需要使用 Cross-attention，即 Decoder 中的每一个向量都需要与 Encoder 中的每一个向量计算 attention score<ul>
<li>具体计算：$q$ 为 Decoder 中经过 Self-attention（Mask），在经过 Transform 后 得到的向量，$k^i$，$v^i$ 为 Encoder 中 $a^i$ 经过 Transform 得到的向量，计算 attention score $\alpha<em>{i} = q k^i$，然后使用 softmax 得到 $\alpha</em>{i}^{\prime}$，最后得到 $v = \sum<em>{i=1}^{n} \alpha</em>{i}^{\prime} v^i$</li>
<li>$v$ 即为 Decoder 中的每一个向量与整个输入序列的信息（包含context）的结合，作为 Decoder 下一个 FFN 的输入</li>
</ul>
</li>
</ul>
<p>Training：</p>
<ul>
<li>训练资料：人工标注语音讯号和文字的对应关系</li>
<li>每一个字表示为一个 One-hot Vector，训练时 Decoder 输出的 distribution 与 One-hot Vector 之间的交叉熵（Cross Entropy）作为 Loss Function （和分类问题相似）</li>
<li>minimize Loss Function，得到模型的参数</li>
<li>Teacher Forcing：训练时，Decoder 的输入是真实的输出，而不是模型的输出，这样可以加速训练，但是可能会导致模型在测试时表现不佳（因为模型在训练时没有看到自己的输出）</li>
<li>Tips:<ul>
<li>Copy Mechanism：如果输入和输出有重叠，可以直接复制输入到输出中<ul>
<li>如：Chat Bot（例子：输入：我是某某某，输出：你好，某某某）</li>
<li>如：Summarization，document -&gt; Model -&gt; summary</li>
</ul>
</li>
<li>Guided Attention：如果输入和输出有对应关系，可以在计算 attention score 时，加入额外的信息，让模型更容易学习到对应关系<ul>
<li>如：语音识别中，输入和输出有对应关系，可以在计算 attention score 时，加入额外的信息，让模型更容易学习到对应关系</li>
</ul>
</li>
<li>Beam Search：原本输出的策略是 Greedy Decoding，即每一步输出概率最大的词，但是这样可能会导致输出的结果不是最好的，因此可以使用 Beam Search，即每一步输出概率最大的几个词，然后选择最好的几个词，再继续输出，如此循环，直到输出 End，这样可以得到一条较好得分的路径输出<ul>
<li>这样有可能有效，也有可能无效，要根据任务本身的特性而定：如答案非常明确时，Beam Search 可能会有效；如答案需要机器发挥创造力时（答案不唯一），Beam Search 可能会无效</li>
</ul>
</li>
<li>Randomness：在一些任务(如句子补全、PTS)中，解码器（Decoder） 在生成序列时需要随机性</li>
<li>当你不知道如何优化（Optimize）的 Loss Function 时，就使用强化学习（Reinforcement Learning）！（把他当做 RL 的 Reward，把Decoder 当做 Agent，把输出当做 Action，然后用RL的方法来训练Decoder）</li>
<li>Scheduled Sampling：在训练时，有一定概率使用模型的输出作为下一个输入，有一定概率使用真实的输出作为下一个输入，这样可以减少模型在测试时表现不佳的风险，但是这一招会伤害到 Transformer 的平行化的能力，需要改良此方法<ul>
<li>Exposure Bias：模型在训练时只看到真实的输出，而在测试时可能会看到模型的输出，因此可能会导致模型在测试时表现不佳。解决方案就是 Scheduled Sampling</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="Lecture-6-Generation"><a href="#Lecture-6-Generation" class="headerlink" title="Lecture 6:Generation"></a>Lecture 6:Generation</h2><h3 id="【機器學習2021】生成式對抗網路-Generative-Adversarial-Network-GAN-一-–-基本概念介紹"><a href="#【機器學習2021】生成式對抗網路-Generative-Adversarial-Network-GAN-一-–-基本概念介紹" class="headerlink" title="【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (一) – 基本概念介紹"></a>【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (一) – 基本概念介紹</h3><p>Network as Generator：</p>
<ul>
<li>network 输入除了 x 还有加上一个随机的变量 z，输出是y</li>
<li>z 是 simple distribution （简单分布），每次使用都是从 distribution 中 sample（取样） 出来的，distribution 可以是任意的，但是必须够简单以至于可以从中 sample 出来（如：高斯分布（Gaussian Distribution），均匀分布（Uniform Distribution），正态分布（Normal Distribution））</li>
<li>此时 y 为 complex distribution（复杂分布）</li>
<li>此时输出变为 y = G(x, z)，把这种可以输出复杂分布的 G 称作 Generator</li>
</ul>
<p>为什么输出需要是一个分布：</p>
<ul>
<li>视频预测： Previous Frame  -&gt; Model -&gt; Next Frame，此时输出应该是一个分布，而不是一个确定的值，因为下一帧有很多种可能</li>
</ul>
<p>什么时候需要用到 Generator：</p>
<ul>
<li>任务需要一点“创造力”的时候</li>
<li>同样的输入有多种可能的输出，且这些输出都是正确的时候</li>
<li>Drawing</li>
<li>Chatbot</li>
</ul>
<p>normal distribution sample 样例： 二维正态分布取样：$z = \left[\begin{array}{c} z_1 \ z_2 \end{array}\right]$，$z_1$ 和 $z_2$ 都是从正态分布中取样出来的，然后 $z$ 作为输入输入到 Generator 中，得到 $y = G(z)$，$y$ 是一个二维的向量，可以视为一个点，这样可以得到一个二维的分布，这个分布是由 Generator 生成的</p>
<p>Generative Adversarial Network（GAN）：</p>
<ul>
<li><p>Anime Face Generation （二次元头像生成）</p>
<ul>
<li>Unconditional Generation： z -&gt; G -&gt; y（z: low dimension vector，y: anime face，hign dimension vector）（z是normal distribution sample出来的）</li>
<li>Discriminator：也是一个 Neural Network（架构自己设计），输入是 y，输出是一个标量，表示 y 的真实性（真实的概率）</li>
<li>Discriminator 的训练目标：让 Discriminator 能够区分真实的 y 和 G 生成的 y</li>
<li>Discriminator 的存在的意义：与 Generator 对抗（Adversarial），让 Generator 生成的 y 越来越真实</li>
</ul>
</li>
<li><p>Algorithm（算法）：</p>
<ul>
<li>初始化 Generator 和 Discriminator 的参数</li>
<li>在每一次训练中：<ul>
<li>固定 Generator，训练（更新） Discriminator，训练材料为 Generator 生成的图像和真实的图像，目标是让 Discriminator 能够区分真实的图像和 Generator 生成的图像</li>
<li>固定 Discriminator，训练（更新） Generator，目的是让 Discriminator 识别 Generator 生成的图像为真实的图像，实际上：把 Generator 和 Discriminator 整体看作一个大的 network，此时输入为 vector ，输出为 Discriminator 的输出（评分标量） ，目标是让 Discriminator 的输出尽可能大，但在训练过程中调整的参数只是 Generator 的参数，同样是使用 Gradient Descent 或者 Gradient Ascent 来调整参数</li>
<li>重复上述步骤直到收敛</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="【機器學習2021】生成式對抗網路-Generative-Adversarial-Network-GAN-二-–-理論介紹與WGAN"><a href="#【機器學習2021】生成式對抗網路-Generative-Adversarial-Network-GAN-二-–-理論介紹與WGAN" class="headerlink" title="【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (二) – 理論介紹與WGAN"></a>【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (二) – 理論介紹與WGAN</h3><p>目标： </p>
<ul>
<li>Normal Distribution（常规分布） -&gt; Generator -&gt; $P_G$ （复杂分布）</li>
<li>真实的分布 $P<em>{data}$ 和 $P</em>{G}$ 越接近越好</li>
<li>使用 Divergence（差异、相似度）： $G^* = \arg \min<em>G Div(P</em>{data} || P_G)$</li>
</ul>
<p>GAN的问题：如何定义与计算 Divergence</p>
<p>解决方法：从 $P<em>{data}$ 和 $P</em>{G}$ 中采样（sample）出一些数据，</p>
<ul>
<li>$P_{data}$ 的采样：直接从数据集中采样</li>
<li>$P<em>{G}$ 的采样：从 Normal Distribution 中采样出一些数据，然后输入到 Generator 中，得到 $P</em>{G}$ 的采样</li>
</ul>
<p>Discriminator：混合 $P<em>{data}$ 和 $P</em>{G}$ 的采样，然后训练一个 Discriminator，训练目标：让 Discriminator 能够区分 $P<em>{data}$ 和 $P</em>{G}$ 的采样</p>
<script type="math/tex; mode=display">
  Training:  D^{*}=\arg \max _{D} V(D, G) \\
  Objective \ Function \ for \ D : \\
  \ \ \ \ \ \ \ V(G, D)=E_{y \sim P_{\text {data }}}[\log D(y)]+E_{y \sim P_{G}}[\log (1-D(y))]</script><p>Objective Function 有其他的写法，这个写法的含义是训练一个分类器（Classifier）， $\max _{D} V(D, G)$ 就等同于最小话cross entropy</p>
<p>Discriminator ：a binary classifier</p>
<p>$\max _{D} V(D, G)$ ：与 JS Divergence 相关 （Jensen-Shannon Divergence），训练后最大的Objective Function值与 Divergence 有关</p>
<p>把 $Div(P<em>{data} || P_G)$ 替换为 $\max </em>{D} V(D, G)$，因此 $G^* = \arg \min<em>G \max </em>{D} V(D, G)$</p>
<p>求解步骤：</p>
<ul>
<li>Initialize generator G and discriminator D</li>
<li>In each iteration:<ul>
<li>Step 1: Fix generator G, and update discriminator D</li>
<li>Step 2: Fix discriminator D, and update generator G</li>
</ul>
</li>
</ul>
<p>$D<em>{f}(P</em>{data} || P_G)$ 可以不是这个 divergence，可以是其他的 divergence，如：Wasserstein Distance, KL Divergence, 等等，不同的 divergence 需要设计不同的 objective function</p>
<p>$G^* = \arg \min<em>G \max </em>{D} V(D, G)$ 形式复杂，因此不是很好 train</p>
<p>JS Divergence 的问题：</p>
<ul>
<li>在大多数情况下，$P_{data}$ 和 $P_G$ 是几乎不重叠的</li>
<li>原因：<ol>
<li>$P_{data}$ 和 $P_G$ 均为低维向量在高维空间中的分布，几乎不可能重叠</li>
<li>$P_{data}$ 和 $P_G$ 均为采样出来的数据，几乎不可能重叠</li>
</ol>
</li>
<li>JS Divergence 对于不重叠的分布算出来结果均为 log(2) （常数），因此均为 log(2) 时无法进一步区分两个分布的好坏程度，进而在训练时无法优化</li>
<li>由于采样样本少导致 Classifier 正确率几乎100%，因此不知道训练时 generator 的效果如何</li>
</ul>
<p>Wasserstein Distance：</p>
<ul>
<li>原理：两个分布之间的距离，是 $P<em>G$ 转变为 $P</em>{data}$ 的最小运输成本（最小运输距离）</li>
<li>可以解决 JS Divergence 不能区分不重叠分布的问题</li>
<li>优点：可以提供训练方向</li>
</ul>
<p>计算公式：</p>
<script type="math/tex; mode=display">
  \max _{D \in 1-\text { Lipschitz }}\left\{E_{y \sim P_{\text {data }}}[D(y)]-E_{y \sim P_{G}}[D(y)]\right\}</script><p>D:足够平滑的函数</p>
<ul>
<li>如果没有约束，则 D 的训练就不会收敛</li>
</ul>
<p>Original GAN ：weight</p>
<ul>
<li>强制参数 w 位于 -c, c 之间，c 为常数，在参数更新时，如果超出 -c, c 之间，就把参数强制拉回 -c, c 之间 </li>
</ul>
<p>Improved GAN ：Gradient Penalty</p>
<p>Spectral Normalization GAN ：Spectral Normalization -&gt; 是目前最好的方法</p>
<p>但 GAN 依旧是难以训练的。</p>
<p>Generator 和 Discriminator 是相互进步的，但只要有一者出现问题，两者都会停滞，所以训练时的Hyperparameter的调整是十分困难的</p>
<p>最困难：使用GAN生成语句<br>token：使用什么作文产生的句子的单位<br>在Decoder做出微调时，token的得分会有细微变化，但得分最大的token可能并没有改变，因此生成的句子可能并没有改变，所以无法梯度下降，也就是无法继续训练</p>
<p>不能梯度下降（Gradient Descent）的问题 -&gt; 当做强化学习（Reinforcement Learning）来解决，但是会变得更难以训练</p>
<p>ScratchGAN：从头开始训练的GAN<br>通常，生成器是根据其他方法学习到的模型进行微调的。<br>然而，有了足够且大量的超参数调整和技巧，ScrachGAN可以从头开始训练。</p>
<p>更多的生成式模型（Generative Model）：VAE，flow-based model。。。</p>
<hr>
<h2 id="Recent-Advance-of-Self-supervised-learning-for-NLP"><a href="#Recent-Advance-of-Self-supervised-learning-for-NLP" class="headerlink" title="Recent Advance of Self-supervised learning for NLP"></a>Recent Advance of Self-supervised learning for NLP</h2><hr>
<h2 id="Lecture-7-Self-supervised-learning-for-Speech-and-Image"><a href="#Lecture-7-Self-supervised-learning-for-Speech-and-Image" class="headerlink" title="Lecture 7:Self-supervised learning for Speech and Image"></a>Lecture 7:Self-supervised learning for Speech and Image</h2><hr>
<h2 id="Lecture-8-Auto-encoder-Anomaly-Detection"><a href="#Lecture-8-Auto-encoder-Anomaly-Detection" class="headerlink" title="Lecture 8:Auto-encoder/ Anomaly Detection"></a>Lecture 8:Auto-encoder/ Anomaly Detection</h2><hr>
<h2 id="Lecture-9-Explainable-AI"><a href="#Lecture-9-Explainable-AI" class="headerlink" title="Lecture 9:Explainable AI"></a>Lecture 9:Explainable AI</h2><hr>
<h2 id="Lecture-10-Attack"><a href="#Lecture-10-Attack" class="headerlink" title="Lecture 10:Attack"></a>Lecture 10:Attack</h2><hr>
<h2 id="Lecture-11-Adaptation"><a href="#Lecture-11-Adaptation" class="headerlink" title="Lecture 11:Adaptation"></a>Lecture 11:Adaptation</h2><hr>
<h2 id="Lecture-12-Reinforcement-Learning"><a href="#Lecture-12-Reinforcement-Learning" class="headerlink" title="Lecture 12:Reinforcement Learning"></a>Lecture 12:Reinforcement Learning</h2><hr>
<h2 id="Lecture-13-Network-Compression"><a href="#Lecture-13-Network-Compression" class="headerlink" title="Lecture 13:Network Compression"></a>Lecture 13:Network Compression</h2><hr>
<h2 id="Lecture-14-Life-long-Learning"><a href="#Lecture-14-Life-long-Learning" class="headerlink" title="Lecture 14:Life-long Learning"></a>Lecture 14:Life-long Learning</h2><hr>
<h2 id="Lecture-15-Meta-Learning"><a href="#Lecture-15-Meta-Learning" class="headerlink" title="Lecture 15:Meta Learning"></a>Lecture 15:Meta Learning</h2><hr>

                
                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2023/10/28/项目实践 - 1/" data-toggle="tooltip" data-placement="top" title="项目实践 - 1">&larr; Previous Post
		<br>
		<span><font size="2" face="Calibri" color="grey">项目实践 - 1</font></span>
	       </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2023/09/30/保研全过程分享/" data-toggle="tooltip" data-placement="top" title="保研全过程分享">Next Post &rarr;
		<br>
		<span><font size="2" face="Calibri" color="grey">保研全过程分享</font></span>
	        </a>
                    </li>
                    
                </ul>

                <!-- tip start -->
                

                
                <div class="comment_notes">
                    <p>
                        by Tan
                    </p>
                </div>
                
                <!-- tip end -->

                <!-- Music start-->
                
                <!-- Music end -->

                <!-- Sharing -->
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            
              <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-1-Introduction-of-Deep-Learning"><span class="toc-nav-text">Lecture 1:Introduction of Deep Learning</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#預測本頻道觀看人數-上-機器學習基本概念簡介"><span class="toc-nav-text">預測本頻道觀看人數 (上) - 機器學習基本概念簡介</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#預測本頻道觀看人數-下-深度學習基本概念簡介"><span class="toc-nav-text">預測本頻道觀看人數 (下) - 深度學習基本概念簡介</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-2-What-to-do-if-my-network-fails-to-train"><span class="toc-nav-text">Lecture 2:What to do if my network fails to train</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#機器學習任務攻略"><span class="toc-nav-text">機器學習任務攻略</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#類神經網路訓練不起來怎麼辦-一-：-局部最小值-local-minima-與鞍點-saddle-point"><span class="toc-nav-text">類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#類神經網路訓練不起來怎麼辦-二-：-批次-batch-與動量-momentum"><span class="toc-nav-text">類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#類神經網路訓練不起來怎麼辦-三-：自動調整學習速率-Learning-Rate"><span class="toc-nav-text">類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#類神經網路訓練不起來怎麼辦-四-：損失函數-Loss-也可能有影響"><span class="toc-nav-text">類神經網路訓練不起來怎麼辦 (四)：損失函數 (Loss) 也可能有影響</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#類神經網路訓練不起來怎麼辦-五-：-批次標準化-Batch-Normalization-簡介"><span class="toc-nav-text">類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-3-Image-as-input"><span class="toc-nav-text">Lecture 3:Image as input</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#【機器學習2021】卷積神經網路-Convolutional-Neural-Networks-CNN"><span class="toc-nav-text">【機器學習2021】卷積神經網路 (Convolutional Neural Networks, CNN)</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-4-Sequence-as-input"><span class="toc-nav-text">Lecture 4:Sequence as input</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#【機器學習2021】自注意力機制-Self-attention-上"><span class="toc-nav-text">【機器學習2021】自注意力機制 (Self-attention) (上)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#【機器學習2021】自注意力機制-Self-attention-下"><span class="toc-nav-text">【機器學習2021】自注意力機制 (Self-attention) (下)</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-5-Sequence-to-sequence"><span class="toc-nav-text">Lecture 5:Sequence to sequence</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#【機器學習2021】Transformer-上"><span class="toc-nav-text">【機器學習2021】Transformer (上)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#【機器學習2021】Transformer-下"><span class="toc-nav-text">【機器學習2021】Transformer (下)</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-6-Generation"><span class="toc-nav-text">Lecture 6:Generation</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#【機器學習2021】生成式對抗網路-Generative-Adversarial-Network-GAN-一-–-基本概念介紹"><span class="toc-nav-text">【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (一) – 基本概念介紹</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#【機器學習2021】生成式對抗網路-Generative-Adversarial-Network-GAN-二-–-理論介紹與WGAN"><span class="toc-nav-text">【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (二) – 理論介紹與WGAN</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Recent-Advance-of-Self-supervised-learning-for-NLP"><span class="toc-nav-text">Recent Advance of Self-supervised learning for NLP</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-7-Self-supervised-learning-for-Speech-and-Image"><span class="toc-nav-text">Lecture 7:Self-supervised learning for Speech and Image</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-8-Auto-encoder-Anomaly-Detection"><span class="toc-nav-text">Lecture 8:Auto-encoder&#x2F; Anomaly Detection</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-9-Explainable-AI"><span class="toc-nav-text">Lecture 9:Explainable AI</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-10-Attack"><span class="toc-nav-text">Lecture 10:Attack</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-11-Adaptation"><span class="toc-nav-text">Lecture 11:Adaptation</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-12-Reinforcement-Learning"><span class="toc-nav-text">Lecture 12:Reinforcement Learning</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-13-Network-Compression"><span class="toc-nav-text">Lecture 13:Network Compression</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-14-Life-long-Learning"><span class="toc-nav-text">Lecture 14:Life-long Learning</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-15-Meta-Learning"><span class="toc-nav-text">Lecture 15:Meta Learning</span></a></li></ol>
            
          
          </div>
        </aside>
      
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
                        
                    </div>
                </section>
                

            </div>
			
			<div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">
				
				<br>
				<br>

				<script src="https://utteranc.es/client.js"
						repo="Master-Tan/Master-Tan.github.io"
						issue-term="pathname"
						label="Comment"
						theme="github-light"
						crossorigin="anonymous"
						async>
				</script>
			</div>
        </div>
    </div>
</article>




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>


<style  type="text/css">
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                
                    <li>
                        <a target="_blank"  href="https://github.com/Master-Tan">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="https://twitter.com/None">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="https://www.facebook.com/None">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Tan 2025 | Powered by 
                    <a href="https://github.com/Master-Tan/Master-Tan.github.io" target="_blank" rel="noopener">
                        <i>Tan's Blog</i>
                    </a>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://master-tan.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        
            <script type="text/javascript" src="/js/mouse-click.js" content='[&quot;Sunny&quot;, &quot;💖&quot;, &quot;Sunny&quot;, &quot;🧡&quot;, &quot;Sunny&quot;, &quot;💛&quot;, &quot;Sunny&quot; , &quot;💚&quot;, &quot;Sunny&quot;, &quot;💙&quot;, &quot;Sunny&quot;, &quot;💜&quot;, &quot;Sunny&quot;, &quot;😍&quot;]' color='[&quot;rgb(255, 0, 0)&quot; ,&quot;rgb(255, 0, 0)&quot; ,&quot;rgb(255, 125, 0)&quot; ,&quot;rgb(255, 125, 0)&quot; ,&quot;rgb(255, 255, 0)&quot; ,&quot;rgb(255, 255, 0)&quot; ,&quot;rgb(0, 255, 0)&quot; ,&quot;rgb(0, 255, 0)&quot; ,&quot;rgb(0, 255, 255)&quot; ,&quot;rgb(0, 255, 255)&quot; ,&quot;rgb(0, 0, 255)&quot; ,&quot;rgb(0, 0, 255)&quot; ,&quot;rgb(255, 0, 255)&quot; ,&quot;rgb(255, 0, 255)&quot;]'></script>
        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
