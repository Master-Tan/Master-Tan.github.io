<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="description" content="Tan&#39;s Blog">
    <meta name="keyword"  content="Hello world">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          机器学习 - 学习笔记 - Tan&#39;s Blog
        
    </title>

    <link rel="canonical" href="https://master-tan.github.io/2023/10/19/机器学习 - 学习笔记/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
        
<link rel="stylesheet" href="/css/dusign-light.css">

        
<link rel="stylesheet" href="/css/dusign-common-light.css">

        
<link rel="stylesheet" href="/css/font-awesome.css">

        
<link rel="stylesheet" href="/css/toc.css">

        <!-- background effects end -->
    
    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- photography -->
    
<link rel="stylesheet" href="/css/photography.css">


    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('../../../../img/default.jpg')
                /*post*/
            
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
                            
                        </div>
                        <h1>机器学习 - 学习笔记</h1>
                        <h2 class="subheading">李宏毅2021春机器学习课程</h2>
                        <span class="meta">
                            Posted by Tan on
                            2023-10-19
                        </span>

	       
                            <div class="blank_box"></div>
                            <span class="meta">
                                 <span class="post-count">2.8k</span> Words
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-light.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-light.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Tan&#39;s Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        	<li>
                          	  <a href="/about/">About Tan</a>
                        	</li>
                        
                    

                        
                        	<li>
                          	  <a href="/archive/">Archives</a>
                        	</li>
                        
                    

                        
                    

                        
                        	<li>
                          	  <a href="/tags/">Tags</a>
                        	</li>
                        
                    

                        
                    

                        
                    

                        
                    
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <p>课程视频：<a href='https://b23.tv/BV1Wv411h7kN' target="_blank" rel="noopener">https://b23.tv/BV1Wv411h7kN</a> 或 <a href='https://www.youtube.com/playlist?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J' target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J</a></p>
<p>课程网站：<a href='https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php' target="_blank" rel="noopener">https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php</a></p>
<p>课程Github：<a href='https://github.com/Fafa-DL/Lhy_Machine_Learning' target="_blank" rel="noopener">https://github.com/Fafa-DL/Lhy_Machine_Learning</a></p>
<hr>
<h2 id="Lecture-1-Introduction-of-Deep-Learning"><a href="#Lecture-1-Introduction-of-Deep-Learning" class="headerlink" title="Lecture 1:Introduction of Deep Learning"></a>Lecture 1:Introduction of Deep Learning</h2><h3 id="預測本頻道觀看人數-上-機器學習基本概念簡介"><a href="#預測本頻道觀看人數-上-機器學習基本概念簡介" class="headerlink" title="預測本頻道觀看人數 (上) - 機器學習基本概念簡介"></a>預測本頻道觀看人數 (上) - 機器學習基本概念簡介</h3><p>机器学习 -&gt; 让机器具备找函式的能力</p>
<p>专有名词：</p>
<ul>
<li>Regression：输出为数值的函式</li>
<li>Classification：给予选项，输出为选项之一的函式</li>
<li>Structured Learning：输出为结构化数据（图像，文本等）的函式</li>
</ul>
<p><strong>机器学习找函式（训练Model）的过程</strong>:</p>
<ol>
<li>写出一个带有未知参数的函式（基于领域知识） -&gt; 机器学习的模型</li>
<li>从训练数据中定义Loss （Loss也为一个关于参数的函式，代表了模型好坏程度）<ul>
<li>error surface</li>
</ul>
</li>
<li>最优化：Gradient Descent（梯度下降）<ul>
<li>取初始点</li>
<li>求该点梯度</li>
<li>向梯度方向根据学习率（hyperparameters）更新参数</li>
<li>迭代</li>
</ul>
</li>
</ol>
<h3 id="預測本頻道觀看人數-下-深度學習基本概念簡介"><a href="#預測本頻道觀看人數-下-深度學習基本概念簡介" class="headerlink" title="預測本頻道觀看人數 (下) - 深度學習基本概念簡介"></a>預測本頻道觀看人數 (下) - 深度學習基本概念簡介</h3><p>Linear Model（线性模型）： sum of (feature * weight) + bias  -&gt; 有很大局限性(Model Bias)</p>
<p>Piecewise Linear Model（分段线性模型）： constraint + sum of  a set of <strong>*Activation function*</strong> （Hard Sigmoid）</p>
<ul>
<li>曲线：取点分割后可以视为分段线性模型</li>
</ul>
<p><strong>*Activation function*</strong>:</p>
<ul>
<li><p>Sigmoid Function:</p>
<script type="math/tex; mode=display">
  \begin{align*}
      y &= c*\frac{1}{1+e^{-(b+w*x_1)}} \\
  ~\\
      &= c*sigmoid(b+w*x_1)
  \end{align*}</script><p>  线性模型到分段线性模型转化：</p>
<script type="math/tex; mode=display">
  \begin{array}{l}
      y=b+w x_{1} \rightarrow 
      y=b+\sum_{i} c_{i} sigmoid\left(\left.b_{i}+w_{i} x_{1}\right)\right. \\
  ~\\
      y=b+\sum_{j} w_{i} x_{i} \rightarrow 
      y=b+\sum_{i} c_{i} sigmoid\left(b_{i}+\sum_{j} w_{i j} x_{i}\right)
  \end{array}</script><p>  写成矩阵形式样例：</p>
<script type="math/tex; mode=display">
  \left[\begin{array}{l}
  r_{1} \\
  ~\\
  r_{2} \\
  ~\\
  r_{3}
  \end{array}\right]=\left[\begin{array}{l}
  b_{1} \\
  ~\\
  b_{2} \\
  ~\\
  b_{3}
  \end{array}\right]+\left[\begin{array}{lll}
  w_{11} & w_{12} & w_{13} \\
  ~\\
  w_{21} & w_{22} & w_{23} \\
  ~\\
  w_{31} & w_{32} & w_{33}
  \end{array}\right]\left[\begin{array}{l}
  x_{1} \\
  ~\\
  x_{2} \\
  ~\\
  x_{3}
  \end{array}\right]</script><p>  即:</p>
<script type="math/tex; mode=display">
  \vec{r}=\vec{b}+\vec{W}  \vec{x}</script><p>  然后：$定义~\sigma = sigmoid~~, ~~\vec{a} = \sigma(\vec{r}) ~~, ~~则：y = b + \vec{c}^{~T} \vec{a}$</p>
<p>  所以：$y = b + \vec{c}^{~T} \sigma(\vec{b}+\vec{W}  \vec{x})$ ，其中 $\vec{x}$ ：feature； $\vec{W}, \vec{b}, \vec{c}^{~T}, b$ ：Unknown parameters</p>
<p>  定义：列向量$\vec{\theta}$为所有Unknown parameters的展开的集合（如果是矩阵，就是所有元素展开的集合）</p>
<p>  最小化：</p>
<script type="math/tex; mode=display">
  \boldsymbol{\theta}^{*}=\arg \min _{\boldsymbol{\theta}} L~~~~
  其中：
  \boldsymbol{\theta}=\left[\begin{array}{c}
  \theta_{1} \\
  ~\\
  {\theta}_{2} \\
  ~\\
  {\theta}_{3} \\
  ~\\
  \vdots
  \end{array}\right]

  \\
  ~\\

  （随机）选择初始值\ \ \boldsymbol{\theta}^{0} \\
  ~\\ 

  ~\\
  ~\\

  取：\begin{array}{l}
  \boldsymbol{g}=\nabla L\left(\boldsymbol{\theta}^{0}\right) \quad ，则参数更新：\boldsymbol{\theta}^{1} \leftarrow \boldsymbol{\theta}^{0}-\eta \boldsymbol{g} \\
  ~\\
  \end{array}

  ~\\
  ~\\
  ~\\
  ~\\

  取：\begin{array}{l}
  \boldsymbol{g}=\nabla L\left(\boldsymbol{\theta}^{1}\right) \quad ，则参数更新：\boldsymbol{\theta}^{2} \leftarrow \boldsymbol{\theta}^{1}-\eta \boldsymbol{g} \\
  ~\\
  \end{array}

  ~\\
  ~\\
  ~\\
  ~\\
  ......</script><p>  每对一个Batch 一次更新参数： Update<br>  遍历一遍 Batch： Epoch</p>
</li>
<li><p>ReLU（Rectified Linear Unit）：</p>
<script type="math/tex; mode=display">
      c ~ max(0, b+w*x_1)</script><p>  则 Hard Sigmoid 可以写成：</p>
<script type="math/tex; mode=display">
      c ~ max(0, b+w*x_1) + c' ~ max(0, b'+w'*x_1)</script><p>  相当于用两个 ReLU 合成一个 Sigmoid</p>
</li>
</ul>
<p>可以套多个Activation function，如$y = b + \vec{c}^{~T} \sigma\left(\vec{b ‘} + \vec{W’}\sigma(\vec{b}+\vec{W}  \vec{x})\right)$</p>
<p><strong>为什么要套用多层Activation function（Deep）？</strong>：后面解答。</p>
<p>Neuron -&gt; Neurl Network<br>layers：一层Neuron<br>many hidden layers -&gt; Deep Learning</p>
<p>在训练集上表现更好而在测试集上变现变差：Overfitting（过拟合） </p>
<hr>
<h2 id="Lecture-2-What-to-do-if-my-network-fails-to-train"><a href="#Lecture-2-What-to-do-if-my-network-fails-to-train" class="headerlink" title="Lecture 2:What to do if my network fails to train"></a>Lecture 2:What to do if my network fails to train</h2><h3 id="機器學習任務攻略"><a href="#機器學習任務攻略" class="headerlink" title="機器學習任務攻略"></a>機器學習任務攻略</h3><p>机器学习框架：</p>
<ul>
<li><p>Training data:  $\left\{\left(x^{1}, \hat{y}^{1}\right),\left(x^{2}, \hat{y}^{2}\right), \ldots,\left(x^{N}, \hat{y}^{N}\right)\right\}$</p>
</li>
<li><p>Training: $y=f<em>{\boldsymbol{\theta}}(\boldsymbol{x})\rightarrow L\left(\boldsymbol{\theta}\right)\rightarrow\boldsymbol{\theta}^{*} = arg\text{min}</em>{\boldsymbol{\theta}}L$</p>
</li>
<li><p>Testing data:  $\left\{x^{N+1}, x^{N+2}, \ldots, x^{N+M}\right\}$</p>
</li>
<li><p>Use  $y<em>{0}=f</em>{\boldsymbol{\theta}^{*}}(\boldsymbol{x})$  to label the testing data  </p>
</li>
<li><p>Get $\left\{y^{N+1}, y^{N+2}, \ldots, y^{N+M}\right\}$ </p>
</li>
</ul>
<p>如何优化：</p>
<ul>
<li>loss on training data<ul>
<li>large：<ul>
<li>model bias（模型偏差）:<br>原因：模型太简单，不能很好的拟合数据<br>解决方法：重新设计model，给你的model更大的弹性（范围）<ul>
<li>more features</li>
<li>deep learning（more neurons，more layers）</li>
</ul>
</li>
<li>optimization issue（最优化过程（梯度下降）中的问题）<ul>
<li>解决方案:更强大的优化技术(下一讲)</li>
</ul>
</li>
<li>如何判断是哪个原因导致的？<ul>
<li>比较不同的模型</li>
<li>从较浅的网络(或其他模型)开始，因为其更容易优化</li>
<li>如果深度网络不能在训练数据上获得更小的损失，那么就存在优化问题</li>
</ul>
</li>
</ul>
</li>
<li>small：<ul>
<li>loss on testing data<ul>
<li>large：<ul>
<li>overfitting（过拟合）<ul>
<li>解决方案：<ul>
<li>more training data（增加训练样本）</li>
<li>data augmentation（数据增强）（如通过旋转、平移、缩放、翻转、加噪声等方式增加图像数据）</li>
<li>constrained model（约束模型）<ul>
<li>less parameters（更少参数），shared parameters（共用参数（CNN））</li>
<li>less features（更少特征）</li>
<li>early stopping（早停）</li>
<li>regularization（正则化）<ul>
<li>L1 regularization</li>
<li>L2 regularization</li>
</ul>
</li>
<li>dropout</li>
</ul>
</li>
<li>注意：不要加以太多限制，否则会有问题（model bias）</li>
</ul>
</li>
</ul>
</li>
<li>mismatch：训练集和测试集不匹配（具有不同的分布）<br>了解资料是如何生成的</li>
</ul>
</li>
<li>small：<ul>
<li><strong>good model ！</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>cross validation（交叉验证）：<br>Training Set -&gt; Training Set + Validation Set<br>不能根据测试集来反向挑选模型</p>
<p>N-fold cross validation（N折交叉验证）：<br>将训练集分成N份，每次用其中一份作为验证集，其余N-1份作为训练集，进行N次训练，取平均值，获得其中最优的模型，然后再把该模型用在全部的训练集上进行训练，得到最终的模型。</p>
<h3 id="類神經網路訓練不起來怎麼辦-一-：-局部最小值-local-minima-與鞍點-saddle-point"><a href="#類神經網路訓練不起來怎麼辦-一-：-局部最小值-local-minima-與鞍點-saddle-point" class="headerlink" title="類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point)"></a>類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point)</h3><p>Optimization（Gradient Descent）：</p>
<p>gradient（梯度）为零：local minima（局部最小值）或saddle point（鞍点）等，统称为critical point（临界点）</p>
<p>如何判断临界点类型：</p>
<ul>
<li><p>泰勒级数近似: </p>
<p>  $L(\boldsymbol{\theta})$ 在 $\boldsymbol{\theta}=\boldsymbol{\theta}^{\prime}$ 可以近似为： </p>
<script type="math/tex; mode=display">
      L(\boldsymbol{\theta}) \approx L\left(\boldsymbol{\theta}^{\prime}\right)+\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right)^{T} \boldsymbol{g}+\frac{1}{2}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right)^{T} \boldsymbol{H}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right)</script><p>  其中Gradient $\boldsymbol{g}$ 是一个向量：</p>
<script type="math/tex; mode=display">
      \boldsymbol{g}=\nabla L\left(\boldsymbol{\theta}^{\prime}\right) \quad \boldsymbol{g}_{i}=\frac{\partial L\left(\boldsymbol{\theta}^{\prime}\right)}{\partial \boldsymbol{\theta}_{i}}</script><p>  其中Hessian $\boldsymbol{H}$ 是一个矩阵：</p>
<script type="math/tex; mode=display">
      \boldsymbol{H}_{i j}=\frac{\partial^{2}}{\partial \boldsymbol{\theta}_{i} \partial \boldsymbol{\theta}_{j}} L\left(\boldsymbol{\theta}^{\prime}\right)</script><p>  因此，在临界点时，由临界点定义：$\boldsymbol{g} = 0$</p>
<p>  代入泰勒级数近似式，得到：</p>
<script type="math/tex; mode=display">
      L(\boldsymbol{\theta}) \approx L\left(\boldsymbol{\theta}^{\prime}\right)+\frac{1}{2}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right)^{T} \boldsymbol{H}\left(\boldsymbol{\theta}-\boldsymbol{\theta}^{\prime}\right)</script><p>  所以，有结论：</p>
<ul>
<li>当 $\boldsymbol{v}^{T}\boldsymbol{H}\boldsymbol{v}$ 永远为正即 $\boldsymbol{H}$ 是正定矩阵（所有特征值均为正数）时，$L(\boldsymbol{\theta})$ 为局部最小值（Local Minima）</li>
<li>当 $\boldsymbol{v}^{T}\boldsymbol{H}\boldsymbol{v}$ 永远为负即 $\boldsymbol{H}$ 是负定矩阵（所有特征值均为负数）时，$L(\boldsymbol{\theta})$ 为局部最大值（Local Maxima）</li>
<li>当 $\boldsymbol{v}^{T}\boldsymbol{H}\boldsymbol{v}$ 有正有负时，$L(\boldsymbol{\theta})$ 为鞍点（Saddle Point），此时最优化更新的方向可由 $\boldsymbol{H}$ 的特征值决定，朝负的特征值对应的特征向量 $L(\boldsymbol{u})$ 的方向更新可使Loss变小（实用性不强） </li>
</ul>
</li>
<li><p>在低维中的local minima，或许在高维中是saddle point</p>
</li>
<li>当你有很多参数时，也许局部最小值很少见?<ul>
<li>对的，在实际操作中，几乎找不到所有特征值均为正的临界点（local minima）的情况</li>
</ul>
</li>
</ul>
<h3 id="類神經網路訓練不起來怎麼辦-二-：-批次-batch-與動量-momentum"><a href="#類神經網路訓練不起來怎麼辦-二-：-批次-batch-與動量-momentum" class="headerlink" title="類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum)"></a>類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum)</h3><p>带有Batch的优化：</p>
<ul>
<li>Shuffle: 每次迭代(epoch)时，将训练集重新分为不同的batch，以避免每次迭代时都是相同的batch</li>
<li>为什么要用Batch:<ul>
<li>small batch 与 large batch 比较：<ul>
<li>较大的Batch size不需要更长的时间来计算梯度(除非批处理大小太大)</li>
<li>较小的Batch size需要更长的epoch时间(一次查看所有数据需要更长的时间)(GPU有一定的并行计算能力)</li>
<li>batch大小越小，性能越好</li>
<li>带有“噪声”的更新更适合训练</li>
<li>猜想：小的批量大小在测试集上的表现更好（泛化性能更好）</li>
</ul>
</li>
<li>结论：small batch 与 large batch各有优劣，在真实训练时作为一个超参数（hyper-parameter），需要根据实际情况进行调整</li>
</ul>
</li>
</ul>
<p>Momentum：梯度下降时每一步的更新方向不仅仅取决于当前的梯度，还取决于之前的更新方向，即当前的更新方向是之前更新方向的一个加权平均，这样做的好处是可以在一定程度上减少梯度下降的震荡，从而加快收敛速度。</p>
<p>流程：</p>
<ul>
<li>Starting at  $\boldsymbol{\theta}^{\mathbf{0}}$</li>
<li>Movement  $\boldsymbol{m}^{\mathbf{0}}=\mathbf{0}$</li>
<li>Compute gradient  $\boldsymbol{g}^{0}$</li>
<li>Movement  $\boldsymbol{m}^{1}=\lambda \boldsymbol{m}^{0}-\eta \boldsymbol{g}^{0}$</li>
<li>Move to  $\boldsymbol{\theta}^{1}=\boldsymbol{\theta}^{0}+\boldsymbol{m}^{1}$</li>
<li>Compute gradient  $\boldsymbol{g}^{\mathbf{1}}$</li>
<li>Movement  $\boldsymbol{m}^{2}=\lambda \boldsymbol{m}^{1}-\eta \boldsymbol{g}^{1}$</li>
<li>Move to  $\boldsymbol{\theta}^{2}=\boldsymbol{\theta}^{1}+\boldsymbol{m}^{2}$</li>
</ul>
<p>易得，$\boldsymbol{m}^{i}$ 是 $\boldsymbol{g}^{j}(j&lt;i)$ 的加权和，加权值也是超参数。</p>
<h3 id="類神經網路訓練不起來怎麼辦-三-：自動調整學習速率-Learning-Rate"><a href="#類神經網路訓練不起來怎麼辦-三-：自動調整學習速率-Learning-Rate" class="headerlink" title="類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate)"></a>類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate)</h3><p>当 error surface 崎岖不平时：<br>训练技巧：自适应学习率（Adaptive Learning Rate）</p>
<p>训练卡顿 $\neq$ 小梯度</p>
<p>即使没有到临界点，训练也会出现困难 -&gt; 学习率不能一成不变 -&gt; 不同的参数需要不同的学习率</p>
<p>对任一参数 $\boldsymbol{\theta}_{i}$：</p>
<script type="math/tex; mode=display">
  \begin{aligned}
  \boldsymbol{\theta}_{i}^{\boldsymbol{t}+\boldsymbol{1}} \leftarrow \boldsymbol{\theta}_{i}^{\boldsymbol{t}}-\eta \boldsymbol{g}_{i}^{\boldsymbol{t}} \\
  \boldsymbol{g}_{i}^{\boldsymbol{t}}=\left.\frac{\partial L}{\partial \boldsymbol{\theta}_{i}}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{\boldsymbol{t}}} \\
  \boldsymbol{\theta}_{i}^{\boldsymbol{t}+\mathbf{1}} \leftarrow \boldsymbol{\theta}_{i}^{\boldsymbol{t}}-\frac{\eta}{\sigma_{i}^{\boldsymbol{t}}} \boldsymbol{g}_{i}^{\boldsymbol{t}}
  \end{aligned}</script><p>则，其中的 $\frac{\eta}{\sigma_{i}^{\boldsymbol{t}}}$ 是参数依赖的学习率：</p>
<ul>
<li><p>Root Mean Square:</p>
<script type="math/tex; mode=display">
  \begin{array}{rlrl}
  \boldsymbol{\theta}_{i}^{1} & \leftarrow \boldsymbol{\theta}_{i}^{\mathbf{0}}-\frac{\eta}{\sigma_{i}^{0}} \boldsymbol{g}_{i}^{0} & \sigma_{i}^{0} & =\sqrt{\left(\boldsymbol{g}_{i}^{0}\right)^{2}}=\left|\boldsymbol{g}_{i}^{0}\right| \\
  \boldsymbol{\theta}_{i}^{2} & \leftarrow \boldsymbol{\theta}_{i}^{\mathbf{1}}-\frac{\eta}{\sigma_{i}^{1}} \boldsymbol{g}_{i}^{1} & \sigma_{i}^{1} & =\sqrt{\frac{1}{2}\left[\left(\boldsymbol{g}_{i}^{0}\right)^{2}+\left(\boldsymbol{g}_{i}^{1}\right)^{2}\right]} \\
  \boldsymbol{\theta}_{i}^{\mathbf{3}} & \leftarrow \boldsymbol{\theta}_{i}^{\mathbf{2}}-\frac{\eta}{\sigma_{i}^{2}} \boldsymbol{g}_{i}^{2} & \sigma_{i}^{2} & =\sqrt{\frac{1}{3}\left[\left(\boldsymbol{g}_{i}^{0}\right)^{2}+\left(\boldsymbol{g}_{i}^{1}\right)^{2}+\left(\boldsymbol{g}_{i}^{2}\right)^{2}\right]} \\
  \vdots & \\
  \boldsymbol{\theta}_{i}^{\boldsymbol{t}+\boldsymbol{1}} & \leftarrow \boldsymbol{\theta}_{i}^{\boldsymbol{t}}-\frac{\eta}{\sigma_{i}^{t}} \boldsymbol{g}_{i}^{\boldsymbol{t}} & \sigma_{i}^{t} & =\sqrt{\frac{1}{t+1} \sum_{i=0}^{t}\left(\boldsymbol{g}_{i}^{\boldsymbol{t}}\right)^{2}}
  \end{array}</script><p>（在AdaGrad算法中使用）</p>
<p>作用：在梯度下降大时，学习率降低；在梯度下降小时，学习率增大</p>
</li>
</ul>
<p>很多时候学习率需要动态适应（error surface 会非常复杂）：</p>
<ul>
<li><p>RMSProp</p>
<script type="math/tex; mode=display">
  \begin{array}{rlrl}
  \boldsymbol{\theta}_{i}^{1} & \leftarrow \boldsymbol{\theta}_{i}^{\mathbf{0}}-\frac{\eta}{\sigma_{i}^{0}} \boldsymbol{g}_{i}^{0} & \sigma_{i}^{0} & =\sqrt{\left(\boldsymbol{g}_{i}^{0}\right)^{2}}=\left|\boldsymbol{g}_{i}^{0}\right| \\
  \boldsymbol{\theta}_{i}^{2} & \leftarrow \boldsymbol{\theta}_{i}^{\mathbf{1}}-\frac{\eta}{\sigma_{i}^{1}} \boldsymbol{g}_{i}^{1} & \sigma_{i}^{1} & =\sqrt{\alpha\left(\sigma_{i}^{0}\right)^{2}+\left(1-\alpha\right)\left(\boldsymbol{g}_{i}^{1}\right)^{2}} \left(0<\alpha<1\right) \\
  \boldsymbol{\theta}_{i}^{\mathbf{3}} & \leftarrow \boldsymbol{\theta}_{i}^{\mathbf{2}}-\frac{\eta}{\sigma_{i}^{2}} \boldsymbol{g}_{i}^{2} & \sigma_{i}^{2} & =\sqrt{\alpha\left(\sigma_{i}^{1}\right)^{2}+\left(1-\alpha\right)\left(\boldsymbol{g}_{i}^{2}\right)^{2}} \left(0<\alpha<1\right) \\
  \vdots & \\
  \boldsymbol{\theta}_{i}^{\boldsymbol{t}+\boldsymbol{1}} & \leftarrow \boldsymbol{\theta}_{i}^{\boldsymbol{t}}-\frac{\eta}{\sigma_{i}^{t}} \boldsymbol{g}_{i}^{\boldsymbol{t}} & \sigma_{i}^{t} & =\sqrt{\alpha\left(\sigma_{i}^{t-1}\right)^{2}+\left(1-\alpha\right)\left(\boldsymbol{g}_{i}^{t}\right)^{2}} \left(0<\alpha<1\right)
  \end{array}</script><p>（$\alpha$是一个超参数）<br>（近期的梯度对学习率的影响更大，远期的梯度对学习率的影响更小）</p>
</li>
<li><p>Adam<br>Adam = RMSProp + Momentum</p>
<ul>
<li>deep learning套件(如pytorch)中大多有现成的</li>
<li>超参数往往用预设的就能得到较为不错的结果</li>
</ul>
</li>
</ul>
<p>Learning Rate Scheduling（学习率调度）</p>
<script type="math/tex; mode=display">
  \begin{aligned}
  \boldsymbol{\theta}_{i}^{\boldsymbol{t}+\mathbf{1}} \leftarrow \boldsymbol{\theta}_{i}^{\boldsymbol{t}}-\frac{\eta^{\boldsymbol{t}}}{\sigma_{i}^{\boldsymbol{t}}} \boldsymbol{g}_{i}^{\boldsymbol{t}}
  \end{aligned}</script><p>中的 $\eta$ 设置为与时间相关</p>
<ul>
<li><p>Learning Rate Decay（学习率衰减）</p>
<p>学习率随着时间衰减</p>
<p>原因：随着训练的进行，我们离目标越来越近，所以我们降低了学习率</p>
<p>（$\alpha$是一个超参数）</p>
</li>
<li><p>Warm Up</p>
<p>学习率先从小到大，再从大到小</p>
<p>原因：（未知），解释之一：在训练开始的时候，对 $\frac{\eta}{\sigma_{i}^{\boldsymbol{t}}}$ 的估计不准确，所以先从小到大，而从大到小的原因与Learning Rate Decay相同</p>
</li>
</ul>
<p>因此，各种改进之后：</p>
<script type="math/tex; mode=display">
  \begin{aligned}
  \boldsymbol{\theta}_{i}^{\boldsymbol{t}+\mathbf{1}} \leftarrow \boldsymbol{\theta}_{i}^{\boldsymbol{t}}-\frac{\eta^{\boldsymbol{t}}}{\sigma_{i}^{\boldsymbol{t}}} \boldsymbol{m}_{i}^{\boldsymbol{t}}
  \end{aligned}</script><p>$\boldsymbol{m}_{i}^{\boldsymbol{t}} \rightarrow$ Momentum 之前梯度的加权和<br>（包括方向）</p>
<p>$\sigma_{i}^{\boldsymbol{t}} \rightarrow$ 梯度的均方根（只有大小）</p>
<p>$\eta^{\boldsymbol{t}} \rightarrow$ Learning Rate Scheduling（学习率调度）</p>
<h3 id="類神經網路訓練不起來怎麼辦-四-：損失函數-Loss-也可能有影響"><a href="#類神經網路訓練不起來怎麼辦-四-：損失函數-Loss-也可能有影響" class="headerlink" title="類神經網路訓練不起來怎麼辦 (四)：損失函數 (Loss) 也可能有影響"></a>類神經網路訓練不起來怎麼辦 (四)：損失函數 (Loss) 也可能有影響</h3><p>Classification as Regression？（把分类问题当成回归问题）</p>
<ul>
<li><p>Regression<br>输入向量 -&gt; 输出<strong>一个</strong>数值 -&gt; 对比与label的接近程度</p>
</li>
<li><p>Classification as Regression？<br>label（$\hat{y}$）使用one-hot vector表示每一个class，输出是一个向量，对比向量的接近程度</p>
</li>
</ul>
<p>例如：</p>
<script type="math/tex; mode=display">
\begin{array}{c} ~~~~~~
\text { Class } 1 ~~~~~
\text { Class } 2 ~~~~~
\text { Class } 3 \\
\hat{\boldsymbol{y}}=\left[\begin{array}{l}
1 \\
0 \\
0
\end{array}\right] \text { or }\left[\begin{array}{l}
0 \\
1 \\
0
\end{array}\right] \text { or }\left[\begin{array}{l}
0 \\
0 \\
1
\end{array}\right]
\end{array}</script><p>Regression的问题：输出的向量为一个实数，而不是一个向量，因此我们需要将其转换为一个向量。</p>
<p>Regression：</p>
<script type="math/tex; mode=display">
  \hat{y} \leftrightarrow y = b + \vec{c}^{~T} \sigma(\vec{b}+\vec{W}  \vec{x})</script><p>Classification：</p>
<script type="math/tex; mode=display">
  y = b + \vec{c}^{~T} \sigma(\vec{b}+\vec{W}  \vec{x}) \\
  \hat{y} \leftrightarrow y' = softmax(y)</script><p>为啥要有 $softmax$ 函数：（一种不准确的解释是）因为 $y$ 中的值是任意的，而 $\hat{y}$ 中的值是 0 / 1，因此 $y$ 需要通过函数$softmax$转换其所有向量中的值为 0 ~ 1 之间以匹配 $\hat{y}$ ，或与 $\hat{y}$ 计算相似度</p>
<p>Softmax：</p>
<script type="math/tex; mode=display">
  y_{i}^{\prime}=\frac{\exp \left(y_{i}\right)}{\sum_{j} \exp \left(y_{i}\right)}  (1 > y_{i}^{\prime} > 0, {\sum_{j} y_{i}^{\prime} = 1})</script><ul>
<li>只有两个class时，softmax = sigmoid</li>
</ul>
<p>Loss of Classification：（即如何度量 $y$ 与 $label: \hat{y}$ 之间的偏差）</p>
<script type="math/tex; mode=display">
  Mean Square Error (MSE)  \quad e=\sum_{i}\left(\widehat{\boldsymbol{y}}_{i}-\boldsymbol{y}_{i}^{\prime}\right)^{2} \\
  Cross-entropy  \quad e=-\sum_{i} \widehat{\boldsymbol{y}}_{i} \ln \boldsymbol{y}_{i}^{\prime} （更常用，更优）</script><ul>
<li><p>Minimizing cross-entropy is equivalent to maximizing likelihood.（最小化交叉熵相当于最大化可能性。）</p>
</li>
<li><p>在PyTorch中，Cross-entropy 和 Softmax 是绑定使用的</p>
</li>
<li><p>改变损失函数可以改变优化的难度</p>
</li>
</ul>
<h3 id="類神經網路訓練不起來怎麼辦-五-：-批次標準化-Batch-Normalization-簡介"><a href="#類神經網路訓練不起來怎麼辦-五-：-批次標準化-Batch-Normalization-簡介" class="headerlink" title="類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介"></a>類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介</h3><hr>
<h2 id="Lecture-3-Image-as-input"><a href="#Lecture-3-Image-as-input" class="headerlink" title="Lecture 3:Image as input"></a>Lecture 3:Image as input</h2><hr>
<h2 id="Lecture-4-Sequence-as-input"><a href="#Lecture-4-Sequence-as-input" class="headerlink" title="Lecture 4:Sequence as input"></a>Lecture 4:Sequence as input</h2><hr>
<h2 id="Lecture-5-Sequence-to-sequence"><a href="#Lecture-5-Sequence-to-sequence" class="headerlink" title="Lecture 5:Sequence to sequence"></a>Lecture 5:Sequence to sequence</h2><hr>
<h2 id="Lecture-6-Generation"><a href="#Lecture-6-Generation" class="headerlink" title="Lecture 6:Generation"></a>Lecture 6:Generation</h2><hr>
<h2 id="Recent-Advance-of-Self-supervised-learning-for-NLP"><a href="#Recent-Advance-of-Self-supervised-learning-for-NLP" class="headerlink" title="Recent Advance of Self-supervised learning for NLP"></a>Recent Advance of Self-supervised learning for NLP</h2><hr>
<h2 id="Lecture-7-Self-supervised-learning-for-Speech-and-Image"><a href="#Lecture-7-Self-supervised-learning-for-Speech-and-Image" class="headerlink" title="Lecture 7:Self-supervised learning for Speech and Image"></a>Lecture 7:Self-supervised learning for Speech and Image</h2><hr>
<h2 id="Lecture-8-Auto-encoder-Anomaly-Detection"><a href="#Lecture-8-Auto-encoder-Anomaly-Detection" class="headerlink" title="Lecture 8:Auto-encoder/ Anomaly Detection"></a>Lecture 8:Auto-encoder/ Anomaly Detection</h2><hr>
<h2 id="Lecture-9-Explainable-AI"><a href="#Lecture-9-Explainable-AI" class="headerlink" title="Lecture 9:Explainable AI"></a>Lecture 9:Explainable AI</h2><hr>
<h2 id="Lecture-10-Attack"><a href="#Lecture-10-Attack" class="headerlink" title="Lecture 10:Attack"></a>Lecture 10:Attack</h2><hr>
<h2 id="Lecture-11-Adaptation"><a href="#Lecture-11-Adaptation" class="headerlink" title="Lecture 11:Adaptation"></a>Lecture 11:Adaptation</h2><hr>
<h2 id="Lecture-12-Reinforcement-Learning"><a href="#Lecture-12-Reinforcement-Learning" class="headerlink" title="Lecture 12:Reinforcement Learning"></a>Lecture 12:Reinforcement Learning</h2><hr>
<h2 id="Lecture-13-Network-Compression"><a href="#Lecture-13-Network-Compression" class="headerlink" title="Lecture 13:Network Compression"></a>Lecture 13:Network Compression</h2><hr>
<h2 id="Lecture-14-Life-long-Learning"><a href="#Lecture-14-Life-long-Learning" class="headerlink" title="Lecture 14:Life-long Learning"></a>Lecture 14:Life-long Learning</h2><hr>
<h2 id="Lecture-15-Meta-Learning"><a href="#Lecture-15-Meta-Learning" class="headerlink" title="Lecture 15:Meta Learning"></a>Lecture 15:Meta Learning</h2><hr>

                
                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2023/10/28/项目实践 - 1/" data-toggle="tooltip" data-placement="top" title="项目实践 - 1">&larr; Previous Post
		<br>
		<span><font size="2" face="Calibri" color="grey">项目实践 - 1</font></span>
	       </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2023/09/30/保研全过程分享/" data-toggle="tooltip" data-placement="top" title="保研全过程分享">Next Post &rarr;
		<br>
		<span><font size="2" face="Calibri" color="grey">保研全过程分享</font></span>
	        </a>
                    </li>
                    
                </ul>

                <!-- tip start -->
                

                
                <div class="comment_notes">
                    <p>
                        by Tan
                    </p>
                </div>
                
                <!-- tip end -->

                <!-- Music start-->
                
                <!-- Music end -->

                <!-- Sharing -->
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            
              <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-1-Introduction-of-Deep-Learning"><span class="toc-nav-text">Lecture 1:Introduction of Deep Learning</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#預測本頻道觀看人數-上-機器學習基本概念簡介"><span class="toc-nav-text">預測本頻道觀看人數 (上) - 機器學習基本概念簡介</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#預測本頻道觀看人數-下-深度學習基本概念簡介"><span class="toc-nav-text">預測本頻道觀看人數 (下) - 深度學習基本概念簡介</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-2-What-to-do-if-my-network-fails-to-train"><span class="toc-nav-text">Lecture 2:What to do if my network fails to train</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#機器學習任務攻略"><span class="toc-nav-text">機器學習任務攻略</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#類神經網路訓練不起來怎麼辦-一-：-局部最小值-local-minima-與鞍點-saddle-point"><span class="toc-nav-text">類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#類神經網路訓練不起來怎麼辦-二-：-批次-batch-與動量-momentum"><span class="toc-nav-text">類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#類神經網路訓練不起來怎麼辦-三-：自動調整學習速率-Learning-Rate"><span class="toc-nav-text">類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate)</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#類神經網路訓練不起來怎麼辦-四-：損失函數-Loss-也可能有影響"><span class="toc-nav-text">類神經網路訓練不起來怎麼辦 (四)：損失函數 (Loss) 也可能有影響</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#類神經網路訓練不起來怎麼辦-五-：-批次標準化-Batch-Normalization-簡介"><span class="toc-nav-text">類神經網路訓練不起來怎麼辦 (五)： 批次標準化 (Batch Normalization) 簡介</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-3-Image-as-input"><span class="toc-nav-text">Lecture 3:Image as input</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-4-Sequence-as-input"><span class="toc-nav-text">Lecture 4:Sequence as input</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-5-Sequence-to-sequence"><span class="toc-nav-text">Lecture 5:Sequence to sequence</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-6-Generation"><span class="toc-nav-text">Lecture 6:Generation</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Recent-Advance-of-Self-supervised-learning-for-NLP"><span class="toc-nav-text">Recent Advance of Self-supervised learning for NLP</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-7-Self-supervised-learning-for-Speech-and-Image"><span class="toc-nav-text">Lecture 7:Self-supervised learning for Speech and Image</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-8-Auto-encoder-Anomaly-Detection"><span class="toc-nav-text">Lecture 8:Auto-encoder&#x2F; Anomaly Detection</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-9-Explainable-AI"><span class="toc-nav-text">Lecture 9:Explainable AI</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-10-Attack"><span class="toc-nav-text">Lecture 10:Attack</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-11-Adaptation"><span class="toc-nav-text">Lecture 11:Adaptation</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-12-Reinforcement-Learning"><span class="toc-nav-text">Lecture 12:Reinforcement Learning</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-13-Network-Compression"><span class="toc-nav-text">Lecture 13:Network Compression</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-14-Life-long-Learning"><span class="toc-nav-text">Lecture 14:Life-long Learning</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Lecture-15-Meta-Learning"><span class="toc-nav-text">Lecture 15:Meta Learning</span></a></li></ol>
            
          
          </div>
        </aside>
      
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
                        
                    </div>
                </section>
                

            </div>
			
			<div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">
				
				<br>
				<br>

				<script src="https://utteranc.es/client.js"
						repo="Master-Tan/Master-Tan.github.io"
						issue-term="pathname"
						label="Comment"
						theme="github-light"
						crossorigin="anonymous"
						async>
				</script>
			</div>
        </div>
    </div>
</article>




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>


<style  type="text/css">
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                
                    <li>
                        <a target="_blank"  href="https://github.com/Master-Tan">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="https://twitter.com/None">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank" href="https://www.facebook.com/None">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Tan 2023 | Powered by 
                    <a href="https://github.com/Master-Tan/Master-Tan.github.io" target="_blank" rel="noopener">
                        <i>Tan's Blog</i>
                    </a>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://master-tan.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        
            <script type="text/javascript" src="/js/mouse-click.js" content='[&quot;Sunny&quot;, &quot;💖&quot;, &quot;Sunny&quot;, &quot;🧡&quot;, &quot;Sunny&quot;, &quot;💛&quot;, &quot;Sunny&quot; , &quot;💚&quot;, &quot;Sunny&quot;, &quot;💙&quot;, &quot;Sunny&quot;, &quot;💜&quot;, &quot;Sunny&quot;, &quot;😍&quot;]' color='[&quot;rgb(255, 0, 0)&quot; ,&quot;rgb(255, 0, 0)&quot; ,&quot;rgb(255, 125, 0)&quot; ,&quot;rgb(255, 125, 0)&quot; ,&quot;rgb(255, 255, 0)&quot; ,&quot;rgb(255, 255, 0)&quot; ,&quot;rgb(0, 255, 0)&quot; ,&quot;rgb(0, 255, 0)&quot; ,&quot;rgb(0, 255, 255)&quot; ,&quot;rgb(0, 255, 255)&quot; ,&quot;rgb(0, 0, 255)&quot; ,&quot;rgb(0, 0, 255)&quot; ,&quot;rgb(255, 0, 255)&quot; ,&quot;rgb(255, 0, 255)&quot;]'></script>
        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
